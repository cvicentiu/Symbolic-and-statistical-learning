<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<title>Regular Paper Abstracts | COLING•ACL 2006</title>
<meta http-equiv="Content-Style-Type" content="text/css" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<base href="" />
<style type="text/css" media="all">@import "misc/drupal.css";</style><style type="text/css" media="all">@import "themes/blix2/style.css";</style></head>
<body > 
<div id="container"> 
  <div id="header"> 
      <img src="aclmasthead2.png">
      <!-- h1><a href="" title="COLING•ACL 2006">COLING•ACL 2006</a></h1> 
      <div class="slogan"></div --> 
  </div> 

  <div id="navigation">
    
     
    <ul>
      
       
      <li><a href="http://www.acl2006.mq.edu.au/registration">Registration</a></li> 
       
      <li><a href="http://www.acl2006.mq.edu.au/dates">Program Overview</a></li> 
       
      <li><a href="http://www.acl2006.mq.edu.au/node">News</a></li> 
       
      <li><a href="http://www.acl2006.mq.edu.au/contact">Contact Us</a></li> 
       
    </ul> 
      </div>

<hr class="low" />
  <div id="subcontent">  
    <div class="block block-menu" id="block-menu-23">
  <h2>Main</h2>
  <div class="content"><div class="menu">
<ul>
<li class="collapsed"><a href="http://www.acl2006.mq.edu.au/welcome" title="">Home</a></li>
<li class="collapsed"><a href="http://www.acl2006.mq.edu.au/program" title="Program">Program</a></li>
<li class="collapsed"><a href="http://www.acl2006.mq.edu.au/registration" title="">Registration</a></li>
<li class="collapsed"><a href="http://www.acl2006.mq.edu.au/venue" title="">Venue</a></li>
<li class="collapsed"><a href="http://www.acl2006.mq.edu.au/accommodation" title="">Accommodation</a></li>
<li class="collapsed"><a href="http://www.acl2006.mq.edu.au/social" title="">Social Activities</a></li>
<li class="collapsed"><a href="http://www.acl2006.mq.edu.au/sponsorship" title="">Sponsorship</a></li>
<li class="leaf"><a href="http://www.acl2006.mq.edu.au/relatedevents" title="Related Events">Related Events</a></li>
<li class="leaf"><a href="http://www.acl2006.mq.edu.au/school" title="ACL/HCSNet Advanced Program in Natural Language Processing">Summer School</a></li>
<li class="collapsed"><a href="http://www.acl2006.mq.edu.au/committees" title="Conference Committees">Conference Committees</a></li>
<li class="leaf"><a href="http://www.acl2006.mq.edu.au/contact" title="">Contact Us</a></li>
<li class="collapsed"><a href="http://www.acl2006.mq.edu.au/archive" title="Archive">Archive</a></li>

</ul>
</div></div>
</div>
<div class="block block-search" id="block-search-0">
  <h2>Search</h2>
  <div class="content"><form action="http://www.acl2006.mq.edu.au/search/" method="post">
 <div class="search-form"><div class="form-item">
 <div class="container-inline"><div class="form-item">
 <input type="text" maxlength="255" class="form-text" name="edit[keys]" id="edit-keys" size="20" value="" />
</div>
<input type="submit" class="form-submit" name="op" value="Search"  />
</div>
</div>
</div>
</form>
</div>
</div>
   </div> 

  <div id="content"> 
    <div class="navigation"> <div class="breadcrumb"><a href="http://www.acl2006.mq.edu.au/">Home</a></div> </div> 
     
     
     
    <h2 class="page-title">Regular Paper Abstracts</h2> 
     
     
     
    <!-- start main content --> 
    <!-- begin content --><div class="entry"> 
   
  <p class=MsoBodyText>All ‘A’ sessions are held in the Bayside Auditorium A; ‘B’
sessions are in Bayside 103; ‘C’ sessions are in Bayside 104; and ‘D’ sessions
and the Student Research Workshop sessions are in Bayside 102.</p>

<div style='border:solid windowtext 1.0pt;mso-border-alt:solid windowtext .5pt;
padding:1.0pt 4.0pt 1.0pt 4.0pt'>

<h2>Monday 17th July 930am–1030am</h2>

</div>

<div style='border:none;border-bottom:solid windowtext 1.0pt;mso-border-bottom-alt:
solid windowtext .5pt;padding:0in 0in 1.0pt 0in'>

<h3>1A: Machine Translation I</h3>

</div>

<p class=SessionChair>Session Chair: David Chiang<span style='mso-tab-count:
7'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b1A1></a>Combination of Arabic Preprocessing
Schemes for Statistical Machine Translation</p>

<p class=AbstractAuthor>Fatiha Sadat<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Sadat, F.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Nizar Habash<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Habash, N.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText><span style='mso-font-kerning:0pt'>Statistical machine
translation is quite robust when it comes to the choice of input
representation. It only requires consistency between training and testing. As a
result, there is a wide range of possible preprocessing choices for data used
in statistical machine translation. This is even more so for morphologically
rich languages such as Arabic. In this paper, we study the effect of different
word-level preprocessing schemes for Arabic on the quality of phrase-based
statistical machine translation. We also present and evaluate different methods
for combining preprocessing schemes resulting in improved translation quality.<o:p></o:p></span></p>

<p class=AbstractTitle><a name=b1A2></a>Going Beyond <st1:stockticker>AER</st1:stockticker>:
An Extensive Analysis of Word Alignments and Their Impact on MT</p>

<p class=AbstractAuthor><span lang=FR style='mso-ansi-language:FR'>Necip Fazil
Ayan</span><!--[if supportFields]><span style='mso-element:field-begin'></span><span
lang=FR style='mso-ansi-language:FR'> XE &quot;Ayan, N.F.&quot; </span><![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span lang=FR
style='mso-ansi-language:FR'><span style='mso-spacerun:yes'>&nbsp;</span>and
Bonnie J. Dorr</span><!--[if supportFields]><span style='mso-element:field-begin'></span><span
lang=FR style='mso-ansi-language:FR'> XE &quot;Dorr, B.&quot; </span><![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span lang=FR
style='mso-ansi-language:FR'><o:p></o:p></span></p>

<p class=MsoBodyText><span style='mso-font-kerning:0pt'>This paper presents an
extensive evaluation of five different alignments and investigates their impact
on the corresponding MT system output. We introduce new measures for intrinsic
evaluations and examine the distribution of phrases and untranslated words
during decoding to identify which characteristics of different alignments
affect translation. We show that precision-oriented alignments yield better MT
output (translating more words and using longer phrases) than recall-oriented
alignments.<o:p></o:p></span></p>

<div style='border:none;border-bottom:solid windowtext 1.0pt;mso-border-bottom-alt:
solid windowtext .5pt;padding:0in 0in 1.0pt 0in'>

<h3>1B: Topic Segmentation</h3>

</div>

<p class=SessionChair>Session Chair: Martha Palmer<span style='mso-tab-count:
7'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b1B1></a>Unsupervised Topic Modelling for
Multi-Party Spoken Discourse</p>

<p class=AbstractAuthor>Matthew Purver<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Purver, M.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Konrad P. K&ouml;rding<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;K&ouml;rding, K.P.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Thomas L. Griffiths<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Griffiths, T.L.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Joshua B. Tenebaum<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Tenebaum, J.B.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText><span style='mso-font-kerning:0pt'>We present a method for
unsupervised topic modelling which adapts methods used in document
classification (Blei et al., 2003; Griffiths and Steyvers, 2004) to unsegmented
multi-party discourse transcripts. We show how Bayesian inference in this
generative model can be used to simultaneously address the problems of topic
segmentation and topic identification: automatically segmenting multi-party
meetings into topically coherent segments with performance which compares well
with previous unsupervised segmentation-only methods (Galley et al., 2003)
while simultaneously extracting topics which rate highly when assessed for
coherence by human judges. We also show that this method appears robust in the
face of off-topic dialogue and speech recognition errors.<o:p></o:p></span></p>

<p class=AbstractTitle><a name=b1B2></a>Minimum Cut Model for Spoken Lecture
Segmentation</p>

<p class=AbstractAuthor>Igor Malioutov<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Malioutov, I.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and <st1:City><st1:place>Regina</st1:place></st1:City>
Barzilay<!--[if supportFields]><span style='mso-element:field-begin'></span> XE
&quot;Barzilay, R.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText><span style='mso-font-kerning:0pt'>We consider the task of
unsupervised lecture segmentation. We formalize segmentation as a
graph-partitioning task that optimizes the normalized cut criterion. Our
approach moves beyond localized comparisons and takes into account long-range
cohesion dependencies. Our results demonstrate that global analysis improves
the segmentation accuracy and is robust in the presence of speech recognition
errors.<o:p></o:p></span></p>

<div style='border:none;border-bottom:solid windowtext 1.0pt;mso-border-bottom-alt:
solid windowtext .5pt;padding:0in 0in 1.0pt 0in'>

<h3>1C: Coreference</h3>

</div>

<p class=SessionChair>Session Chair: Vincent Ng<span style='mso-tab-count:7'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b1C1></a>Bootstrapping Path-Based Pronoun
Resolution</p>

<p class=AbstractAuthor>Shane Bergsma<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Bergsma, S.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Dekang Lin<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Lin, D.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText><span lang=EN-GB style='mso-ansi-language:EN-GB'>We
present an approach to pronoun resolution based on syntactic paths. Through a
simple bootstrapping procedure, we learn the likelihood of coreference between
a pronoun and a candidate noun based on the path in the parse tree between the
two entities. This path information enables us to handle previously challenging
resolution instances, and also robustly addresses traditional syntactic
coreference constraints. Highly coreferent paths also allow mining of precise
probabilistic gender/number information. We combine statistical knowledge with
well-known features in a Support Vector Machine pronoun resolution classifier.
Significant gains in performance are observed on several datasets.<o:p></o:p></span></p>

<p class=AbstractTitle><a name=b1C2></a>Kernel-Based Pronoun Resolution with
Structured Syntactic Knowledge</p>

<p class=AbstractAuthor><span lang=FR style='mso-ansi-language:FR'>Xiaofeng
Yang</span><!--[if supportFields]><span style='mso-element:field-begin'></span><span
lang=FR style='mso-ansi-language:FR'> XE &quot;Yang, X.&quot; </span><![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span lang=FR
style='mso-ansi-language:FR'>, Jian Su</span><!--[if supportFields]><span
style='mso-element:field-begin'></span><span lang=FR style='mso-ansi-language:
FR'> XE &quot;Su, J.&quot; </span><![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span lang=FR
style='mso-ansi-language:FR'><span style='mso-spacerun:yes'>&nbsp;</span>and
Chew Lim Tan</span><!--[if supportFields]><span style='mso-element:field-begin'></span><span
lang=FR style='mso-ansi-language:FR'> XE &quot;Tan, C.L.&quot; </span><![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span lang=FR
style='mso-ansi-language:FR'><o:p></o:p></span></p>

<p class=MsoBodyText>Syntactic knowledge is important for pronoun resolution. Traditionally,
the syntactic information for pronoun resolution is represented in terms of
features that have to be selected and defined heuristically. In the paper, we
propose a kernel-based method that can automatically mine the syntactic
information from the parse trees for pronoun resolution. Specifically, we
utilize the parse trees directly as a structured feature and apply kernel
functions to this feature, as well as other normal features, to learn the
resolution classifier. In this way, our approach avoids the efforts of decoding
the parse trees into the set of flat syntactic features. The experimental
results show that our approach can bring significant performance improvement
and is reliably effective for the pronoun resolution task.</p>

<div style='border:none;border-bottom:solid windowtext 1.0pt;mso-border-bottom-alt:
solid windowtext .5pt;padding:0in 0in 1.0pt 0in'>

<h3>1D: Grammars I</h3>

</div>

<p class=SessionChair>Session Chair: Martin Kay<span style='mso-tab-count:7'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b1D1></a>A Finite-State Model of Human Sentence
Processing</p>

<p class=AbstractAuthor><st1:place><st1:PlaceName>Jihyun</st1:PlaceName> <st1:PlaceType>Park</st1:PlaceType></st1:place><!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Park, J.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Chris Brew<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Brew, C.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText><span lang=EN-GB style='mso-font-kerning:0pt;mso-ansi-language:
EN-GB'>It has previously been assumed in the psycholinguistic literature that
finite-state models of language are crucially limited in their explanatory
power by the locality of the probability distribution and the narrow scope of
information used by the model. We show that a simple computational model (a
bigram part-of-speech tagger based on the design used by Corley and Crocker
(2000) makes correct predictions on processing difficulty observed in a wide
range of empirical sentence processing data.<span
style='mso-spacerun:yes'>&nbsp; </span>We use two modes of evaluation: one that
relies on comparison with a control sentence, paralleling practice in human studies;
another that measures probability drop in the disambiguating region of the
sentence. Both are surprisingly good indicators of the processing difficulty of
garden-path sentences. The sentences tested are drawn from published sources
and systematically explore five different types of ambiguity: previous studies
have been narrower in scope and smaller in scale. We do not deny the
limitations of finite-state models, but argue that our results show that their
usefulness has been underestimated.<o:p></o:p></span></p>

<p class=AbstractTitle><a name=b1D2></a>Acceptability Prediction by Means of
Grammaticality Quantification</p>

<p class=AbstractAuthor>Philippe Blache<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Blache, P.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Barbara Hemforth<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Hemforth, B.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and St&eacute;phane Rauzy<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Rauzy, S.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText>We propose in this paper a method for quantifying sentence<span
style='mso-ansi-language:EN-GB'> </span>grammaticality. The approach based on
Property Grammars, a<span style='mso-ansi-language:EN-GB'> </span>constraint-based
syntactic formalism, makes it possible to evaluate<span style='mso-ansi-language:
EN-GB'> </span>a grammaticality index for any kind of sentence, including<span
style='mso-ansi-language:EN-GB'> </span>ill-formed ones. We compare on a sample
of sentences the<span style='mso-ansi-language:EN-GB'> </span>grammaticality
indices obtained from PG formalism and the<span style='mso-ansi-language:EN-GB'>

</span>acceptability judgements measured by means of a psycholinguistic<span
style='mso-ansi-language:EN-GB'> </span>analysis. The results show that the
derived grammaticality index is<span style='mso-ansi-language:EN-GB'> </span>a
fairly good tracer of acceptability scores.<span style='mso-tab-count:7'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<div style='border:solid windowtext 1.0pt;mso-border-alt:solid windowtext .5pt;
padding:1.0pt 4.0pt 1.0pt 4.0pt'>

<h2>Monday 17th July 1100am–1230pm</h2>

</div>

<div style='border:none;border-bottom:solid windowtext 1.0pt;mso-border-bottom-alt:
solid windowtext .5pt;padding:0in 0in 1.0pt 0in'>

<h3>2A: Machine Translation II</h3>

</div>

<p class=SessionChair>Session Chair: David Chiang<span style='mso-tab-count:
7'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b2A1></a>Discriminative Word Alignment with
Conditional Random Fields</p>

<p class=AbstractAuthor>Phil Blunsom<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Blunsom, P.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Trevor Cohn<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Cohn, T.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText><span lang=EN-GB style='mso-ansi-language:EN-GB'>In this
paper we present a novel approach for inducing word alignments from sentence-aligned
data. We use a Conditional Random Field (CRF), a discriminative model, which is
estimated on a small supervised training set. The CRF is conditioned on both
the source and target texts, and thus allows for the use of arbitrary and
overlapping features over these data. Moreover, the CRF has efficient training
and decoding processes which both find globally optimal solutions.<o:p></o:p></span></p>

<p class=MsoBodyText><span lang=EN-GB style='mso-ansi-language:EN-GB'>We apply
this alignment model to both French-English and Romanian-English language
pairs. We show how a large number of highly predictive features can be easily
incorporated into the CRF, and demonstrate that even with only a few hundred
word-aligned training sentences, our model improves over the current
state-of-the-art with alignment error rates of 5.29 and 25.8 for the two tasks
respectively.<o:p></o:p></span></p>

<p class=AbstractTitle><a name=b2A2></a>Named Entity Transliteration with
Comparable Corpora</p>

<p class=AbstractAuthor>Richard Sproat<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Sproat, R.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Tao Tao<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Tao, T.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and ChengXiang Zhai<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Zhai, C.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText>In this paper we investigate Chinese-English name transliteration
using comparable corpora, corpora where texts in the two languages deal in some
of the same topics --- and therefore share references to named entities --- but
are not translations of each other.<span style='mso-spacerun:yes'>&nbsp;

</span>We present two distinct methods for transliteration, one approach using
phonetic transliteration, and the second using the temporal distribution of
candidate pairs.<span style='mso-spacerun:yes'>&nbsp; </span>Each of these
approaches works quite well, but by combining the approaches one can achieve
even better results. We then propose a novel score propagation method that
utilizes the co-occurrence of transliteration pairs within document pairs. This
propagation method achieves further improvement over the best results from the
previous step.</p>

<p class=AbstractTitle><a name=b2A3></a>Extracting Parallel Sub-Sentential
Fragments from Non-Parallel Corpora</p>

<p class=AbstractAuthor>Dragos Stefan Munteanu<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Munteanu, D.S.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Daniel Marcu<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Marcu, D.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText>We present a novel method for extracting parallel
sub-sentential fragments from comparable, non-parallel bilingual corpora. By
analyzing potentially similar sentence pairs using a signal processing-inspired
approach, we detect which segments of the source sentence are translated into
segments in the target sentence, and which are not. This method enables us to
extract useful machine translation training data even from very non-parallel
corpora, which contain no parallel sentence pairs. We evaluate the quality of
the extracted data by showing that it improves the performance of a
state-of-the-art statistical machine translation system.</p>

<div style='border:none;border-bottom:solid windowtext 1.0pt;mso-border-bottom-alt:
solid windowtext .5pt;padding:0in 0in 1.0pt 0in'>

<h3>2B: Word Sense Disambiguation I</h3>

</div>

<p class=SessionChair>Session Chair: Martha Palmer<span style='mso-tab-count:
7'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b2B1></a>Estimating Class Priors in Domain
Adaptation for Word Sense Disambiguation</p>

<p class=AbstractAuthor>Yee Seng Chan<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Chan, Y.S.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Hwee Tou Ng<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Ng, H.T.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText>Instances of a word drawn from different domains may have different<span
style='mso-ansi-language:EN-GB'> </span>sense priors (the proportions of the
different senses of a word). This<span style='mso-ansi-language:EN-GB'> </span>in
turn affects the accuracy of word sense disambiguation (WSD)<span
style='mso-ansi-language:EN-GB'> </span>systems trained and applied on
different domains. This paper presents<span style='mso-ansi-language:EN-GB'> </span>a
method to estimate the sense priors of words drawn from a new<span
style='mso-ansi-language:EN-GB'> </span>domain, and highlights the importance
of using well-calibrated<span style='mso-ansi-language:EN-GB'> </span>probabilities
when performing these estimations.<span style='mso-spacerun:yes'>&nbsp;
</span>By using well<span lang=EN-GB style='mso-ansi-language:EN-GB'>-calibrated</span>

probabilities, we are able to estimate the sense priors<span style='mso-ansi-language:
EN-GB'> </span>effectively to achieve significant improvements in WSD accuracy.<span
style='mso-tab-count:6'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b2B2></a>Ensemble Methods for Unsupervised WSD</p>

<p class=AbstractAuthor>Samuel Brody<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;<span style='mso-fareast-font-family:
SimSun;mso-fareast-language:ZH-CN'>Brody, S.</span>&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Roberto Navigli<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Navigli<span style='mso-fareast-font-family:
SimSun;mso-fareast-language:ZH-CN'>, R.</span>&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Mirella Lapata<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Lapata, M.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText>Combination methods are an effective way of improving
system performance. This paper examines the benefits of system combination for
unsupervised WSD. We investigate several voting- and arbiter-based combination
strategies over a diverse pool of unsupervised WSD systems. Our combination
methods rely on predominant senses which are derived automatically from raw
text. Experiments using the SemCor and Senseval-3 data sets demonstrate that
our ensembles yield significantly better results when compared with
state-of-the-art.</p>

<p class=AbstractTitle><a name=b2B3></a>Meaningful Clustering of Senses Helps
Boost Word Sense Disambiguation Performance</p>

<p class=AbstractAuthor>Roberto Navigli<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Navigli<span style='mso-fareast-font-family:
SimSun;mso-fareast-language:ZH-CN'>, R.</span>&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText>Fine-grained sense distinctions are one of the major
obstacles to successful Word Sense Disambiguation. In this paper, we present a
method for reducing the granularity of the WordNet sense inventory based on the
mapping to a manually crafted dictionary encoding sense hierarchies, namely the
Oxford Dictionary of English. We assess the quality of the mapping and the
induced clustering, and evaluate the performance of coarse WSD systems in the
Senseval-3 English all-words task.</p>

<div style='border:none;border-bottom:solid windowtext 1.0pt;mso-border-bottom-alt:
solid windowtext .5pt;padding:0in 0in 1.0pt 0in'>

<h3>2C: Information Extraction I</h3>

</div>

<p class=SessionChair>Session Chair: Vincent Ng<span style='mso-tab-count:7'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b2C1></a>Espresso: Leveraging Generic Patterns
for Automatically Harvesting Semantic Relations</p>

<p class=AbstractAuthor>Patrick Pantel<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Pantel, P.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Marco Pennacchiotti<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Pennacchiotti, M.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText><span lang=EN-GB style='mso-ansi-language:EN-GB'>In this
paper, we present Espresso, a weakly-supervised, general-purpose, and accurate
algorithm for harvesting semantic relations. The main contributions are: i) a
method for exploiting generic patterns by filtering incorrect instances using
the Web; and ii) a principled measure of pattern and instance reliability
enabling the filtering algorithm. We present an empirical comparison of
Espresso with various state of the art systems, on different size and genre
corpora, on extracting various general and specific relations. Experimental
results show that our exploitation of generic patterns substantially increases
system recall with small effect on overall precision.<o:p></o:p></span></p>

<p class=AbstractTitle><a name=b2C2></a>Modeling Commonality among Related
Classes in Relation Extraction</p>

<p class=AbstractAuthor><span lang=FR style='mso-ansi-language:FR'>Zhou GuoDong</span><!--[if supportFields]><span
lang=FR style='mso-ansi-language:FR'><span style='mso-element:field-begin'></span>
XE &quot;Zhou, G.&quot; </span><![endif]--><!--[if supportFields]><span
lang=FR style='mso-ansi-language:FR'><span style='mso-element:field-end'></span></span><![endif]--><span
lang=FR style='mso-ansi-language:FR'>, Su Jian</span><!--[if supportFields]><span
lang=FR style='mso-ansi-language:FR'><span style='mso-element:field-begin'></span>
XE &quot;Su, J.&quot; </span><![endif]--><!--[if supportFields]><span lang=FR
style='mso-ansi-language:FR'><span style='mso-element:field-end'></span></span><![endif]--><span
lang=FR style='mso-ansi-language:FR'><span
style='mso-spacerun:yes'>&nbsp;</span>and Zhang Min</span><!--[if supportFields]><span
style='mso-element:field-begin'></span><span lang=FR style='mso-ansi-language:
FR'> XE &quot;Zhang, M.&quot; </span><![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span lang=FR
style='mso-ansi-language:FR'><o:p></o:p></span></p>

<p class=MsoBodyText>This paper proposes a novel hierarchical learning strategy
to deal with the<span style='mso-ansi-language:EN-GB'> </span>data sparseness
problem in relation extraction by modeling the commonality<span
style='mso-ansi-language:EN-GB'> </span>among related classes. For each class
in the hierarchy either predefined<span style='mso-ansi-language:EN-GB'> </span>manually
or automatically clustered, a linear discriminative function is<span
style='mso-ansi-language:EN-GB'> </span>determined in a top-down way using a
perceptron algorithm with the lower-level<span style='mso-ansi-language:EN-GB'>
</span>weight vector derived from the upper-level weight vector. As the
upper-level<span style='mso-ansi-language:EN-GB'> </span>class normally has
much more positive training examples than the lower-level<span
style='mso-ansi-language:EN-GB'> </span>class, the corresponding linear
discriminative function can be determined more<span style='mso-ansi-language:
EN-GB'> </span>reliably. The upper-level discriminative function then can
effectively guide<span style='mso-ansi-language:EN-GB'> </span>the
discriminative function learning in the lower-level, which otherwise might<span
style='mso-ansi-language:EN-GB'> </span>suffer from limited training data.
Evaluation on the ACE <st1:stockticker>RDC</st1:stockticker> 2003 corpus shows<span
style='mso-ansi-language:EN-GB'> </span>that the hierarchical strategy much
improves the performance by 5.6 and 5.1 in<span style='mso-ansi-language:EN-GB'>

</span>F-measure on least- and medium- frequent relations respectively. It also
shows<span style='mso-ansi-language:EN-GB'> </span>that our system outperforms
the previous best-reported system by 2.7 in<span style='mso-ansi-language:EN-GB'>
</span>F-measure on the 24 subtypes using the same feature set.</p>

<p class=AbstractTitle><a name=b2C3></a>Relation Extraction Using Label
Propagation Based Semi-supervised Learning</p>

<p class=AbstractAuthor>Jinxiu Chen<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Chen, J.(1)&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Donghong Ji<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Ji, D.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Chew Lim Tan<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Tan, C.L.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Zhengyu Niu<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Niu, Z.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText><span lang=EN-GB style='mso-ansi-language:EN-GB'>Shortage
of manually labeled data is an obstacle to supervised relation extraction
methods. In this paper we investigate a graph based semi-supervised learning
algorithm, a label propagation (LP) algorithm, for relation extraction. It
represents labeled and unlabeled examples and their distances as the nodes and
the weights of edges of a graph, and tries to obtain a labeling function to
satisfy two constraints: 1) it should be fixed on the labeled nodes, 2) it
should be smooth on the whole graph. Experiment results on the ACE corpus
showed that this LP algorithm achieves better performance than </span><st1:stockticker><span
 lang=EN-GB style='mso-ansi-language:EN-GB'>SVM</span></st1:stockticker><span
lang=EN-GB style='mso-ansi-language:EN-GB'> when only very few labeled examples
are available, and it also performs better than bootstrapping for the relation
extraction task.<o:p></o:p></span></p>

<div style='border:none;border-bottom:solid windowtext 1.0pt;mso-border-bottom-alt:
solid windowtext .5pt;padding:0in 0in 1.0pt 0in'>

<h3>2D: Grammars II</h3>

</div>

<p class=SessionChair>Session Chair: Martin Kay<span style='mso-tab-count:7'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b2D1></a>Polarized Unification Grammars</p>

<p class=AbstractAuthor>Sylvain Kahane<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Kahane, S.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText><span style='mso-font-kerning:0pt'>This paper proposes a
generic mathematical formalism for the combination of various structures:
strings, trees, dags, graphs and products of them. The polarization of the
objects of the elementary structures controls the saturation of the final
structure. This formalism is both elementary and powerful enough to strongly
simulate many grammar formalisms, such as rewriting systems, dependency
grammars, </span><st1:stockticker><span style='mso-font-kerning:0pt'>TAG</span></st1:stockticker><span
style='mso-font-kerning:0pt'>, HPSG and </span><st1:stockticker><span
 style='mso-font-kerning:0pt'>LFG</span></st1:stockticker><span
style='mso-font-kerning:0pt'>.<o:p></o:p></span></p>

<p class=AbstractTitle><a name=b2D2></a>Partially Specified Signatures: a
Vehicle for Grammar Modularity</p>

<p class=AbstractAuthor>Yael Cohen-Sygal<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Cohen-Sygal, Y.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Shuly Wintner<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Wintner, S.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText><span style='mso-font-kerning:0pt'>This work provides the
essential foundations</span><span style='mso-font-kerning:0pt;mso-ansi-language:
EN-GB'> </span><span style='mso-font-kerning:0pt'>for modular construction of
(typed)</span><span style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>unification grammars for natural languages.</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>Much of the information in such</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>grammars is encoded in the signature, and</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>hence the key is facilitating a modularized</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>development of type signatures. We introduce</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>a definition of signature modules and</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>show how two modules combine. Our definitions</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>are motivated by the actual needs</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>of grammar developers obtained through a</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>careful examination of large scale grammars.</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>We show that our definitions meet</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>these needs by conforming to a detailed set</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>of desiderata.<o:p></o:p></span></p>

<p class=AbstractTitle><a name=b2D3></a>Morphology-Syntax Interface for Turkish
<st1:stockticker>LFG</st1:stockticker></p>

<p class=AbstractAuthor>&Ouml;zlem &Ccedil;etino&#287;lu<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;&Ccedil;etino&#287;lu, &Ouml;.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Kemal Oflazer<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Oflazer, K.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText><span lang=EN-GB style='mso-font-kerning:0pt;mso-ansi-language:
EN-GB'>This paper investigates the use of sublexical units as a solution to
handling the complex morphology with productive derivational processes, in the
development of a lexical functional grammar for Turkish. Such sublexical units
make it possible to expose the internal structure of words with multiple
derivations to the grammar rules in a uniform manner. This in turn leads to
more succinct and manageable rules. Further, the semantics of the derivations
can also be systematically reflected in a compositional way by constructing </span><st1:stockticker><span
 lang=EN-GB style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'>PRED</span></st1:stockticker><span
lang=EN-GB style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> values on the
fly. We illustrate how we use sublexical units for handling simple productive
derivational morphology and more interesting cases such as causativization,
etc., which change verb valency. Our priority is to handle several linguistic
phenomena in order to observe the effects of our approach on both the
c-structure and the f-structure representation, and grammar writing, leaving
the coverage and evaluation issues aside for the moment.<o:p></o:p></span></p>

<div style='border:solid windowtext 1.0pt;mso-border-alt:solid windowtext .5pt;
padding:1.0pt 4.0pt 1.0pt 4.0pt'>

<h2>Monday 17th July 200pm–330pm</h2>

</div>

<div style='border:none;border-bottom:solid windowtext 1.0pt;mso-border-bottom-alt:
solid windowtext .5pt;padding:0in 0in 1.0pt 0in'>

<h3>3A: Parsing I</h3>

</div>

<p class=SessionChair>Session Chair: Joakim Nivre<span style='mso-tab-count:
7'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b3A1></a>PCFGs with Syntactic and Prosodic
Indicators of Speech Repairs</p>

<p class=AbstractAuthor>John Hale<!--[if supportFields]><span style='mso-element:
field-begin'></span> XE &quot;Hale, J.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Izhak Shafran<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Shafran, I.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Lisa Yung<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Yung, L.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Bonnie Dorr<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Dorr, B.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Mary Harper<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Harper, M.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Anna Krasnyanskaya<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Krasnyanskaya, A.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Matthew Lease<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Lease, M.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Yang Liu<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Liu, Y.(2)&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Brian Roark<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Roark, B.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Matthew Snover<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Snover, M.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Robin Stewart<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Stewart, R.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText><span style='mso-font-kerning:0pt'>A grammatical method of
combining two kinds of speech repair cues is presented. One cue, prosodic
disjuncture, is detected by a decision tree-based ensemble classifier that uses
acoustic cues to identify where normal prosody seems to be interrupted
(Lickley, 1996). The other cue, syntactic parallelism, codifies the expectation
that repairs continue a syntactic category that was left unfinished in the
reparandum (Levelt, 1983). The two cues are combined in a Treebank PCFG whose
states are split using a few simple tree transformations. Parsing performance
on the Switchboard and Fisher corpora suggests that these two cues help to
locate speech repairs in a synergistic way.<o:p></o:p></span></p>

<p class=AbstractTitle><a name=b3A2></a>Dependency Parsing of Japanese Spoken
Monologue Based on Clause Boundaries</p>

<p class=AbstractAuthor>Tomohiro Ohno<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Ohno, T.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Shigeki Matsubara<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Matsubara, S.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Hideki Kashioka<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Kashioka, H.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Takehiko Maruyama<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Maruyama, T.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Yasuyoshi Inagaki<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Inagaki, Y.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText>Spoken monologues feature greater sentence length and
structural complexity<span style='mso-ansi-language:EN-GB'> </span>than do
spoken dialogues.<span style='mso-ansi-language:EN-GB'> </span>To achieve high
parsing performance for spoken monologues, it could prove effective to simplify
the structure by dividing a sentence into suitable language units. This paper
proposes a method for dependency parsing of Japanese monologues based on
sentence segmentation. In this method, the dependency parsing is executed in
two stages: at the clause level and the sentence level. First, the dependencies
within a clause are identified by dividing a sentence into clauses and
executing stochastic dependency parsing for each clause. Next, the dependencies
over clause boundaries are identified stochastically, and the dependency
structure of the entire sentence is thus completed. An experiment using a
spoken monologue corpus shows this method to be effective for efficient dependency
parsing of Japanese monologue sentences.</p>

<p class=AbstractTitle><a name=b3A3></a>Trace Prediction and Recovery With
Unlexicalized PCFGs and Slash Features</p>

<p class=AbstractAuthor>Helmut Schmid<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Schmid, H.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText><span style='mso-font-kerning:0pt'>This paper describes a
parser which generates</span><span style='mso-font-kerning:0pt;mso-ansi-language:
EN-GB'> </span><span style='mso-font-kerning:0pt'>parse trees with empty
elements in</span><span style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>which traces and fillers are co-indexed.</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>The parser is an unlexicalized PCFG</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>parser which is guaranteed to return the</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>most probable parse. The grammar is</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>extracted from a version of the </span><st1:stockticker><span
 style='mso-font-kerning:0pt'>PENN</span></st1:stockticker><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>treebank which was automatically annotated</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>with features in the style of Klein</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>and Manning (2003). The annotation includes</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>GPSG-style slash features which</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>link traces and fillers, and other features</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>which improve the general parsing accuracy.</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>In an evaluation on the </span><st1:stockticker><span
 style='mso-font-kerning:0pt'>PENN</span></st1:stockticker><span
style='mso-font-kerning:0pt'> treebank</span><span style='mso-font-kerning:
0pt;mso-ansi-language:EN-GB'> </span><span style='mso-font-kerning:0pt'>(Marcus
et al., 1993), the parser</span><span style='mso-font-kerning:0pt;mso-ansi-language:
EN-GB'> </span><span style='mso-font-kerning:0pt'>outperformed other
unlexicalized PCFG</span><span style='mso-font-kerning:0pt;mso-ansi-language:
EN-GB'> </span><span style='mso-font-kerning:0pt'>parsers in terms of labeled
bracketing f-score.</span><span style='mso-font-kerning:0pt;mso-ansi-language:
EN-GB'> </span><span style='mso-font-kerning:0pt'>Its results for the empty
category</span><span style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>prediction task and the trace-filler coindexation</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>task exceed all previously reported</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>results with 84.1% and 77.4% f-score,</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>respectively.<o:p></o:p></span></p>

<div style='border:none;border-bottom:solid windowtext 1.0pt;mso-border-bottom-alt:
solid windowtext .5pt;padding:0in 0in 1.0pt 0in'>

<h3>3B: Dialogue I</h3>

</div>

<p class=SessionChair>Session Chair: Stanley Peters<span style='mso-tab-count:
7'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b3B1></a>Learning More Effective Dialogue
Strategies Using Limited Dialogue Move Features</p>

<p class=AbstractAuthor>Matthew Frampton<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Frampton, M.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Oliver Lemon<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Lemon, O.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText><span style='mso-font-kerning:0pt'>We explore the use of
restricted dialogue contexts in reinforcement learning (RL) of effective
dialogue strategies for information seeking spoken dialogue systems (e.g.
COMMUNICATOR (Walker et al., 2001)). The contexts we use are richer than
previous research in this area, e.g. (Levin and Pieraccini, 1997; Schefer and
Young, 2001; Singh et al., 2002; Pietquin, 2004), which use only slot-based
information, but are much less complex than the full dialogue Information
States explored in (Henderson et al., 2005), for which tractable learning is an
issue. We explore how incrementally adding richer features allows learning of
more effective dialogue strategies. We use 2 user simulations learned from
COMMUNICATOR data (Walker et al., 2001; Georgila et al., 2005b) to explore the
effects of different features on learned dialogue strategies. Our results show
that adding the dialogue moves of the last system and user turns increases the
average reward of the automatically learned strategies by 65:9% over the
original (hand-coded) COMMUNICATOR systems, and by 7:8% over a baseline RL
policy that uses only slot-status features. We show that the learned strategies
exhibit an emergent focus switching strategy and effective use of the `give
help' action.<o:p></o:p></span></p>

<p class=AbstractTitle><a name=b3B2></a>Dependencies between Student State and
Speech Recognition Problems in Spoken Tutoring Dialogues</p>

<p class=AbstractAuthor>Mihai Rotaru<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Rotaru, M.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Diane J. Litman<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Litman, D.J.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText><span style='mso-font-kerning:0pt'>Speech recognition
problems are a reality in current spoken dialogue systems. In order to better
understand these phenomena, we study dependencies between speech recognition
problems and several higher level dialogue factors that define our notion of
student state: frustration/anger, certainty and correctness. We apply Chi
Square (?2) analysis to a corpus of speech-based computer tutoring dialogues to
discover these dependencies both within and across turns. Significant
dependencies are combined to produce interesting insights regarding speech
recognition problems and to propose new strategies for handling these problems.
We also find that tutoring, as a new domain for speech applications, exhibits
interesting tradeoffs and new factors to consider for spoken dialogue design.<o:p></o:p></span></p>

<p class=AbstractTitle><a name=b3B3></a>Learning the Structure of Task-driven
Human-Human Dialogs</p>

<p class=AbstractAuthor>Srinivas Bangalore<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Bangalore, S.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Giuseppe Di Fabbrizio<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Di Fabbrizio, G.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Amanda Stent<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Stent, A.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText><span style='mso-font-kerning:0pt'>Data-driven techniques
have been used for many computational linguistics tasks. Models derived from
data are generally more robust than hand-crafted systems since they better
reflect the distribution of the phenomena being modeled. With the availability
of large corpora of spoken dialog, dialog management is now reaping the
benefits of data-driven techniques. In this paper, we compare two approaches to
modeling subtask structure in dialog: a chunk-based model of subdialog
sequences, and a parse-based, or hierarchical, model. We evaluate these models
using customer agent dialogs from a catalog service domain.<o:p></o:p></span></p>

<div style='border:none;border-bottom:solid windowtext 1.0pt;mso-border-bottom-alt:
solid windowtext .5pt;padding:0in 0in 1.0pt 0in'>

<h3>3C: Machine Learning Methods I</h3>

</div>

<p class=SessionChair>Session Chair: Hal Daum&eacute; <st1:stockticker>III</st1:stockticker><span
style='mso-tab-count:7'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b3C1></a>Semi-Supervised Conditional Random
Fields for Improved Sequence Segmentation and Labeling</p>

<p class=AbstractAuthor>Feng Jiao<!--[if supportFields]><span style='mso-element:
field-begin'></span> XE &quot;Jiao, F.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Shaojun Wang<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Wang, S.(1)&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Chi-Hoon Lee<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Lee, C-H.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Russell Greiner<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Greiner, R.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Dale Schuurmans<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Schuurmans, D.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText>We present a new semi-supervised training procedure for<span
style='mso-ansi-language:EN-GB'> </span>conditional random fields (CRFs) that
can be used to train<span style='mso-ansi-language:EN-GB'> </span>sequence
segmentors and labelers from a combination of labeled and<span
style='mso-ansi-language:EN-GB'> </span>unlabeled training data. Our approach
is based on extending the<span style='mso-ansi-language:EN-GB'> </span>minimum entropy
regularization framework to the structured<span style='mso-ansi-language:EN-GB'>

</span>prediction case, yielding a training objective that combines<span
style='mso-ansi-language:EN-GB'> </span>unlabeled conditional entropy with
labeled conditional likelihood.<span style='mso-ansi-language:EN-GB'> </span>Although
the training objective is no longer concave, it can still<span
style='mso-ansi-language:EN-GB'> </span>be used to improve an initial model
(e.g. obtained from supervised<span style='mso-ansi-language:EN-GB'> </span>training)
by iterative ascent. We apply our new training algorithm<span style='mso-ansi-language:
EN-GB'> </span>to the problem of identifying gene and protein mentions in<span
style='mso-ansi-language:EN-GB'> </span>biological texts, and show that
incorporating unlabeled data<span style='mso-ansi-language:EN-GB'> </span>improves
the performance of the supervised CRF in this case.</p>

<p class=AbstractTitle><a name=b3C2></a>Training Conditional Random Fields with
Multivariate Evaluation Measures</p>

<p class=AbstractAuthor>Jun Suzuki<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Suzuki, J.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Erik McDermott<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;McDermott, E.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Hideki Isozaki<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Isozaki, H.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText><span lang=EN-GB style='mso-ansi-language:EN-GB'>This
paper proposes a framework for training Conditional Random Fields (CRFs) to
optimize multivariate evaluation measures, including non-linear measures such
as F-score. Our proposed framework is derived from an error minimization
approach that provides a simple solution for directly optimizing any evaluation
measure. Specifically focusing on sequential segmentation tasks, i.e. text
chunking and named entity recognition, we introduce a loss function which
closely reflects the target evaluation measure for these tasks, namely, segmentation
F-score. Our experiments show that our method performs better than standard CRF
training.<o:p></o:p></span></p>

<p class=AbstractTitle><a name=b3C3></a>Approximation Lasso Methods for
Language Modeling</p>

<p class=AbstractAuthor>Jianfeng Gao<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Gao, J.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Hisami Suzuki<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Suzuki, H.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Bin Yu<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Yu, B.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText>Lasso is a regularization method for parameter estimation
in linear models. It optimizes the model parameters with respect to a loss
function subject to model complexities. This paper explores the use of lasso
for statistical language modeling for text input. Owing to the very large
number of parameters, directly optimizing the penalized lasso loss function is
impossible. Therefore, we investigate two approximation methods, the boosted
lasso (BLasso) and the forward stagewise linear regression (FSLR). Both
methods, when used with the exponential loss function, bear strong resemblance
to the boosting algorithm which has been used as a discriminative training
method for language modeling. Evaluations on the task of Japanese text input
show that BLasso is able to produce the best approximation to the lasso
solution, and leads to a significant improvement, in terms of character error
rate, over boosting and the traditional maximum likelihood estimation.</p>

<div style='border:none;border-bottom:solid windowtext 1.0pt;mso-border-bottom-alt:
solid windowtext .5pt;padding:0in 0in 1.0pt 0in'>

<h3>3D: Applications I</h3>

</div>

<p class=SessionChair>Session Chair: John Prager<span style='mso-tab-count:
7'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b3D1></a>Automated Japanese Essay Scoring System
based on Articles Written by Experts</p>

<p class=AbstractAuthor>Tsunenori Ishioka<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Ishioka, T.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Masayuki Kameda<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Kameda, M.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText><span lang=EN-GB style='mso-ansi-language:EN-GB'>We have
developed an automated Japanese essay scoring system called Jess. The system
needs expert writings rather than expert raters to build the evaluation model.
By detecting statistical outliers of predetermined aimed essay features
compared with many professional writings for each prompt, our system can
evaluate essays. The following three features are examined: (1) rhetoric –
syntactic variety, or the use of various structures in the arrangement of
phases, clauses, and sentences, (2) organization – characteristics associated
with the orderly presentation of ideas, such as rhetorical features and
linguistic cues, and (3) content – vocabulary related to the topic, such as
relevant information and precise or specialized vocabulary. The final
evaluation score is calculated by deducting from a perfect score assigned by a
learning process using editorials and columns from the Mainichi Daily News
newspaper. A diagnosis for the essay is also given.<o:p></o:p></span></p>

<p class=AbstractTitle><a name=b3D2></a>A Feedback-Augmented Method for
Detecting Errors in the Writing of Learners of English</p>

<p class=AbstractAuthor>Ryo Nagata<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Nagata, R.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Atsuo Kawai<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Kawai, A.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Koichiro Morihiro<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Morihiro, K.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Naoki Isu<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Isu, N.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText>This paper proposes a method for detecting errors in
article usage and singular plural usage based on the mass count distinction.
First, it learns decision lists from training data generated automatically to
distinguish mass and count nouns. Then, in order to improve its performance, it
is augmented by feedback that is obtained from the writing of learners.
Finally, it detects errors by applying rules to the mass count distinction.
Experiments show that it achieves a recall of 0.71 and a precision of 0.72 and
outperforms other methods used for comparison when augmented by feedback.</p>

<p class=AbstractTitle><a name=b3D3></a>Correcting <st1:stockticker>ESL</st1:stockticker>
Errors Using Phrasal <st1:stockticker>SMT</st1:stockticker> Techniques</p>

<p class=AbstractAuthor>Chris Brockett<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Brockett, C.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, William B. Dolan<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Dolan, W.B.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Michael Gamon<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Gamon, M.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText><span style='mso-font-kerning:0pt'>This paper presents a
pilot study of the use of phrasal Statistical Machine Translation (</span><st1:stockticker><span
 style='mso-font-kerning:0pt'>SMT</span></st1:stockticker><span
style='mso-font-kerning:0pt'>) techniques to identify and correct writing
errors made by learners of English as a Second Language (</span><st1:stockticker><span
 style='mso-font-kerning:0pt'>ESL</span></st1:stockticker><span
style='mso-font-kerning:0pt'>). Using examples of mass noun errors found in the
Chinese Learner Error Corpus (</span><st1:stockticker><span style='mso-font-kerning:
 0pt'>CLEC</span></st1:stockticker><span style='mso-font-kerning:0pt'>) to
guide creation of an engineered training set, we show that application of the </span><st1:stockticker><span
 style='mso-font-kerning:0pt'>SMT</span></st1:stockticker><span
style='mso-font-kerning:0pt'> paradigm can capture errors not well addressed by
widely-used proofing tools designed for native speakers. Our system was able to
correct 61.81% of mistakes in a set of naturally- occurring examples of mass
noun errors found on the World Wide Web, suggesting that efforts to collect
alignable corpora of pre- and post-editing </span><st1:stockticker><span
 style='mso-font-kerning:0pt'>ESL</span></st1:stockticker><span
style='mso-font-kerning:0pt'> writing samples offer can enable the development
of </span><st1:stockticker><span style='mso-font-kerning:0pt'>SMT</span></st1:stockticker><span
style='mso-font-kerning:0pt'>-based writing assistance tools capable of
repairing many of the complex syntactic and lexical problems found in the
writing of </span><st1:stockticker><span style='mso-font-kerning:0pt'>ESL</span></st1:stockticker><span
style='mso-font-kerning:0pt'> learners.<o:p></o:p></span></p>

<div style='border:solid windowtext 1.0pt;mso-border-alt:solid windowtext .5pt;
padding:1.0pt 4.0pt 1.0pt 4.0pt'>

<h2>Monday 17th July 400pm–430pm</h2>

</div>

<div style='border:none;border-bottom:solid windowtext 1.0pt;mso-border-bottom-alt:
solid windowtext .5pt;padding:0in 0in 1.0pt 0in'>

<h3>4A: Parsing II</h3>

</div>

<p class=SessionChair>Session Chair: Joakim Nivre<span style='mso-tab-count:
7'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b4A1></a>Graph Transformations in Data-Driven
Dependency Parsing</p>

<p class=AbstractAuthor>Jens Nilsson<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Nilsson, J.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Joakim Nivre<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Nivre, J.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Johan Hall<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Hall, J.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText>Transforming syntactic representations in order to improve
parsing accuracy<span style='mso-ansi-language:EN-GB'> </span>has been
exploited successfully in statistical parsing systems using<span
style='mso-ansi-language:EN-GB'> </span>constituency-based representations. In
this paper, we show that similar<span style='mso-ansi-language:EN-GB'> </span>transformations
can give substantial improvements also in data-driven<span style='mso-ansi-language:
EN-GB'> </span>dependency parsing. Experiments on the Prague Dependency
Treebank show<span style='mso-ansi-language:EN-GB'> </span>that systematic
transformations of coordinate structures and verb groups result in a 10% error
reduction for a deterministic data-driven dependency parser. Combining these
transformations with previously proposed techniques for recovering
non-projective dependencies leads to state-of-the-art accuracy for the given
data set.</p>

<div style='border:none;border-bottom:solid windowtext 1.0pt;mso-border-bottom-alt:
solid windowtext .5pt;padding:0in 0in 1.0pt 0in'>

<h3>4B: Dialogue II</h3>

</div>

<p class=SessionChair>Session Chair: Stanley Peters<span style='mso-tab-count:
7'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b4B1></a>Learning to Generate Naturalistic
Utterances Using Reviews in Spoken Dialogue Systems</p>

<p class=AbstractAuthor>Ryuichiro Higashinaka<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Higashinaka, R.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Rashmi Prasad<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Prasad, R.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Marilyn A. Walker<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Walker, M.A.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText><span style='mso-font-kerning:0pt'>Spoken language
generation for dialogue systems requires a dictionary of mappings between
semantic representations of concepts the system wants to express and realizations
of those concepts. Dictionary creation is a costly process; it is currently
done by hand for each dialogue domain. We propose a novel unsupervised method
for learning such mappings from user reviews in the target domain, and test it
on restaurant reviews. We test the hypothesis that user reviews that provide
individual ratings for distinguished attributes of the domain entity make it
possible to map review sentences to their semantic representation with high
precision. Experimental analyses show that the mappings learned cover most of
the domain ontology, and provide good linguistic variation. A subjective user
evaluation shows that the consistency between the semantic representations and
the learned realizations is high and that the naturalness of the realizations
is higher than a hand-crafted baseline.<o:p></o:p></span></p>

<div style='border:none;border-bottom:solid windowtext 1.0pt;mso-border-bottom-alt:
solid windowtext .5pt;padding:0in 0in 1.0pt 0in'>

<h3>4C: Linguistic Kinships</h3>

</div>

<p class=SessionChair><a name=b4C1></a>Session Chair: Hal Daum&eacute; <st1:stockticker>III</st1:stockticker><span
style='mso-tab-count:7'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle>Measuring Language Divergence by Intra-Lexical
Comparison</p>

<p class=AbstractAuthor>T. Mark Ellison<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Ellison, T.M.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Simon Kirby<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Kirby, S.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText>This paper presents a method for building genetic language
taxonomies<span style='mso-ansi-language:EN-GB'> </span>based on a new approach
to comparing lexical forms. Instead of<span style='mso-ansi-language:EN-GB'> </span>comparing
forms cross-linguistically, a matrix of language-internal<span
style='mso-ansi-language:EN-GB'> </span>similarities between forms is
calculated. These matrices are then<span style='mso-ansi-language:EN-GB'> </span>compared
to give distances between languages. We argue that this<span style='mso-ansi-language:
EN-GB'> </span>coheres better with current thinking in linguistics and<span
style='mso-ansi-language:EN-GB'> </span>psycholinguistics. An implementation of
this approach, called<span style='mso-ansi-language:EN-GB'> </span>PHILOLOGICON,
is described, along with its application to Dyen et<span style='mso-ansi-language:
EN-GB'> </span>al.'s (1992) ninety-five wordlists from Indo-European languages.</p>

<div style='border:none;border-bottom:solid windowtext 1.0pt;mso-border-bottom-alt:
solid windowtext .5pt;padding:0in 0in 1.0pt 0in'>

<h3>4D: Applications II</h3>

</div>

<p class=SessionChair>Session Chair: John Prager<span style='mso-tab-count:
7'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b4D1></a>Enhancing electronic dictionaries with
an index based on associations</p>

<p class=AbstractAuthor>Olivier Ferret<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Ferret, O.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Michael Zock<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Zock, M.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText><span style='mso-font-kerning:0pt'>A good dictionary
contains not only</span><span style='mso-font-kerning:0pt;mso-ansi-language:
EN-GB'> </span><span style='mso-font-kerning:0pt'>many entries and a lot of
information</span><span style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>concerning each one of them, but also</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>adequate means to reveal the stored information.
Information access depends</span><span style='mso-font-kerning:0pt;mso-ansi-language:
EN-GB'> </span><span style='mso-font-kerning:0pt'>crucially on the quality of
the index. We</span><span style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'>
</span><span style='mso-font-kerning:0pt'>will present here some ideas of how a</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>dictionary could be enhanced to support a</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>speaker/writer to find the word s/he is</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>looking for. To this end we suggest to</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>add to an existing electronic resource an</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>index based on the notion of association.</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>We will also present preliminary work of</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>how a subset of such associations, for example,</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>topical associations, can be acquired</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>by filtering a network of lexical</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>co-occurrences extracted from a corpus.<o:p></o:p></span></p>

<div style='border:solid windowtext 1.0pt;mso-border-alt:solid windowtext .5pt;
padding:1.0pt 4.0pt 1.0pt 4.0pt'>

<h2>Tuesday 18th July 1000am–1030am</h2>

</div>

<div style='border:none;border-bottom:solid windowtext 1.0pt;mso-border-bottom-alt:
solid windowtext .5pt;padding:0in 0in 1.0pt 0in'>

<h3>5A: Parsing III</span></h3>

</div>

<p class=SessionChair>Session Chair: Dan Klein<span style='mso-tab-count:7'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b5A1></a>Guiding a Constraint Dependency Parser
with Supertags</p>

<p class=AbstractAuthor>K<span style='mso-fareast-font-family:SimSun;
mso-fareast-language:ZH-CN'>i</span>lian A. Foth<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;<span style='mso-fareast-font-family:
SimSun;mso-fareast-language:ZH-CN'>Foth, K.A.</span>&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Tomas By<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;<span style='mso-fareast-font-family:
SimSun;mso-fareast-language:ZH-CN'>By, T.</span>&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Wolfgang Menzel<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;<span style='mso-fareast-font-family:
SimSun;mso-fareast-language:ZH-CN'>Menzel, W.</span>&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText>We investigate the utility of supertag information for
guiding an existing<span style='mso-ansi-language:EN-GB'> </span>dependency
parser of German. Using weighted constraints to integrate the<span
style='mso-ansi-language:EN-GB'> </span>additionally available information, the
decision process of the parser is<span style='mso-ansi-language:EN-GB'> </span>influenced
by changing its preferences, without excluding alternative<span
style='mso-ansi-language:EN-GB'> </span>structural interpretations from being
considered. The paper reports on a series<span style='mso-ansi-language:EN-GB'>
</span>of experiments using varying models of supertags that significantly
increase<span style='mso-ansi-language:EN-GB'> </span>the parsing accuracy. In
addition, an upper bound on the accuracy that can be<span style='mso-ansi-language:
EN-GB'> </span>achieved with perfect supertags is estimated.</p>

<div style='border:none;border-bottom:solid windowtext 1.0pt;mso-border-bottom-alt:
solid windowtext .5pt;padding:0in 0in 1.0pt 0in'>

<h3>5B: Lexical Issues I</h3>

</div>

<p class=SessionChair>Session Chair: Chu Ren Huang<span style='mso-tab-count:
7'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b5B1></a>Efficient Unsupervised Discovery of
Word Categories Using Symmetric Patterns and High Frequency Words</p>

<p class=AbstractAuthor>Dmitry Davidov<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Davidov, D.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Ari Rappoport<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Rappoport, A.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText><span lang=EN-GB style='mso-ansi-language:EN-GB'>We present
a novel approach for discovering word categories, sets of words sharing a
significant aspect of their meaning. We utilize meta-patterns of high-frequency
words and content words in order to discover pattern candidates. Symmetric
patterns are then identified using graph-based measures, and word categories
are created based on graph clique sets. Our method is the first pattern-based
method that requires no corpus annotation or manually provided seed patterns or
words. We evaluate our algorithm on very large corpora in two languages, using
both human judgments and WordNet-based evaluation. Our fully unsupervised
results are superior to previous work that used a </span><st1:stockticker><span
 lang=EN-GB style='mso-ansi-language:EN-GB'>POS</span></st1:stockticker><span
lang=EN-GB style='mso-ansi-language:EN-GB'> tagged corpus, and computation time
for huge corpora are orders of magnitude faster than previously reported.<o:p></o:p></span></p>

<div style='border:none;border-bottom:solid windowtext 1.0pt;mso-border-bottom-alt:
solid windowtext .5pt;padding:0in 0in 1.0pt 0in'>

<h3>5C: Summarization I</h3>

</div>

<p class=SessionChair>Session Chair: Simone Teufel<span style='mso-tab-count:
7'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b5C1></a>Bayesian Query-Focused Summarization</p>

<p class=AbstractAuthor>Hal Daum&eacute; <st1:stockticker>III</st1:stockticker><!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Daum&eacute; III, H.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Daniel Marcu<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Marcu, D.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText>We present BayeSum (for &quot;Bayesian
summarization&quot;), a model for<span style='mso-ansi-language:EN-GB'> </span>sentence
extraction in query-focused summarization. BayeSum leverages<span
style='mso-ansi-language:EN-GB'> </span>the common case in which multiple
documents are relevant to a single<span style='mso-ansi-language:EN-GB'> </span>query.
Using these documents as reinforcement for query terms,<span style='mso-ansi-language:
EN-GB'> </span>BayeSum is not afflicted by the paucity of information in short
queries. We show that approximate inference in BayeSum is possible on<span
style='mso-ansi-language:EN-GB'> </span>large data sets and results in a
state-of-the-art summarization<span style='mso-ansi-language:EN-GB'> </span>system.
Furthermore, we show how BayeSum can be understood as a<span style='mso-ansi-language:
EN-GB'> </span>justified query expansion technique in the language modeling for
IR<span style='mso-ansi-language:EN-GB'> </span>framework.</p>

<div style='border:none;border-bottom:solid windowtext 1.0pt;mso-border-bottom-alt:
solid windowtext .5pt;padding:0in 0in 1.0pt 0in'>

<h3>5D: Semantics I</h3>

</div>

<p class=SessionChair>Session Chair: Johan Bos<span style='mso-tab-count:7'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b5D1></a>Expressing Implicit Semantic Relations
without Supervision</p>

<p class=AbstractAuthor>Peter D. Turney<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Turney, P.D.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoNormal><span lang=EN-GB style='mso-bidi-font-family:Arial;
mso-ansi-language:EN-GB'>We present an unsupervised learning algorithm that mines
large text corpora for patterns that express implicit semantic relations. F</span>o<span
lang=EN-GB style='mso-bidi-font-family:Arial;mso-ansi-language:EN-GB'>r a given
input word pair X:Y with some unspecified semantic relations, the corresponding
output list of patterns &lt;P1,...,Pm&gt; is ranked according to how well each
pattern Pi expresses the relations between X and Y. For example, given
X=ostrich and Y=bird, the two highest-ranking output patterns are &quot;X is
the largest Y&quot; and &quot;Y such as the X&quot;. The output patterns are
intended to be useful for finding further pairs with the same relations, to
support the construction of lexicons, ontologies, and semantic networks. The
patterns are sorted by pertinence, where the pertinence of a pattern Pi for a
word pair X:Y is the expected relational similarity between the given pair and
typical pairs for Pi. The algorithm is empirically evaluated on two tasks,
solving multiple-choice </span><st1:stockticker><span lang=EN-GB
 style='mso-bidi-font-family:Arial;mso-ansi-language:EN-GB'>SAT</span></st1:stockticker><span
lang=EN-GB style='mso-bidi-font-family:Arial;mso-ansi-language:EN-GB'> word
analogy questions and classifying semantic relations in noun-modifier pairs. On
both tasks, the algorithm achieves state-of-the-art results, performing significantly
better than several alternative pattern-ranking algorithms, based on tf-idf.</span><span
style='mso-tab-count:2'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<div style='border:solid windowtext 1.0pt;mso-border-alt:solid windowtext .5pt;
padding:1.0pt 4.0pt 1.0pt 4.0pt'>

<h2>Tuesday 18th July 1100am–1230pm</h2>

</div>

<div style='border:none;border-bottom:solid windowtext 1.0pt;mso-border-bottom-alt:
solid windowtext .5pt;padding:0in 0in 1.0pt 0in'>

<h3>6A: Parsing IV</h3>

</div>

<p class=SessionChair>Session Chair: Owen Rambow<span style='mso-tab-count:
7'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b6A1></a>Hybrid Parsing: Using Probabilistic
Models as Predictors for a Symbolic Parser</p>

<p class=AbstractAuthor>Kilian A. Foth<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Foth, K.A.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Wolfgang Menzel<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;<span style='mso-fareast-font-family:
SimSun;mso-fareast-language:ZH-CN'>Menzel, W.</span>&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText>In this paper we investigate the benefit of stochastic
predictor components for<span style='mso-ansi-language:EN-GB'> </span>the
parsing quality which can be obtained with a rule-based dependency grammar.<span
style='mso-ansi-language:EN-GB'> </span>By including a chunker, a supertagger,
a PP attacher, and a fast probabilistic<span style='mso-ansi-language:EN-GB'> </span>parser
we were able to improve upon the baseline by 3.2%, bringing the overall<span
style='mso-ansi-language:EN-GB'> </span>labelled accuracy to 91.1% on the
German NEGRA corpus. We attribute the<span style='mso-ansi-language:EN-GB'> </span>successful
integration to the ability of the underlying grammar model to<span
style='mso-ansi-language:EN-GB'> </span>combine uncertain evidence in a soft
manner, thus avoiding the problem of error<span style='mso-ansi-language:EN-GB'>

</span>propagation.<span style='mso-tab-count:5'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b6A2></a>Error mining in parsing results</p>

<p class=AbstractAuthor><span lang=FR style='mso-ansi-language:FR'>Beno&icirc;t Sagot</span><!--[if supportFields]><span
style='mso-element:field-begin'></span><span lang=FR style='mso-ansi-language:
FR'> XE &quot;Sagot, B.&quot; </span><![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span lang=FR
style='mso-ansi-language:FR'><span style='mso-spacerun:yes'>&nbsp;</span>and
&Eacute;ric de La Clergerie</span><!--[if supportFields]><span lang=FR
style='mso-ansi-language:FR'><span style='mso-element:field-begin'></span> XE
&quot;de La Clergerie, &Eacute;.&quot; </span><![endif]--><!--[if supportFields]><span
lang=FR style='mso-ansi-language:FR'><span style='mso-element:field-end'></span></span><![endif]--><span
lang=FR style='mso-ansi-language:FR'><o:p></o:p></span></p>

<p class=MsoBodyText><span lang=EN-GB style='mso-ansi-language:EN-GB'>We
introduce an error mining technique for automatically detecting errors in
resources that are used in parsing systems. We applied this technique on
parsing results produced on several million words by two distinct parsing
systems, which share the syntactic lexicon and the pre-parsing processing
chain. We were thus able to identify missing and erroneous information in these
resources.</span><span style='mso-tab-count:2'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b6A3></a>Reranking and Self-Training for Parser
Adaptation</p>

<p class=AbstractAuthor>David McClosky<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;McClosky, D.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Eugene Charniak<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Charniak, E.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Mark Johnson<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Johnson, M.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText><span lang=EN-GB style='mso-ansi-language:EN-GB'>Statistical
parsers trained and tested on the Penn Wall Street Journal (WSJ) treebank have
shown vast improvements over the last 10 years. Much of this improvement,
however, is based upon an ever-increasing number of features to be trained on
(typically) the WSJ treebank data. This has led to concern that such parsers
may be too finely tuned to this corpus at the expense of portability to other
genres. Such worries have merit. The standard &quot;Charniak parser&quot;
checks in at a labeled precision-recall f-measure of 89.7% on the Penn WSJ test
set, but only 82.9% on the test set from the Brown treebank corpus.<o:p></o:p></span></p>

<p class=MsoBodyText><span lang=EN-GB style='mso-ansi-language:EN-GB'>This
paper should allay these fears. In particular, we show that the reranking
parser described in Charniak and Johnson (2005) improves performance of the
parser on Brown to 85.2%.<span style='mso-spacerun:yes'>&nbsp;
</span>Furthermore, use of the self-training techniques described in (McClosky
et al. 2006) raise this to 87.8% (an error reduction of 28%) again without any
use of labeled Brown data.<span style='mso-spacerun:yes'>&nbsp; </span>This is
remarkable since training the parser and reranker on labeled Brown data
achieves only 88.4%.</span><span style='mso-tab-count:1'>&nbsp; </span></p>

<div style='border:none;border-bottom:solid windowtext 1.0pt;mso-border-bottom-alt:
solid windowtext .5pt;padding:0in 0in 1.0pt 0in'>

<h3>6B: Lexical Issues II</h3>

</div>

<p class=SessionChair>Session Chair: Chu Ren Huang<span style='mso-tab-count:
7'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b6B1></a>Automatic Classification of Verbs in
Biomedical Texts</p>

<p class=AbstractAuthor>Anna Korhonen<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Korhonen, A.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Yuval Krymolowski<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Krymolowski, Y.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Nigel Collier<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Collier, N.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText><span style='mso-font-kerning:0pt'>Lexical classes, when
tailored to the application and domain in question, can provide an effective
means to deal with a number of natural language processing (NLP) tasks. While
manual construction of such classes is difficult, recent research shows that it
is possible to automatically induce verb classes from cross-domain corpora with
promising accuracy. We report a novel experiment where similar technology is
applied to the important, challenging domain of biomedicine. We show that the
resulting classification, acquired from a corpus of biomedical journal
articles, is highly accurate and strongly domain specific. It can be used to
aid </span><st1:stockticker><span style='mso-font-kerning:0pt'>BIO</span></st1:stockticker><span
style='mso-font-kerning:0pt'>-NLP directly or as useful material for
investigating the syntax and semantics of verbs in biomedical texts.</span></p>

<p class=AbstractTitle><a name=b6B2></a>Selection of Effective Contextual
Information for Automatic Synonym Acquisition </p>

<p class=AbstractAuthor>Masato Hagiwara<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Hagiwara, M.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Yasuhiro Ogawa<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Ogawa, Y.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Katsuhiko Toyama<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Toyama, K.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText><span style='mso-font-kerning:0pt'>Various methods </span>have
been<span style='mso-font-kerning:0pt'> proposed for automatic synonym
acquisition, as synonyms are one of the most fundamental lexical knowledge.
Whereas many methods are based on contextual clues of words, little attention
has been paid to what kind of categories of contextual information are useful
for the purpose. This study has experimentally investigated the impact of
contextual information selection, by extracting three kinds of word
relationships from corpora: dependency, sentence co-occurrence, and proximity.
The evaluation result shows that while dependency and proximity perform
relatively well by themselves, combination of two or more kinds of contextual
information gives more stable performance. We’ve further investigated useful
selection of dependency relations and modification categories, and it is found
that modification has the greatest contribution, even greater than the widely
adopted subject object combination.</span><span style='mso-tab-count:6'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b6B3></a>Scaling Distributional Similarity to
Large Corpora</p>

<p class=AbstractAuthor>James Gorman<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Gorman, J.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and James R. Curran<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Curran, J.R.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText><span style='mso-font-kerning:0pt'>Accurately representing
synonymy using distributional similarity requires large volumes of data to
reliably represent infrequent words. However, the naive nearest-neighbour
approach to comparing context vectors extracted from large corpora scales
poorly (O (n2) in the vocabulary size).<o:p></o:p></span></p>

<p class=MsoBodyText><span style='mso-font-kerning:0pt'>In this paper, we
compare several existing approaches to approximating the nearest-neighbour
search for distributional similarity. We investigate the trade-off between efficiency
and accuracy, and find that SASH (Houle and Sakuma, 2005) provides the best
balance.</span><span style='mso-tab-count:6'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<div style='border:none;border-bottom:solid windowtext 1.0pt;mso-border-bottom-alt:
solid windowtext .5pt;padding:0in 0in 1.0pt 0in'>

<h3>6C: Summarization II</h3>

</div>

<p class=SessionChair>Session Chair: Simone Teufel<span style='mso-tab-count:
7'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b6C1></a>Extractive Summarization using Inter-
and Intra- Event Relevance</p>

<p class=AbstractAuthor>Wenjie Li<!--[if supportFields]><span style='mso-element:
field-begin'></span> XE &quot;Li, W.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Mingli Wu<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Wu, M.(2)&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Qin Lu<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Lu, Q.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Wei Xu<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Xu, W.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Chunfa Yuan<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Yuan, C.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText>Event-based summarization attempts to select and organize
sentences in a summary with respect to events or sub-events that the sentences
describe. Each<span style='mso-ansi-language:EN-GB'> </span>event has its own
internal structure and meanwhile relates to the other events<span
style='mso-ansi-language:EN-GB'> </span>semantically, temporally, spatially,
causally or conditionally. In this paper,<span style='mso-ansi-language:EN-GB'>
</span>we define an event as one or more event terms along with the named
entities<span style='mso-ansi-language:EN-GB'> </span>associated, and present a
novel approach to derive intra- and inter- event<span style='mso-ansi-language:
EN-GB'> </span>relevance using the information of internal association,
semantic related-ness,<span style='mso-ansi-language:EN-GB'> </span>distributional
similarity and named entity clustering. We then apply PageRank<span
style='mso-ansi-language:EN-GB'> </span>ranking algorithm to estimate the
significance of an event for inclusion in a<span style='mso-ansi-language:EN-GB'>

</span>summary from the event relevance derived. Experiments on the DUC 2001
test<span style='mso-ansi-language:EN-GB'> </span>data shows that the relevance
of the named entities involved in events achieves better result when their
relevance is derived from the event terms they associate. It also reveals that
the topic-specific from documents themselves outperforms the semantic relevance
from a general purpose knowledge<span style='mso-ansi-language:EN-GB'> </span>base
like Word-Net.</p>

<p class=AbstractTitle><a name=b6C2></a>Models for Sentence Compression: A
Comparison across Domains, Training Requirements and Evaluation Measures</p>

<p class=AbstractAuthor>James Clarke<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Clarke, J.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Mirella Lapata<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Lapata, M.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText><span lang=EN-GB style='mso-font-kerning:0pt;mso-ansi-language:
EN-GB'>Sentence compression is the task of producing a summary at the sentence
level.<span style='mso-spacerun:yes'>&nbsp; </span>This paper focuses on three
aspects of this task which have not received detailed treatment in the
literature: training requirements, scalability, and automatic evaluation. We
provide a novel comparison between a supervised constituent-based and a weakly
supervised word-based compression algorithm and examine how these models port
to different domains (written vs. spoken text).<span
style='mso-spacerun:yes'>&nbsp; </span>To achieve this, a human-authored
compression corpus has been created and our study highlights potential problems
with the automatically gathered compression corpora currently used. Finally, we
assess whether automatic evaluation measures can be used to determine
compression quality.</span><span style='mso-tab-count:1'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b6C3></a>A Bottom-up Approach to Sentence
Ordering for Multi-document Summarization</p>

<p class=AbstractAuthor>Danushka Bollegala<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Bollegala, D.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Naoaki Okazaki<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Okazaki, N.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Mitsuru Ishizuka<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Ishizuka, M.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText>Ordering information is a difficult but important task for
applications generating natural-language text. We present a bottom-up approach
to arranging sentences extracted for multi-document summarization. To capture
the association and order of two textual segments (eg, sentences), we define
four criteria, chronology, topical-closeness, precedence, and succession. These
criteria are integrated into a criterion by a supervised learning approach. We
repeatedly concatenate two textual segments into one segment based on the
criterion until we obtain the overall segment with all sentences arranged. Our
experimental results show a significant improvement over existing sentence
ordering strategies.<span style='mso-tab-count:6'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<div style='border:none;border-bottom:solid windowtext 1.0pt;mso-border-bottom-alt:
solid windowtext .5pt;padding:0in 0in 1.0pt 0in'>

<h3>6D: Semantics II</h3>

</div>

<p class=SessionChair>Session Chair: Johan Bos<span style='mso-tab-count:7'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b6D1></a>Learning Event Durations from Event
Descriptions</p>

<p class=AbstractAuthor>Feng Pan<!--[if supportFields]><span style='mso-element:
field-begin'></span> XE &quot;Pan, F.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Rutu Mulkar<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Mulkar, R.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Jerry R. Hobbs<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Hobbs, J.R.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText>We have constructed a corpus of news articles in which
events are annotated for estimated bounds on their durations. Here we describe
a method for measuring inter-annotator agreement for these event duration
distributions. We then show that machine learning techniques applied to this
data yield coarse-grained event duration information, considerably outperforming
a baseline and approaching human performance.</p>

<p class=AbstractTitle><a name=b6D2></a>Automatic learning of textual
entailments with cross-pair similarities</p>

<p class=AbstractAuthor>Fabio Massimo Zanzotto<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Zanzotto, F.M.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Alessandro Moschitti<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Moschitti, A.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText><span lang=EN-GB style='mso-ansi-language:EN-GB'>In this
paper we define a novel similarity measure between examples of textual
entailments and we use it as a kernel function in Support Vector Machines
(SVMs). This allows us to automatically learn the rewrite rules that describe a
non trivial set of entailment cases. The experiments with the data sets of the
RTE 2005 challenge show an improvement of 4.4% over the state-of-the-art
methods.</span><span style='mso-tab-count:2'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b6D3></a>An Improved Redundancy Elimination
Algorithm for Underspecified Representations</p>

<p class=AbstractAuthor>Alexander Koller<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;<span style='mso-fareast-font-family:
SimSun;mso-fareast-language:ZH-CN'>Koller, A.</span>&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Stefan Thater<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Thater<span style='mso-fareast-font-family:
SimSun;mso-fareast-language:ZH-CN'>, S.</span>&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText><span lang=EN-GB style='mso-ansi-language:EN-GB'>We
present an efficient algorithm for the redundancy elimination problem: Given an
underspecified semantic representation (USR) of a scope ambiguity, compute an
USR with fewer mutually equivalent readings. The algorithm operates on
underspecified chart representations which are derived from dominance graphs;
it can be applied to the USRs computed by large-scale grammars. We evaluate the
algorithm on a corpus, and show that it reduces the degree of ambiguity
significantly while taking negligible runtime.</span></p>

<div style='border:solid windowtext 1.0pt;mso-border-alt:solid windowtext .5pt;
padding:1.0pt 4.0pt 1.0pt 4.0pt'>

<h2>Tuesday 18th July 200pm–330pm</h2>

</div>

<div style='border:none;border-bottom:solid windowtext 1.0pt;mso-border-bottom-alt:
solid windowtext .5pt;padding:0in 0in 1.0pt 0in'>

<h3>7A: Parsing V</h3>

</div>

<p class=SessionChair>Session Chair: Takashi Ninomiya<span style='mso-tab-count:
7'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b7A1></a>Integrating Syntactic Priming into an
Incremental Probabilistic Parser, with an Application to Psycholinguistic
Modeling</p>

<p class=MsoBodyText><span style='mso-font-kerning:0pt'>Amit Dubey</span><!--[if supportFields]><span
style='mso-font-kerning:0pt'><span style='mso-element:field-begin'></span></span>
XE &quot;<span style='mso-font-kerning:0pt'>Dubey, A.</span>&quot; <![endif]--><!--[if supportFields]><span
style='mso-font-kerning:0pt'><span style='mso-element:field-end'></span></span><![endif]--><span
style='mso-font-kerning:0pt'>, Frank Keller</span><!--[if supportFields]><span
style='mso-font-kerning:0pt'><span style='mso-element:field-begin'></span></span>
XE &quot;<span style='mso-font-kerning:0pt'>Keller, F.</span>&quot; <![endif]--><!--[if supportFields]><span
style='mso-font-kerning:0pt'><span style='mso-element:field-end'></span></span><![endif]--><span
style='mso-font-kerning:0pt'><span style='mso-spacerun:yes'>&nbsp;</span>and
Patrick Sturt</span><!--[if supportFields]><span style='mso-font-kerning:0pt'><span
style='mso-element:field-begin'></span></span> XE &quot;<span style='mso-font-kerning:
0pt'>Sturt, P.</span>&quot; <![endif]--><!--[if supportFields]><span
style='mso-font-kerning:0pt'><span style='mso-element:field-end'></span></span><![endif]--><span
style='mso-font-kerning:0pt'><o:p></o:p></span></p>

<p class=MsoBodyText>The psycholinguistic literature provides evidence for
syntactic priming, i.e., the tendency to repeat structures.<span
style='mso-spacerun:yes'>&nbsp; </span>This paper describes a method for incorporating
priming into an incremental probabilistic parser. Three models are compared,
which involve priming of rules between sentences, within sentences, and within
coordinate structures. These models simulate the reading time advantage for
parallel structures found in human data, and also yield a small increase in
overall parsing accuracy.<span style='mso-tab-count:5'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b7A2></a>A Fast, Accurate Deterministic Parser
for Chinese</p>

<p class=AbstractAuthor>Mengqiu Wang<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Wang, M.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Kenji Sagae<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Sagae, K.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Teruko Mitamura<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Mitamura, T.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText>We present a novel classifier-based deterministic parser
for Chinese constituency parsing. Our parser computes parse trees from bottom
up in one pass, and uses classifiers to make shift-reduce decisions. Trained
and evaluated on the standard training and test sets, our best model (using
stacked classifiers) runs in linear time and has labeled precision and recall
above 88% using gold-standard part-of-speech tags, surpassing the best
published results. Our <st1:stockticker>SVM</st1:stockticker> parser is 2-13
times faster than state-of-the-art parsers, while producing more accurate
results. Our Maxent and DTree parsers run at speeds 40-270 times faster than
state-of-the-art parsers, but with 5-6% losses in accuracy.</p>

<p class=AbstractTitle><a name=b7A3></a>Learning Accurate, Compact, and
Interpretable Tree Annotation</p>

<p class=MsoBodyText><span style='mso-font-kerning:0pt'>Slav Petrov</span><!--[if supportFields]><span
style='mso-font-kerning:0pt'><span style='mso-element:field-begin'></span></span>
XE &quot;<span style='mso-font-kerning:0pt'>Petrov, S.</span>&quot; <![endif]--><!--[if supportFields]><span
style='mso-font-kerning:0pt'><span style='mso-element:field-end'></span></span><![endif]--><span
style='mso-font-kerning:0pt'>, Leon Barrett</span><!--[if supportFields]><span
style='mso-font-kerning:0pt'><span style='mso-element:field-begin'></span></span>
XE &quot;<span style='mso-font-kerning:0pt'>Barrett, L.</span>&quot; <![endif]--><!--[if supportFields]><span
style='mso-font-kerning:0pt'><span style='mso-element:field-end'></span></span><![endif]--><span
style='mso-font-kerning:0pt'>, Romain Thibaux</span><!--[if supportFields]><span
style='mso-font-kerning:0pt'><span style='mso-element:field-begin'></span></span>
XE &quot;<span style='mso-font-kerning:0pt'>Thibaux, R.</span>&quot; <![endif]--><!--[if supportFields]><span
style='mso-font-kerning:0pt'><span style='mso-element:field-end'></span></span><![endif]--><span
style='mso-font-kerning:0pt'><span style='mso-spacerun:yes'>&nbsp;</span>and
Dan Klein</span><!--[if supportFields]><span style='mso-font-kerning:0pt'><span
style='mso-element:field-begin'></span></span> XE &quot;<span style='mso-font-kerning:
0pt'>Klein, D.</span>&quot; <![endif]--><!--[if supportFields]><span
style='mso-font-kerning:0pt'><span style='mso-element:field-end'></span></span><![endif]--><span
style='mso-font-kerning:0pt'><o:p></o:p></span></p>

<p class=MsoBodyText><span style='mso-font-kerning:0pt'>We present an automatic
approach to tree annotation in which basic nonterminal symbols are alternately
split and merged to maximize the likelihood of a training treebank. Starting
with a simple Xbar grammar, we learn a new grammar whose nonterminals are
subsymbols of the original nonterminals. In contrast with previous work, we are
able to split various terminals to different degrees, as appropriate to the
actual complexity in the data. Our grammars automatically learn the kinds of
linguistic distinctions exhibited in previous work on manual tree annotation.
On the other hand, our grammars are much more compact and substantially more
accurate than previous work on automatic annotation. Despite its simplicity,
our best grammar achieves an F<sub>1</sub> of 90.2% on the Penn Treebank,
higher than fully lexicalized systems.</span><span style='mso-tab-count:6'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<div style='border:none;border-bottom:solid windowtext 1.0pt;mso-border-bottom-alt:
solid windowtext .5pt;padding:0in 0in 1.0pt 0in'>

<h3>7B: Word Sense Disambiguation II</h3>

</div>

<p class=SessionChair>Session Chair: Hwee Tou Ng<span style='mso-tab-count:
7'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b7B1></a>Semi-Supervised Learning of Partial
Cognates using Bilingual Bootstrapping</p>

<p class=AbstractAuthor>Oana Frunza<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Frunza, O.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Diana Inkpen<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Inkpen, D.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText>Partial cognates are pairs of words in two languages that
have the same meaning in some, but not all contexts. Detecting the actual
meaning of a partial cognate in con-text can be useful for Machine Translation
tools and for Computer-Assisted Language Learning tools. In this paper we
propose a supervised and a semi-supervised method to disambiguate partial
cognates between two languages: French and English. The methods use only
automatically-labeled data; therefore they can be applied for other pairs of
languages as well. We also show that our methods perform well when using
corpora from different domains.<span style='mso-tab-count:2'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span
style='mso-tab-count:4'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b7B2></a>Direct Word Sense Matching for Lexical
Substitution</p>

<p class=AbstractAuthor>Ido Dagan<!--[if supportFields]><span style='mso-element:
field-begin'></span> XE &quot;Dagan, I.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Oren Glickman<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Glickman, O.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Alfio Gliozzo<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Gliozzo, A.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Efrat Marmorshtein<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Marmorshtein, E.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Carlo Strapparava<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Strapparava, C.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText><span style='mso-font-kerning:0pt'>This paper investigates
conceptually and empirically the novel sense matching task, which requires to
recognize whether the senses of two synonymous words match in context. We
suggest direct approaches to the problem, which avoid the intermediate step of
explicit word sense disambiguation, and demonstrate their appealing advantages
and stimulating potential for future research.<o:p></o:p></span></p>

<p class=AbstractTitle><a name=b7B3></a>An Equivalent Pseudoword for
Unsupervised Chinese Word Sense Disambiguation</p>

<p class=AbstractAuthor>Zhimao Lu<!--[if supportFields]><span style='mso-element:
field-begin'></span> XE &quot;Lu, Z.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Haifeng Wang<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Wang, H.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Jianmin Yao<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Yao, J.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Ting Liu<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Liu, T.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Sheng Li<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Li, S.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText>This paper presents a new approach based on Equivalent
Pseudowords (EPs) to tackle Word Sense Disambiguation (WSD) in Chinese
language. EPs are particular artificial ambiguous words, which can be used to realize
unsupervised WSD. A Bayesian classifier is implemented to test the efficacy of
the EP solution on Senseval-3 Chinese test set. The performance is better than
state-of-the-art results with an average F-measure of 0.80. The experiment
verifies the value of EP for unsupervised WSD.</p>

<div style='border:none;border-bottom:solid windowtext 1.0pt;mso-border-bottom-alt:
solid windowtext .5pt;padding:0in 0in 1.0pt 0in'>

<h3>7C: Information Extraction II</h3>

</div>

<p class=SessionChair>Session Chair: Ming Zhou<span style='mso-tab-count:7'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b7C1></a>Improving the Scalability of
Semi-Markov Conditional Random Fields for Named Entity Recognition</p>

<p class=AbstractAuthor>Daisuke Okanohara<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Okanohara, D.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Yusuke Miyao<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Miyao, Y.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Yoshimasa Tsuruoka<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Tsuruoka, Y.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Jun'ichi Tsujii<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Tsujii, J.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText><span lang=EN-GB style='mso-ansi-language:EN-GB'>This
paper presents techniques to apply semi-CRFs to Named Entity Recognition tasks
with a tractable computational cost. Our framework can handle an </span><st1:stockticker><span
 lang=EN-GB style='mso-ansi-language:EN-GB'>NER</span></st1:stockticker><span
lang=EN-GB style='mso-ansi-language:EN-GB'> task that has long named entities
and many labels which increase the computational cost. To reduce the
computational cost, we propose two techniques: the first is the use of feature
forests, which enables us to pack feature-equivalent states, and the second is
the introduction of a filtering process which significantly reduces the number
of candidate states. This framework allows us to use a rich set of features
extracted from the chunk-based representation that can capture informative
characteristics of entities. We also introduce a simple trick to transfer
information about distant entities by embedding label information into
non-entity labels. Experimental results show that our model achieves an F-score
of 71.48% on the JNLPBA 2004 shared task without using any external resources
or post-processing techniques.</span><span style='mso-tab-count:1'>&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b7C2></a>Factorizing Complex Models: A Case
Study in Mention Detection</p>

<p class=MsoBodyText><span style='mso-font-kerning:0pt'>Radu Florian</span><!--[if supportFields]><span
style='mso-font-kerning:0pt'><span style='mso-element:field-begin'></span></span>
XE &quot;<span style='mso-font-kerning:0pt'>Florian, R.</span>&quot; <![endif]--><!--[if supportFields]><span
style='mso-font-kerning:0pt'><span style='mso-element:field-end'></span></span><![endif]--><span
style='mso-font-kerning:0pt'>, Hongyan Jing</span><!--[if supportFields]><span
style='mso-font-kerning:0pt'><span style='mso-element:field-begin'></span></span>
XE &quot;<span style='mso-font-kerning:0pt'>Jing, H.</span>&quot; <![endif]--><!--[if supportFields]><span
style='mso-font-kerning:0pt'><span style='mso-element:field-end'></span></span><![endif]--><span
style='mso-font-kerning:0pt'>, Nanda Kambhatla</span><!--[if supportFields]><span
style='mso-font-kerning:0pt'><span style='mso-element:field-begin'></span></span>
XE &quot;<span style='mso-font-kerning:0pt'>Kambhatla, N.</span>&quot; <![endif]--><!--[if supportFields]><span
style='mso-font-kerning:0pt'><span style='mso-element:field-end'></span></span><![endif]--><span
style='mso-font-kerning:0pt'><span style='mso-spacerun:yes'>&nbsp;</span>and
Imed Zitouni</span><!--[if supportFields]><span style='mso-font-kerning:0pt'><span
style='mso-element:field-begin'></span></span> XE &quot;<span style='mso-font-kerning:
0pt'>Zitouni, I.</span>&quot; <![endif]--><!--[if supportFields]><span
style='mso-font-kerning:0pt'><span style='mso-element:field-end'></span></span><![endif]--><span
style='mso-font-kerning:0pt'><o:p></o:p></span></p>

<p class=MsoBodyText><span style='mso-font-kerning:0pt'>As natural language
processing moves towards natural language understanding, the tasks are becoming
more and more subtle: we are interested in more nuanced word characteristics,
more linguistic properties, more semantic and syntactic features. One such example,
which we consider in this article, is the mention detection in the ACE project
(NIST, 2004), where the goal is to identify named, nominal or pronominal
references to real-world entities – mentions – and label them with three types
of information: entity type, entity subtype and mention type. In this article,
we investigate several methods to assign these related tags and compare them on
several data sets. A system based on the methods presented in this article
ranked very well in the ACE’04 evaluation.</span><span style='mso-tab-count:
3'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b7C3></a>Segment-based Hidden Markov Models for
Information Extraction</p>

<p class=AbstractAuthor><span lang=FR style='mso-ansi-language:FR'>Zhenmei Gu</span><!--[if supportFields]><span
style='mso-element:field-begin'></span><span lang=FR style='mso-ansi-language:
FR'> XE &quot;Gu, Z.&quot; </span><![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span lang=FR
style='mso-ansi-language:FR'><span style='mso-spacerun:yes'>&nbsp;</span>and
Nick Cercone</span><!--[if supportFields]><span style='mso-element:field-begin'></span><span
lang=FR style='mso-ansi-language:FR'> XE &quot;Cercone, N.&quot; </span><![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span lang=FR
style='mso-ansi-language:FR'><o:p></o:p></span></p>

<p class=MsoBodyText><span style='mso-font-kerning:0pt'>Hidden Markov models
(HMMs) are powerful statistical models that have found successful applications in
Information Extraction (IE). In current approaches to applying HMMs to IE, an
HMM is used to model text at the document level. This modeling might cause
undesired redundancy in extraction in the sense that more than one filler is
identified and extracted. We propose to use HMMs to model text at the segment
level, in which the extraction process consists of two steps: a segment
retrieval step followed by an extraction step. In order to retrieve extraction
relevant segments from documents, we introduce a method to use HMMs to model
and retrieve segments. Our experimental results show that the resulting segment
HMM IE system not only achieves near zero extraction redundancy, but also has
better overall extraction performance than traditional document HMM IE systems.<o:p></o:p></span></p>

<div style='border:none;border-bottom:solid windowtext 1.0pt;mso-border-bottom-alt:
solid windowtext .5pt;padding:0in 0in 1.0pt 0in'>

<h3>7D: Resources I</h3>

</div>

<p class=SessionChair>Session Chair: Erhard Hinrichs<span style='mso-tab-count:
7'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b7D1></a>A <st1:stockticker>DOM</st1:stockticker>
Tree Alignment Model for Mining Parallel Data from the Web</p>

<p class=AbstractAuthor>Lei Shi<!--[if supportFields]><span style='mso-element:
field-begin'></span> XE &quot;Shi, L.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Cheng Niu<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Niu, C.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Ming Zhou<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Zhou, M.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Jianfeng Gao<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Gao, J.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText><span style='mso-font-kerning:0pt'>This paper presents a
new web mining</span><span style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'>
</span><span style='mso-font-kerning:0pt'>scheme for parallel data acquisition.</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>Based on the Document Object Model</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>(</span><st1:stockticker><span style='mso-font-kerning:
 0pt'>DOM</span></st1:stockticker><span style='mso-font-kerning:0pt'>), a web
page is represented as a</span><span style='mso-font-kerning:0pt;mso-ansi-language:
EN-GB'> </span><st1:stockticker><span style='mso-font-kerning:0pt'>DOM</span></st1:stockticker><span
style='mso-font-kerning:0pt'> tree. Then a </span><st1:stockticker><span
 style='mso-font-kerning:0pt'>DOM</span></st1:stockticker><span
style='mso-font-kerning:0pt'> tree alignment</span><span style='mso-font-kerning:
0pt;mso-ansi-language:EN-GB'> </span><span style='mso-font-kerning:0pt'>model
is proposed to identify the translationally</span><span style='mso-font-kerning:
0pt;mso-ansi-language:EN-GB'> </span><span style='mso-font-kerning:0pt'>equivalent
texts and hyperlinks</span><span style='mso-font-kerning:0pt;mso-ansi-language:
EN-GB'> </span><span style='mso-font-kerning:0pt'>between two parallel </span><st1:stockticker><span
 style='mso-font-kerning:0pt'>DOM</span></st1:stockticker><span
style='mso-font-kerning:0pt'> trees. By</span><span style='mso-font-kerning:
0pt;mso-ansi-language:EN-GB'> </span><span style='mso-font-kerning:0pt'>tracing
the identified parallel hyperlinks, parallel web documents are recursively</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>mined. Compared with previous mining</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>schemes, the benchmarks show that this</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>new mining scheme improves the mining</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>coverage, reduces mining bandwidth, and</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>enhances the quality of mined parallel</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>sentences.<o:p></o:p></span></p>

<p class=AbstractTitle><a name=b7D2></a>QuestionBank: Creating a Corpus of
Parse-Annotated Questions </p>

<p class=AbstractAuthor>John Judge<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Judge, J.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Aoife Cahill<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Cahill, A.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Josef van Genabith<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;van Genabith, J.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText><span style='mso-font-kerning:0pt'>This paper describes
the development of QuestionBank, a corpus of 4000 parse annotated questions for
(i) use in training parsers employed in QA, and (ii) evaluation of question
parsing. We present a series of experiments to investigate the effectiveness of
QuestionBank as both an exclusive and supplementary training resource for a
state-of-the-art parser in parsing both question and non-question test sets. We
introduce a new method for recovering empty nodes and their antecedents
(capturing long distance dependencies) from parser output in CFG trees using </span><st1:stockticker><span
 style='mso-font-kerning:0pt'>LFG</span></st1:stockticker><span
style='mso-font-kerning:0pt'> f-structure reentrancies. Our main findings are
(i) using QuestionBank training data improves parser performance to 89.75%
labelled bracketing f-score, an increase of almost 11% over the baseline; (ii)
back-testing experiments on nonquestion data (Penn-II WSJ Section 23) shows
that the retrained parser does not suffer a performance drop on non-question
material; (iii) ablation experiments show that the size of training material
provided by QuestionBank is sufficient to achieve optimal results; (iv) our
method for recovering empty nodes captures long distance dependencies in
questions from the </span><st1:stockticker><span style='mso-font-kerning:0pt'>ATIS</span></st1:stockticker><span
style='mso-font-kerning:0pt'> corpus with high precision (96.82%) and low
recall (39.38%). In summary, QuestionBank provides a useful new resource in
parser-based QA research.</span><span style='mso-tab-count:6'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b7D3></a>Creating a CCGbank and a wide-coverage <st1:stockticker>CCG</st1:stockticker>
lexicon for German</p>

<p class=AbstractAuthor>Julia Hockenmaier<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Hockenmaier, J.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText><span style='mso-font-kerning:0pt'>We present an algorithm
which creates a German CCGbank by translating the syntax graphs in the German
Tiger corpus into </span><st1:stockticker><span style='mso-font-kerning:0pt'>CCG</span></st1:stockticker><span
style='mso-font-kerning:0pt'> derivation trees. The resulting corpus contains
46,628 derivations, covering 95% of all complete sentences in Tiger. Lexicons
extracted from this corpus contain correct lexical entries for 94% of all known
tokens in unseen text.<o:p></o:p></span></p>

<div style='border:solid windowtext 1.0pt;mso-border-alt:solid windowtext .5pt;
padding:1.0pt 4.0pt 1.0pt 4.0pt'>

<h2>Tuesday 18th July 400pm–530pm</h2>

</div>

<div style='border:none;border-bottom:solid windowtext 1.0pt;mso-border-bottom-alt:
solid windowtext .5pt;padding:0in 0in 1.0pt 0in'>

<h3>8A: Machine Translation III</h3>

</div>

<p class=SessionChair>Session Chair: Kevin Knight<span style='mso-tab-count:
7'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b8A1></a>Improved Discriminative Bilingual Word
Alignment</p>

<p class=AbstractAuthor>Robert C. Moore<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Moore, R.C.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Wen-tau Yih<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Yih, W.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Andreas Bode<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Bode, A.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText>For many years, statistical machine translation relied on
generative models to provide bilingual word alignments.<span
style='mso-spacerun:yes'>&nbsp; </span>In 2005, several independent efforts
showed that discriminative models could be used to enhance or replace the
standard generative approach.<span style='mso-spacerun:yes'>&nbsp;
</span>Building on this work, we demonstrate substantial improvement in
word-alignment accuracy, partly though improved training methods, but
predominantly through selection of more and better features.<span
style='mso-spacerun:yes'>&nbsp; </span>Our best model produces the lowest
alignment error rate yet reported on Canadian Hansard’s bilingual data.</p>

<p class=AbstractTitle><a name=b8A2></a>Maximum Entropy Based Phrase Reordering
Model for Statistical Machine Translation</p>

<p class=AbstractAuthor>Deyi Xiong<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Xiong, D.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Qun Liu<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Liu, Q.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Shouxun Lin<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Lin, S.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText>We propose a novel reordering model for<span
style='mso-ansi-language:EN-GB'> </span>phrase-based statistical machine translation
(<st1:stockticker>SMT</st1:stockticker>) that uses a maximum entropy<span
style='mso-ansi-language:EN-GB'> </span>(MaxEnt) model to predicate reorderings<span
style='mso-ansi-language:EN-GB'> </span>of neighbor blocks (phrase pairs). The<span
style='mso-ansi-language:EN-GB'> </span>model provides content-dependent,
hierarchical<span style='mso-ansi-language:EN-GB'> </span>phrasal reordering
with generalization<span style='mso-ansi-language:EN-GB'> </span>based on
features automatically<span style='mso-ansi-language:EN-GB'> </span>learned from
a real-world bitext. We<span style='mso-ansi-language:EN-GB'> </span>present an
algorithm to extract all reordering<span style='mso-ansi-language:EN-GB'> </span>events
of neighbor blocks from bilingual<span style='mso-ansi-language:EN-GB'> </span>data.
In our experiments on Chinese-to-English translation, this MaxEnt-based<span
style='mso-ansi-language:EN-GB'> </span>reordering model obtains significant
improvements<span style='mso-ansi-language:EN-GB'> </span>in BLEU score on the
NIST MT-05 and IWSLT-04 tasks.<span style='mso-tab-count:1'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span
style='mso-tab-count:5'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b8A3></a>Distortion Models for Statistical
Machine Translation</p>

<p class=AbstractAuthor>Yaser Al-Onaizan<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Al-Onaizan, Y.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Kishore Papineni<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Papineni, K.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText>In this paper, we argue that n-gram language models are
not sufficient to address word reordering required for Machine Translation. We
propose a new distortion model that can be used with existing phrase-based <st1:stockticker>SMT</st1:stockticker>
decoders to address those n-gram language model limitations. We present
empirical results in Arabic to English Machine Translation that show
statistically significant improvements when our proposed model is used. We also
propose a novel metric to measure word order similarity (or difference) between
any pair of languages based on word alignments.<span style='mso-tab-count:2'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<div style='border:none;border-bottom:solid windowtext 1.0pt;mso-border-bottom-alt:
solid windowtext .5pt;padding:0in 0in 1.0pt 0in'>

<h3>8B: Text Classification I</h3>

</div>

<p class=SessionChair>Session Chair: Janyce Wiebe<span style='mso-tab-count:
7'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b8B1></a>A Study on Automatically Extracted
Keywords in Text Categorization</p>

<p class=AbstractAuthor>Anette Hulth<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Hulth, A.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Be&aacute;ta B. Megyesi<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Megyesi, B.B.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText><span style='mso-font-kerning:0pt'>This paper presents a study
on if and how automatically extracted keywords can be used to improve text
categorization. In summary we show that a higher performance – as measured by
micro-averaged F-measure on a standard text categorization collection – is
achieved when the full-text representation is combined with the automatically
extracted keywords. The combination is obtained by giving higher weights to
words in the full-texts that are also extracted as keywords. We also present
results for experiments in which the keywords are the only input to the
categorizer, either represented as unigrams or intact. Of these two
experiments, the unigrams have the best performance, although neither performs
as well as headlines only</span><span style='mso-tab-count:2'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span
style='mso-tab-count:4'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b8B2></a>A Comparison and Semi-Quantitative
Analysis of Words and Character-Bigrams as Features in Chinese Text
Categorization</p>

<p class=AbstractAuthor>Jingyang Li<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;<span style='mso-fareast-font-family:
SimSun;mso-fareast-language:ZH-CN'>Li, J.</span>(3)&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Maosong Sun<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Sun<span style='mso-fareast-font-family:
SimSun;mso-fareast-language:ZH-CN'>, M.</span>&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Xian Zhang<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Zhang<span style='mso-fareast-font-family:
SimSun;mso-fareast-language:ZH-CN'>, X.</span>&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText><span style='mso-font-kerning:0pt'>Words and
character-bigrams are both</span><span style='mso-font-kerning:0pt;mso-ansi-language:
EN-GB'> </span><span style='mso-font-kerning:0pt'>used as features in Chinese
text processing</span><span style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'>

</span><span style='mso-font-kerning:0pt'>tasks, but no systematic comparison</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>or analysis of their values as features for</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>Chinese text categorization has been reported</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>heretofore. We carry out here a</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>full performance comparison between</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>them by experiments on various document</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>collections (including a manually</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>word-segmented corpus as a golden standard),</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>and a semi-quantitative analysis to</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>elucidate the characteristics of their behavior;</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>and try to provide some preliminary</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>clue for feature term choice (in most</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>cases, character-bigrams are better than</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>words) and dimensionality setting in text</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>categorization systems.<o:p></o:p></span></p>

<p class=AbstractTitle><a name=b8B3></a>Exploiting Comparable Corpora and
Bilingual Dictionaries for Cross-Language Text Categorization</p>

<p class=AbstractAuthor>Alfio Gliozzo<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Gliozzo, A.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Carlo Strapparava<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Strapparava, C.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText><span lang=EN-GB style='mso-ansi-language:EN-GB'>Cross-language
Text Categorization is the task to assign semantic classes to documents written
in a target language (e.g. English) while the system is trained using labeled
documents in a source language (e.g. Italian).<o:p></o:p></span></p>

<p class=MsoNormal style='text-align:justify;text-justify:inter-ideograph'><span
lang=EN-GB style='mso-bidi-font-family:Arial;mso-ansi-language:EN-GB'>In this
work we present many solutions according to the availability of bilingual
resources, and we show that it is possible to deal with the problem even when
no such resources are accessible. The core technique relies on the automatic
acquisition of Multilingual Domain Models from comparable corpora.<o:p></o:p></span></p>

<p class=MsoBodyText><span lang=EN-GB style='mso-ansi-language:EN-GB'>Experiments
show the effectiveness of our approach, providing a low cost solution for the
Cross Language Text Categorization task. In particular, when bilingual
dictionaries are available the performance of the categorization gets close to
that of monolingual text categorization.</span><span style='mso-tab-count:6'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<div style='border:none;border-bottom:solid windowtext 1.0pt;mso-border-bottom-alt:
solid windowtext .5pt;padding:0in 0in 1.0pt 0in'>

<h3>8C: Machine Learning Methods II</h3>

</div>

<p class=SessionChair>Session Chair: Anoop Sarkar<span style='mso-tab-count:
7'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b8C1></a>A Progressive Feature Selection
Algorithm for Ultra Large Feature Spaces</p>

<p class=AbstractAuthor>Qi Zhang<!--[if supportFields]><span style='mso-element:
field-begin'></span> XE &quot;Zhang, Q.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Fuliang Weng<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Weng, F.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Zhe Feng<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Feng, Z.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText>Recent developments in statistical modeling of various
linguistic phenomena have shown that additional features give consistent
performance improvements. Quite often, improvements are limited by the number
of features a system is able to explore. This paper describes a novel
progressive training algorithm that selects features from virtually unlimited
feature spaces for conditional maximum entropy (<st1:stockticker>CME</st1:stockticker>)
modeling. Experimental results in edit region identification demonstrate the
benefits of the progressive feature selection (PFS) algorithm: the PFS
algorithm maintains the same accuracy performance as previous <st1:stockticker>CME</st1:stockticker>
feature selection algorithms (e.g., Zhou et al., 2003) when the same feature
spaces are used. When additional features and their combinations are used, the
PFS gives 17.66% relative improvement over the previously reported best result
in edit region identification on Switchboard corpus (Kahn et al., 2005), which
leads to a 20% relative error reduction in parsing the Switchboard corpus when
gold edits are used as the upper bound.</p>

<p class=AbstractTitle><a name=b8C2></a>Annealing Structural Bias in
Multilingual Weighted Grammar Induction</p>

<p class=AbstractAuthor>Noah A. Smith<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Smith, N.A.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Jason Eisner<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Eisner, J.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText>We first show how a structural locality bias can improve
the accuracy of state-of-the-art dependency grammar induction models trained by
EM from unannotated examples (Klein and Manning, 2004).<span
style='mso-spacerun:yes'>&nbsp; </span>Next, by annealing the free parameter
that controls this bias, we achieve further improvements.<span
style='mso-spacerun:yes'>&nbsp; </span>We then describe an alternative kind of
structural bias, toward &quot;broken&quot; hypotheses consisting of partial
structures over segmented sentences, and show a similar pattern of improvement.
We relate this approach to contrastive estimation (Smith and Eisner, 2005),
apply the latter to grammar induction in si languages, and show that our new
approach improves accuracy by 1-17% (absolute) over CE (and 8-30% over EM),
achieving to our knowledge the best results on this task to date.<span
style='mso-spacerun:yes'>&nbsp; </span>Our method, structural annealing, is a general
technique with broad applicability to hidden-structure discovery problems.</p>

<p class=AbstractTitle><a name=b8C3></a>Maximum Entropy Based Restoration of
Arabic Diacritics</p>

<p class=AbstractAuthor>Imed Zitouni<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Zitouni, I.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Jeffrey S. Sorensen<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Sorensen, J.S.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Ruhi Sarikaya<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Sarikaya, R.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText><span style='mso-font-kerning:0pt'>Short vowels and other
diacritics are not part of written Arabic scripts. Exceptions are made for
important political and religious texts and in scripts for beginning students
of Arabic. Script without diacritics have considerable ambiguity because many
words with different diacritic patterns appear identical in a diacritic-less
setting. We propose in this paper a maximum entropy approach for restoring
diacritics in a document. The approach can easily integrate and make effective
use of diverse types of information; the model we propose integrates a wide
array of lexical, segment based and part-of-speech tag features. The
combination of these feature types leads to a state-of-the-art diacritization
model. Using a publicly available corpus (LDC's Arabic Treebank Part 3), we
achieve a diacritic error rate of 5:1%, a segment error rate 8:5%, and a word
error rate of 17:3%. In case-ending-less setting, we obtain a diacritic error
rate of 2:2%, a segment error rate 4:0%, and a word error rate of 7:2%.<o:p></o:p></span></p>

<div style='border:none;border-bottom:solid windowtext 1.0pt;mso-border-bottom-alt:
solid windowtext .5pt;padding:0in 0in 1.0pt 0in'>

<h3>8D: Information Retrieval I</h3>

</div>

<p class=SessionChair>Session Chair: Jian-Yun Nie<span style='mso-tab-count:
7'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b8D1></a>An Iterative Implicit Feedback Approach
to Personalized Search</p>

<p class=AbstractAuthor>Yuanhua Lv<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Lv, Y.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Le Sun<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Sun, L.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Junlin Zhang<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Zhang, J.(2)&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Jian-Yun Nie<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Nie, J-Y.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Wan Chen<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Chen, W.(1)&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Wei Zhang<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Zhang, W.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText>General information retrieval systems are designed to
serve all users without considering individual needs. In this paper, we propose
a novel approach to personalized search. It can, in a unified way, exploit and
utilize implicit feedback information, such as query logs and immediately
viewed documents. Moreover, our approach can implement result re-ranking and
query expansion simultaneously and collaboratively. Based on this approach, we
develop a client-side personalized web search agent PAIR (Personalized Assistant
for Information Retrieval), which supports both English and Chinese. Our
experiments on TREC and HTRDP collections clearly show that the new approach is
both effective and efficient.<span style='mso-tab-count:6'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b8D2></a>The Effect of Translation Quality in
MT-Based Cross-Language Information Retrieval</p>

<p class=AbstractAuthor>Jiang Zhu<!--[if supportFields]><span style='mso-element:
field-begin'></span> XE &quot;Zhu, J.(1)&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Haifeng Wang<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Wang, H.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText><span lang=EN-GB style='mso-ansi-language:EN-GB'>This
paper explores the relationship between the translation quality and the
retrieval effectiveness in Machine Translation (MT) based Cross-Language
Information Retrieval (CLIR). To obtain MT systems of different translation
quality, we degrade a rule-based MT system by decreasing the size of the rule
base and the size of the dictionary. We use the degraded MT systems to
translate queries and submit the translated queries of varying quality to the
IR system. Retrieval effectiveness is found to correlate highly with the
translation quality of the queries. We further analyze the factors that affect
the retrieval effectiveness. Title queries are found to be preferred in MT-based
CLIR. In addition, dictionary-based degradation is shown to have stronger
impact than rule-based degradation in MT-based CLIR.<o:p></o:p></span></p>

<p class=AbstractTitle><a name=b8D3></a>A Comparison of Document, Sentence, and
Term Event Spaces </p>

<p class=AbstractAuthor>Catherine Blake<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Blake, C.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText><span style='mso-font-kerning:0pt'>The trend in information
retrieval systems is from document to sub-document retrieval, such as sentences
in a summarization system and words or phrases in question-answering system.
Despite this trend, systems continue to model language at a document level
using the inverse document frequency (IDF). In this paper, we compare and
contrast IDF with inverse sentence frequency (ISF) and inverse term frequency
(ITF). A direct comparison reveals that all language models are highly
correlated; however, the average ISF and ITF values are 5.5 and 10.4 higher
than IDF. All language models appeared to follow a power law distribution with
a slope coefficient of 1.6 for documents and 1.7 for sentences and terms. We
conclude with an analysis of IDF stability with respect to random, journal, and
section partitions of the 100,830 full-text scientific articles in our
experimental corpus.</span></p>

<div style='border:solid windowtext 1.0pt;mso-border-alt:solid windowtext .5pt;
padding:1.0pt 4.0pt 1.0pt 4.0pt'>

<h2>Thursday 20th July 900am–930am</h2>

</div>

<div style='border:none;border-bottom:solid windowtext 1.0pt;mso-border-bottom-alt:
solid windowtext .5pt;padding:0in 0in 1.0pt 0in'>

<h3>Best Asian Language Paper Nominees</h3>

</div>

<p class=AbstractTitle><a name=b9A1></a>Tree-to-String Alignment Template for
Statistical Machine Translation</p>

<p class=AbstractAuthor>Yang Liu<!--[if supportFields]><span style='mso-element:
field-begin'></span> XE &quot;Liu, Y.(1)&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Qun Liu<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Liu, Q.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Shouxun Lin<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Lin, S.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText>We present a novel translation model based on
tree-to-string<span style='mso-ansi-language:EN-GB'> </span>alignment template
(TAT) which describes the alignment between a<span style='mso-ansi-language:
EN-GB'> </span>source parse tree and a target string. A TAT is capable of<span
style='mso-ansi-language:EN-GB'> </span>generating both terminals and
non-terminals and performing<span style='mso-ansi-language:EN-GB'> </span>reordering
at both low and high levels. The model is<span style='mso-ansi-language:EN-GB'>

</span>linguistically syntax-based because TATs are extracted<span
style='mso-ansi-language:EN-GB'> </span>automatically from word-aligned, source
side parsed parallel<span style='mso-ansi-language:EN-GB'> </span>texts. To
translate a source sentence, we first employ a parser to<span style='mso-ansi-language:
EN-GB'> </span>produce a source parse tree and then apply TATs to transform the<span
style='mso-ansi-language:EN-GB'> </span>tree into a target string. Our
experiments show that the TAT-based model<span lang=EN-GB style='mso-ansi-language:
EN-GB'> significantly outperforms </span>Pharaoh, a state-of-the-art decoder
for phrase-based models.</p>

<p class=AbstractTitle><a name=b9B1></a>Incorporating speech recognition
confidence into discriminative named entity recognition of speech data</p>

<p class=AbstractAuthor>Katsuhito Sudoh<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Sudoh, K.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Hajime Tsukada<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Tsukada, H.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Hideki Isozaki<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Isozaki, H.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText>This paper proposes a named entity recognition (<st1:stockticker>NER</st1:stockticker>)
method for speech recognition results that uses confidence on automatic speech
recognition (<st1:stockticker>ASR</st1:stockticker>) as a feature. The <st1:stockticker>ASR</st1:stockticker>
confidence feature indicates whether each word has been correctly recognized.
The <st1:stockticker>NER</st1:stockticker> model is trained using <st1:stockticker>ASR</st1:stockticker>

results with named entity (NE) labels as well as the corresponding
transcriptions with NE labels. In experiments using support vector machines
(SVMs) and speech data from Japanese newspaper articles, the proposed method
outperformed a simple application of text-based <st1:stockticker>NER</st1:stockticker>
to <st1:stockticker>ASR</st1:stockticker> results in <st1:stockticker>NER</st1:stockticker>
F-measure by improving precision. These results show that the proposed method
is effective in <st1:stockticker>NER</st1:stockticker> for noisy inputs.</p>

<p class=AbstractTitle><a name=b9C1></a>Exploiting Syntactic Patterns as Clues
in Zero-Anaphora Resolution</p>

<p class=AbstractAuthor>Ryu Iida<!--[if supportFields]><span style='mso-element:
field-begin'></span> XE &quot;Iida, R.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->,<span style='mso-bidi-font-weight:
bold'> Kentaro Inui</span><!--[if supportFields]><span style='mso-bidi-font-weight:
bold'><span style='mso-element:field-begin'></span></span> XE &quot;<span
style='mso-bidi-font-weight:bold'>Inui, K.</span>&quot; <![endif]--><!--[if supportFields]><span
style='mso-bidi-font-weight:bold'><span style='mso-element:field-end'></span></span><![endif]--><span
style='mso-bidi-font-weight:bold'><span
style='mso-spacerun:yes'>&nbsp;</span>and Yuji Matsumoto</span><!--[if supportFields]><span
style='mso-bidi-font-weight:bold'><span style='mso-element:field-begin'></span></span>
XE &quot;Matsumoto, Y.&quot; <![endif]--><!--[if supportFields]><span
style='mso-bidi-font-weight:bold'><span style='mso-element:field-end'></span></span><![endif]--></p>

<p class=MsoBodyText><span style='mso-font-kerning:0pt'>We approach the
zero-anaphora resolution problem by decomposing it into intra-sentential and
inter-sentential zero-anaphora resolution. For the former problem, syntactic patterns
of the appearance of zero-pronouns and their antecedents are useful clues.
Taking Japanese as a target language, we empirically demonstrate that
incorporating rich syntactic pattern features in a state-of-the-art
learning-based anaphora resolution model dramatically improves the accuracy of
intra-sentential zero-anaphora, which consequently improves the overall
performance of zero-anaphora resolution.<o:p></o:p></span></p>

<p class=AbstractTitle><a name=b9D1></a>Self-Organizing n-gram Model for
Automatic Word Spacing</p>

<p class=AbstractAuthor>Seong-Bae Park<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Park, S-B.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Yoon-Shik Tae<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Tae, Y-S.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Se-Young Park<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Park, S-Y.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText><span lang=EN-GB style='mso-ansi-language:EN-GB'>Automatic
word spacing is one of the important tasks in Korean language processing and
information retrieval. Since there are a number of confusing cases in word
spacing of Korean, there are some mistakes in many texts including news
articles. This paper presents a high-accurate method for automatic word spacing
based on self-organizing n-gram model. This method is basically a variant of
n-gram model, but achieves high accuracy by automatically adapting context
size.<o:p></o:p></span></p>

<p class=MsoNormal style='text-align:justify;text-justify:inter-ideograph'><span
lang=EN-GB style='mso-bidi-font-family:Arial;mso-ansi-language:EN-GB'>In order
to find the optimal context size, the proposed method automatically increases
the context size when the contextual distribution after increasing it does not
agree with that of the current context. It also decreases the context size when
the distribution of reduced context is similar to that of the current context.
This approach achieves high accuracy by considering higher dimensional data in
case of necessity, and the increased computational cost are compensated by the
reduced context size. The experimental results show that the self-organizing
structure of n-gram model enhances the basic model.<o:p></o:p></span></p>

<div style='border:solid windowtext 1.0pt;mso-border-alt:solid windowtext .5pt;
padding:1.0pt 4.0pt 1.0pt 4.0pt'>

<h2>Thursday 20th July 1030am–1230pm</h2>

</div>

<div style='border:none;border-bottom:solid windowtext 1.0pt;mso-border-bottom-alt:
solid windowtext .5pt;padding:0in 0in 1.0pt 0in'>

<h3>10A: Asian Language Processing</h3>

</div>

<p class=SessionChair>Session Chair: Michael White<span style='mso-tab-count:
7'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b10A1></a>Concept Unification of Terms in
Different Languages for IR</p>

<p class=AbstractAuthor>Qing Li<!--[if supportFields]><span style='mso-element:
field-begin'></span> XE &quot;Li, Q.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Sung-Hyon Myaeng<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Myaeng, S-H.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Yun Jin<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Jin, Y.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Bo-yeong Kang<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Kang, B.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText>Due to the historical and cultural reasons, English
phases, especially the<span style='mso-ansi-language:EN-GB'> </span>proper
nouns and new words, frequently appear in Web pages written primarily in<span
style='mso-ansi-language:EN-GB'> </span>Asian languages such as Korean and
Chinese. Although these English terms and<span style='mso-ansi-language:EN-GB'>

</span>their equivalences in the Asian language refer to the same concept, they
are<span style='mso-ansi-language:EN-GB'> </span>erroneously treated as
independent index units in traditional Information<span style='mso-ansi-language:
EN-GB'> </span>Retrieval (IR). This paper describes the degree to which the
problem arises in<span style='mso-ansi-language:EN-GB'> </span>IR and suggests
a novel technique to solve it. Our method firstly extracts the English phrase
from Asian language Web pages, and then unifies the extracted phrase and its
equivalence(s) in the language<span style='mso-ansi-language:EN-GB'> </span>as
one index unit. Experimental results show that the high precision of our<span
style='mso-ansi-language:EN-GB'> </span>conceptual unification approach greatly
improves the IR performance.</p>

<p class=AbstractTitle><a name=b10A2></a>Word Alignment in English-Hindi
Parallel Corpus Using Recency-Vector Approach: Some Studies</p>

<p class=AbstractAuthor>Niladri Chatterjee<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Chatterjee, N.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Saumya Agrawal<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Agrawal, S.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText><span style='mso-font-kerning:0pt'>Word alignment using
recency-vector</span><span style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'>
</span><span style='mso-font-kerning:0pt'>based approach has recently become
popular.</span><span style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>One major advantage of these techniques</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>is that unlike other approaches they</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>perform well even if the size of the parallel</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>corpora is small. This makes these</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>algorithms worth-studying for languages</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>where resources are scarce. In this work</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>we studied the performance of two very</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>popular recency-vector based approaches,</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>proposed in (Fung and McKeown, 1994)</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>and (Somers, 1998), respectively, for word</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>alignment in English-Hindi parallel corpus.</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>But performance of the above algorithms</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>was not found to be satisfactory.</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>However, subsequent addition of</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>some new constraints improved the performance</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>of the recency-vector based alignment</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>technique significantly for the said</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>corpus. The present paper discusses the</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>new version of the algorithm and its</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>performance</span><span style='mso-font-kerning:
0pt;mso-ansi-language:EN-GB'> </span><span style='mso-font-kerning:0pt'>in
detail.<o:p></o:p></span></p>

<p class=AbstractTitle><a name=b10A3></a>Extracting loanwords from Mongolian
corpora and producing a Japanese-Mongolian bilingual dictionary</p>

<p class=AbstractAuthor>Badam-Osor Khaltar<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Khaltar, B-O.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Atsushi Fujii<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Fujii, A.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Tetsuya Ishikawa<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Ishikawa, T.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText>This paper proposes methods for extracting loanwords from
Cyrillic Mongolian corpora and producing a Japanese–Mongolian bilingual
dictionary. We extract loanwords from Mongolian corpora using our own
handcrafted rules. To complement the rule-based extraction, we also extract
words in Mongolian corpora that are phonetically similar to Japanese Katakana
words as loanwords. In addition, we correspond the extracted loanwords to
Japanese words and produce a bilingual dictionary. We propose a stemming method
for Mongolian to extract loanwords correctly. We verify the effectiveness of
our methods experimentally.<span style='mso-tab-count:6'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<div style='border:none;border-bottom:solid windowtext 1.0pt;mso-border-bottom-alt:
solid windowtext .5pt;padding:0in 0in 1.0pt 0in'>

<h3>10B: Morphology and Word Segmentation</h3>

</div>

<p class=SessionChair>Session Chair: Yuji Matsumoto<span style='mso-tab-count:
7'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b10B1></a>An Unsupervised Morpheme-Based HMM for
Hebrew Morphological Disambiguation </p>

<p class=AbstractAuthor>Meni Adler<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Adler, M.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Michael Elhadad<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Elhadad, M.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText>Morphological disambiguation is the process of assigning
one set of morphological features to each individual word in a text.<span
style='mso-spacerun:yes'>&nbsp; </span>When the word is ambiguous (there are
several possible analyses for the word), a disambiguation procedure based on
the word context must be applied. This paper deals with morphological
disambiguation of the Hebrew language, which combines morphemes into a word in
both agglutinative and fusional ways.<span style='mso-spacerun:yes'>&nbsp;

</span>We present an unsupervised stochastic model - the only resource we use
is a morphological analyzer - which deals with the data sparseness problem
caused by the affixational morphology of the Hebrew language.</p>

<p class=MsoBodyText>We present a text encoding method for languages with
affixational morphology in which the knowledge of word formation rules (which
are quite restricted in Hebrew) helps in the disambiguation. We adapt HMM
algorithms for learning and searching this text representation, in such a way
that segmentation and tagging can be learned in parallel in one step. Results
on a large-scale evaluation indicate that this learning improves disambiguation
for complex tag sets.<span style='mso-spacerun:yes'>&nbsp; </span>Our method is
applicable to other languages with affix morphology.</p>

<p class=AbstractTitle><a name=b10B2></a>Contextual Dependencies in
Unsupervised Word Segmentation</p>

<p class=AbstractAuthor>Sharon Goldwater<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Goldwater, S.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Thomas L. Griffiths<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Griffiths, T.L.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Mark Johnson<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Johnson, M.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText>Developing better methods for segmenting continuous text
into words is important for improving the processing of Asian languages, and
may shed light on how humans learn to segment speech.<span
style='mso-spacerun:yes'>&nbsp; </span>We propose two new Bayesian word
segmentation methods that assume unigram and bigram models of word dependencies
respectively.<span style='mso-spacerun:yes'>&nbsp; </span>The bigram model
greatly outperforms the unigram model (and previous probabilistic models), demonstrating
the importance of such dependencies for word segmentation.<span
style='mso-spacerun:yes'>&nbsp; </span>We also show that previous probabilistic
models rely crucially on suboptimal search procedures.</p>

<p class=AbstractTitle><a name=b10B3></a>MAGEAD: A Morphological Analyzer and
Generator for the Arabic Dialects</p>

<p class=AbstractAuthor>Nizar Habash<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Habash, N.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Owen Rambow<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Rambow, O.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText>We present MAGEAD, a morphological analyzer and generator
for the Arabic language family.<span style='mso-spacerun:yes'>&nbsp; </span>Our
work is novel in that it explicitly addresses the need for processing the
morphology of the dialects. MAGEAD performs an on-line analysis to or
generation from a root+pattern+features representation, it has separate
phonological and orthographic representations, and it allows for combining<span
style='mso-spacerun:yes'>&nbsp;&nbsp; </span>morphemes from different dialects.<span
style='mso-spacerun:yes'>&nbsp; </span>We present a detailed evaluation of
MAGEAD.<span style='mso-tab-count:1'>&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<div style='border:none;border-bottom:solid windowtext 1.0pt;mso-border-bottom-alt:
solid windowtext .5pt;padding:0in 0in 1.0pt 0in'>

<h3>10C: Tagging and Chunking</h3>

</div>

<p class=SessionChair>Session Chair: Jan Haji&#269;<span style='mso-tab-count:7'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b10C1></a>Noun Phrase Chunking in Hebrew –
Influence of Lexical and Morphological Features</p>

<p class=AbstractAuthor>Yoav Goldberg<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Goldberg, Y.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Meni Adler<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Adler, M.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Michael Elhadad<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Elhadad, M.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText>We present a method for Noun Phrase chunking in Hebrew. We
show that the traditional definition of base-NPs as non-recursive noun phrases
does not apply in Hebrew, and propose an alternative definition of Simple
NPs.<span style='mso-spacerun:yes'>&nbsp; </span>We review syntactic properties
of Hebrew related to noun phrases, which indicate that the task of Hebrew
SimpleNP chunking is harder than base-NP chunking in English. As a
confirmation, we apply methods known to work well for English to Hebrew data.
These methods give low results (F from 76 to 86) in Hebrew. We then discuss our
method, which applies <st1:stockticker>SVM</st1:stockticker> induction over
lexical and morphological features. Morphological features improve the average
precision by ~0.5%, recall by ~1%, and F-measure by ~0.75, resulting in a
system with average performance of 93% precision, 93.4% recall and 93.2
F-measure.</p>

<p class=AbstractTitle><a name=b10C2></a>Multi-Tagging for Lexicalized-Grammar
Parsing</p>

<p class=AbstractAuthor>James R. Curran<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Curran, J.R.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Stephen Clark<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Clark, S.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and David Vadas<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Vadas, D.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText><span style='mso-font-kerning:0pt'>With performance above
97% accuracy for newspaper text, part of speech (</span><st1:stockticker><span
 style='mso-font-kerning:0pt'>POS</span></st1:stockticker><span
style='mso-font-kerning:0pt'>) tagging might be considered a solved problem.
Previous studies have shown that allowing the parser to resolve </span><st1:stockticker><span
 style='mso-font-kerning:0pt'>POS</span></st1:stockticker><span
style='mso-font-kerning:0pt'> tag ambiguity does not improve performance.
However, for grammar formalisms which use more fine-grained grammatical
categories, for example </span><st1:stockticker><span style='mso-font-kerning:
 0pt'>TAG</span></st1:stockticker><span style='mso-font-kerning:0pt'> and </span><st1:stockticker><span
 style='mso-font-kerning:0pt'>CCG</span></st1:stockticker><span
style='mso-font-kerning:0pt'>, tagging accuracy is much lower. In fact, for
these formalisms, premature ambiguity resolution makes parsing infeasible.<o:p></o:p></span></p>

<p class=MsoBodyText><span style='mso-font-kerning:0pt'>We describe a
multi-tagging approach which maintains a suitable level of lexical category
ambiguity for accurate and efficient </span><st1:stockticker><span
 style='mso-font-kerning:0pt'>CCG</span></st1:stockticker><span
style='mso-font-kerning:0pt'> parsing. We extend this multi-tagging approach to
the </span><st1:stockticker><span style='mso-font-kerning:0pt'>POS</span></st1:stockticker><span
style='mso-font-kerning:0pt'> level to overcome errors introduced by
automatically assigned </span><st1:stockticker><span style='mso-font-kerning:
 0pt'>POS</span></st1:stockticker><span style='mso-font-kerning:0pt'> tags.
Although </span><st1:stockticker><span style='mso-font-kerning:0pt'>POS</span></st1:stockticker><span
style='mso-font-kerning:0pt'> tagging accuracy seems high, maintaining some </span><st1:stockticker><span
 style='mso-font-kerning:0pt'>POS</span></st1:stockticker><span
style='mso-font-kerning:0pt'> tag ambiguity in the language processing pipeline
results in more accurate </span><st1:stockticker><span style='mso-font-kerning:
 0pt'>CCG</span></st1:stockticker><span style='mso-font-kerning:0pt'>

supertagging.</span><span style='mso-tab-count:6'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b10C3></a>Guessing Parts-of-Speech of Unknown
Words Using Global Information</p>

<p class=AbstractAuthor>Tetsuji Nakagawa<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Nakagawa, T.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Yuji Matsumoto<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Matsumoto, Y.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText><span style='mso-font-kerning:0pt'>In this paper, we
present a method for</span><span style='mso-font-kerning:0pt;mso-ansi-language:
EN-GB'> </span><span style='mso-font-kerning:0pt'>guessing </span><st1:stockticker><span
 style='mso-font-kerning:0pt'>POS</span></st1:stockticker><span
style='mso-font-kerning:0pt'> tags of unknown words using</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>local and global information. Although</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>many existing methods use only</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>local information (i.e. limited window</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>size or intra-sentential features), global
information</span><span style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>(extra-sentential features) provides</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>valuable clues for predicting </span><st1:stockticker><span
 style='mso-font-kerning:0pt'>POS</span></st1:stockticker><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>tags of unknown words. We propose a</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>probabilistic model for </span><st1:stockticker><span
 style='mso-font-kerning:0pt'>POS</span></st1:stockticker><span
style='mso-font-kerning:0pt'> guessing of</span><span style='mso-font-kerning:
0pt;mso-ansi-language:EN-GB'> </span><span style='mso-font-kerning:0pt'>unknown
words using global information</span><span style='mso-font-kerning:0pt;
mso-ansi-language:EN-GB'> </span><span style='mso-font-kerning:0pt'>as well as
local information, and estimate</span><span style='mso-font-kerning:0pt;
mso-ansi-language:EN-GB'> </span><span style='mso-font-kerning:0pt'>its
parameters using Gibbs sampling. We</span><span style='mso-font-kerning:0pt;
mso-ansi-language:EN-GB'> </span><span style='mso-font-kerning:0pt'>also
attempt to apply the model to semisupervised</span><span style='mso-font-kerning:
0pt;mso-ansi-language:EN-GB'> </span><span style='mso-font-kerning:0pt'>learning,
and conduct experiments</span><span style='mso-font-kerning:0pt;mso-ansi-language:
EN-GB'> </span><span style='mso-font-kerning:0pt'>on multiple corpora.</span><span
style='mso-tab-count:6'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<div style='border:none;border-bottom:solid windowtext 1.0pt;mso-border-bottom-alt:
solid windowtext .5pt;padding:0in 0in 1.0pt 0in'>

<h3>SRW 1: Multilinguality</h3>

</div>

<p class=SessionChair>Session Chair: Marine Carpuat<span style='mso-tab-count:
7'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b10D1></a>S1<span style='mso-tab-count:1'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>Discursive
Usage of Six Chinese Punctuation Marks</p>

<p class=AbstractAuthor>Ming Yue<span style='mso-tab-count:1'>&nbsp; </span></p>

<p class=MsoBodyText>Both rhetorical structure and punctuation have been helpful
in discourse processing. Based on a corpus annotation project, this paper
reports the discursive usage of 6 Chinese punctuation marks in news commentary
texts: Colon, Dash, Ellipsis, Exclamation Mark, Question Mark, and Semicolon.
The rhetorical patterns of these marks are compared against patterns around cue
phrases in general. Results show that these Chinese punctuation marks, though
fewer in number than cue phrases, are easy to identify, have strong correlation
with certain relations, and can be used as distinctive indicators of nuclearity
in Chinese texts.<span style='mso-tab-count:2'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span
style='mso-tab-count:3'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b10D2></a>S2<span style='mso-tab-count:1'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>Integrated
Morphological and Syntactic Disambiguation for Modern Hebrew</p>

<p class=AbstractAuthor>Reut Tsarfaty</p>

<p class=MsoBodyText>Current parsing models are not immediately applicable for
languages that exhibit strong interaction between morphology and syntax, e.g.,
Modern Hebrew (MH), Arabic and other Semitic languages. This work represents a
first attempt at modeling morphological-syntactic interaction in a generative
probabilistic framework to allow for MH parsing. We show that morphological
information selected in tandem with syntactic categories is instrumental for
parsing Semitic languages. We further show that redundant morphological
information helps syntactic disambiguation.<span style='mso-tab-count:4'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b10D3></a>S3<span style='mso-tab-count:1'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>A
Hybrid Relational Approach for WSD</p>

<p class=AbstractAuthor>Lucia Specia</p>

<p class=MsoBodyText>We present a novel hybrid approach for Word Sense
Disambiguation (WSD) which makes use of a relational formalism to represent
instances and background knowledge. It is built using Inductive Logic
Programming techniques to combine evidence coming from both sources during the
learning process, producing a rule-based WSD model. We experimented with this
approach to disambiguate 7 highly ambiguous verbs in English- Portuguese
translation. Results showed that the approach is promising, achieving an average
accuracy of 75%, which outperforms the other machine learning techniques
investigated (66%).<span style='mso-tab-count:2'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span
style='mso-tab-count:2'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=MsoNormal><span style='mso-tab-count:7'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<div style='border:solid windowtext 1.0pt;mso-border-alt:solid windowtext .5pt;
padding:1.0pt 4.0pt 1.0pt 4.0pt'>

<h2>Thursday 20th July 230pm–330pm</h2>

</div>

<div style='border:none;border-bottom:solid windowtext 1.0pt;mso-border-bottom-alt:
solid windowtext .5pt;padding:0in 0in 1.0pt 0in'>

<h3>11A: Machine Translation IV</h3>

</div>

<p class=SessionChair>Session Chair: Alon Lavie<span style='mso-tab-count:7'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b11A1></a>A Clustered Global Phrase Reordering
Model for Statistical Machine Translation</p>

<p class=AbstractAuthor>Masaaki Nagata<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Nagata, M.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Kuniko Saito<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Saito, K.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Kazuhide Yamamoto<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Yamamoto, K.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Kazuteru Ohashi<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Ohashi, K.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText>In this paper, we present a novel global reordering model
that can be incorporated into standard phrase-based statistical machine
translation. Unlike previous local reordering models that emphasize the
reordering of adjacent phrase pairs [Tillmann-Zhang05], our model explicitly
models the reordering of long distances by directly estimating the parameters
from the phrase alignments of bilingual training sentences. In principle, the
global phrase-reordering model is conditioned on the source and target phrases
that are currently being translated, and the previously translated source and target
phrases. To cope with sparseness, we use N-best phrase alignments and bilingual
phrase clustering, and investigate a variety of combinations of conditioning
factors. Through experiments, we show, that the global reordering model
significantly improves the translation accuracy of a standard Japanese-English
translation task.</p>

<p class=AbstractTitle><a name=b11A2></a>A Discriminative Global Training
Algorithm for Statistical MT</p>

<p class=AbstractAuthor>Christoph Tillmann<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Tillmann, C.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Tong Zhang<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Zhang, T.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText><span lang=EN-GB style='mso-ansi-language:EN-GB'>This
paper presents a novel training algorithm for a linearly-scored block sequence
translation model. The key component is a new procedure to directly optimize
the global scoring function used by a </span><st1:stockticker><span lang=EN-GB
 style='mso-ansi-language:EN-GB'>SMT</span></st1:stockticker><span lang=EN-GB
style='mso-ansi-language:EN-GB'> decoder.<span style='mso-spacerun:yes'>&nbsp;
</span>No translation, language, or distortion model probabilities are used as
in earlier work on </span><st1:stockticker><span lang=EN-GB style='mso-ansi-language:
 EN-GB'>SMT</span></st1:stockticker><span lang=EN-GB style='mso-ansi-language:
EN-GB'>. Therefore our method, which employs less domain specific knowledge, is
both simpler and more extensible than previous approaches. Moreover, the
training procedure treats the decoder as a black-box, and thus can be used to
optimize any decoding scheme. The training algorithm is evaluated on a standard
Arabic-English translation task.<o:p></o:p></span></p>

<div style='border:none;border-bottom:solid windowtext 1.0pt;mso-border-bottom-alt:
solid windowtext .5pt;padding:0in 0in 1.0pt 0in'>

<h3>11B: Speech</h3>

</div>

<p class=MsoBodyText>Session Chair: Roland Kuhn<span style='mso-tab-count:7'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b11B1></a>Phoneme-to-Text Transcription System
with an Infinite Vocabulary</p>

<p class=AbstractAuthor>Shinsuke Mori<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Mori, S.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Daisuke Takuma<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Takuma, D.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Gakuto Kurata<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Kurata, G.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText><span lang=EN-GB style='mso-ansi-language:EN-GB'>The noisy
channel model approach is successfully applied to various natural language
processing tasks. Currently the main research focus of this approach is
adaptation methods, how to capture characteristics of words and expressions in
a target domain given example sentences in that domain. As a solution we
describe a method enlarging the vocabulary of a language model to an almost
infinite size and capturing their context information. Especially the new
method is suitable for languages in which words are not delimited by
whitespace. We applied our method to a phoneme-to-text transcription task in
Japanese and reduced about 10% of the errors in the results of an existing
method.</span><span style='mso-tab-count:2'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b11B2></a>Automatic Generation of Domain Models
for Call Centers from Noisy Transcriptions</p>

<p class=AbstractAuthor>Shourya Roy<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Roy, S.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and L Venkata Subramaniam<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Subramaniam, L.V.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText><span style='mso-font-kerning:0pt'>Call centers handle
customer queries from various domains such as computer sales and support,
mobile phones, car rental, etc. Each such domain generally has a domain model
which is essential to handle customer complaints. These models contain common
problem categories, typical customer issues and their solutions, greeting
styles. Currently these models are manually created over time. Towards this, we
propose an unsupervised technique to generate domain models automatically from
call transcriptions. We use a state of the art Automatic Speech Recognition
system to transcribe the calls between agents and customers, which still
results in high word error rates (40%) and show that even from these noisy
transcriptions of calls we can automatically build a domain model. The domain
model is comprised of primarily a topic taxonomy where every node is
characterized by topic(s), typical Questions-Answers (Q&amp;As), typical
actions and call statistics. We show how such a domain model can be used for
topic identification of unseen calls. We also propose applications for aiding
agents while handling calls and for agent monitoring based on the domain model.<o:p></o:p></span></p>

<div style='border:none;border-bottom:solid windowtext 1.0pt;mso-border-bottom-alt:
solid windowtext .5pt;padding:0in 0in 1.0pt 0in'>

<h3>11C: Discourse</h3>

</div>

<p class=SessionChair>Session Chair: Daniel Marcu<span style='mso-tab-count:
7'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b11C1></a>Proximity in Context: an empirically
grounded computational model of proximity for processing topological spatial
expressions</p>

<p class=AbstractAuthor>John D. Kelleher<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Kelleher, J.D.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Geert-Jan M. Kruijff<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Kruijff, G-J. M.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Fintan J. Costello<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Costello, F.J.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText>The paper presents a new model for context-dependent
interpretation of linguistic expressions about spatial proximity between
objects in a natural scene. The paper discusses novel psycholinguistic
experimental data that tests and verifies the model. The model has been
implemented, and enables a conversational robot to identify objects in a scene
through topological spatial relations (e.g. “X near Y''). The model can help
motivate the choice between topological and projective prepositions.<span
style='mso-tab-count:1'>&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b11C2></a>Machine Learning of Temporal Relations
</p>

<p class=AbstractAuthor>Inderjeet Mani<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Mani, I.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Marc Verhagen<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Verhagen, M.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Ben Wellner<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Wellner, B.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Chong Min Lee<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Lee, C.M.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and James Pustejovsky<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Pustejovsky, J.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText>This paper investigates a machine learning approach for
temporally ordering and anchoring events in natural language texts. To address
data sparseness, we used temporal reasoning as an over-sampling method to
dramatically expand the amount of training data, resulting in predictive
accuracy on link labeling as high as 93% using a Maximum Entropy classifier on
human annotated data. This method compared favorably against a series of
increasingly sophisticated baselines involving expansion of rules derived from
human intuitions.</p>

<div style='border:none;border-bottom:solid windowtext 1.0pt;mso-border-bottom-alt:
solid windowtext .5pt;padding:0in 0in 1.0pt 0in'>

<h3>SRW 2: Speech</h3>

</div>

<p class=SessionChair>Session Chair: Kevin Duh<span style='mso-tab-count:7'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b11D1></a>S4<span style='mso-tab-count:1'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>On2L
- A Framework for Incremental Ontology Learning in Spoken Dialog Systems</p>

<p class=AbstractAuthor>Berenike Loos</p>

<p class=MsoBodyText>An open-domain spoken dialog system has to deal with the
challenge of lacking lexical as well as conceptual knowledge. As the real world
is constantly changing, it is not possible to store all necessary knowledge
beforehand. Therefore, this knowledge has to be acquired during the run time of
the system, with the help of the out-of-vocabulary information of a speech
recognizer. As every word can have various meanings depending on the context in
which it is uttered, additional context information is taken into account, when
searching for the meaning of such a word. In this paper, I will present the
incremental ontology learning framework On2L. The defined tasks for the
framework are: the hypernym extraction from Internet texts for unknown terms
delivered by the speech recognizer; the mapping of those and their hypernyms
into ontological concepts and instances; and the following integration of them
into the system’s ontology.<span style='mso-tab-count:6'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b11D2></a>S5<span style='mso-tab-count:1'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>Focus
to Emphasize Tone Structures for Prosodic Analysis in Spoken Language
Generation</p>

<p class=AbstractAuthor>Lalita Narupiyakul<span style='mso-tab-count:1'>&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=MsoBodyText>We analyze the concept of focus in speech and the
relationship between focus and speech acts for prosodic generation. We
determine how the speaker’s utterances are influenced by speaker’s intention.
The relationship between speech acts and focus information is used to define
which parts of the sentence serve as the focus parts. We propose the Focus to
Emphasize Tones (FET) structure to analyze the focus components. We also design
the FET grammar to analyze the intonation patterns and produce tone marks as a
result of our analysis. We present a proof-of-the-concept working example to
validate our proposal. More comprehensive evaluations are part of our current
work.<span style='mso-tab-count:4'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<div style='border:solid windowtext 1.0pt;mso-border-alt:solid windowtext .5pt;
padding:1.0pt 4.0pt 1.0pt 4.0pt'>

<h2>Thursday 20th July 400pm–530pm</h2>

</div>

<div style='border:none;border-bottom:solid windowtext 1.0pt;mso-border-bottom-alt:
solid windowtext .5pt;padding:0in 0in 1.0pt 0in'>

<h3>12A: Machine Translation V</h3>

</div>

<p class=SessionChair>Session Chair: Alon Lavie<span style='mso-tab-count:7'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b12A1></a>An End-to-End Discriminative Approach
to Machine Translation</p>

<p class=AbstractAuthor><span lang=FR style='mso-ansi-language:FR'>Percy Liang</span><!--[if supportFields]><span
style='mso-element:field-begin'></span><span lang=FR style='mso-ansi-language:
FR'> XE &quot;Liang, P.&quot; </span><![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span lang=FR
style='mso-ansi-language:FR'>, Alexandre Bouchard-C&ocirc;t&eacute;</span><!--[if supportFields]><span
lang=FR style='mso-ansi-language:FR'><span style='mso-element:field-begin'></span>
XE &quot;Bouchard-C&ocirc;t&eacute;, A.&quot; </span><![endif]--><!--[if supportFields]><span
lang=FR style='mso-ansi-language:FR'><span style='mso-element:field-end'></span></span><![endif]--><span
lang=FR style='mso-ansi-language:FR'>, Dan Klein</span><!--[if supportFields]><span
style='mso-element:field-begin'></span><span lang=FR style='mso-ansi-language:
FR'> XE &quot;Klein, D.&quot; </span><![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span lang=FR
style='mso-ansi-language:FR'><span style='mso-spacerun:yes'>&nbsp;</span>and
Ben Taskar</span><!--[if supportFields]><span lang=FR style='mso-ansi-language:
FR'><span style='mso-element:field-begin'></span> XE &quot;Taskar, B.&quot; </span><![endif]--><!--[if supportFields]><span
lang=FR style='mso-ansi-language:FR'><span style='mso-element:field-end'></span></span><![endif]--><span
lang=FR style='mso-ansi-language:FR'><o:p></o:p></span></p>

<p class=MsoBodyText><span style='mso-font-kerning:0pt'>We present a
perceptron-style discriminative approach to machine translation in which large
feature sets can be exploited. Unlike discriminative reranking approaches, our
system can take advantage of learned features in all stages of decoding. We
first discuss several challenges to error-driven discriminative approaches. In
particular, we explore different ways of updating parameters given a training
example. We find that making frequent but smaller updates is preferable to
making fewer but larger updates. Then, we discuss an array of features and show
both how they quantitatively increase BLEU score and how they qualitatively
interact on specific examples. One particular feature we investigate is a novel
way to introduce learning into the initial phrase extraction process, which has
previously been entirely heuristic.<o:p></o:p></span></p>

<p class=AbstractTitle><a name=b12A2></a>Semi-Supervised Training for
Statistical Word Alignment</p>

<p class=AbstractAuthor>Alexander Fraser<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Fraser, A.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Daniel Marcu<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Marcu, D.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText>We introduce a semi-supervised approach to training for
statistical machine translation that alternates the traditional Expectation
Maximization step that is applied on a large training corpus with a
discriminative step aimed at increasing word-alignment quality on a small,
manually word-aligned sub-corpus. We show that our algorithm leads not only to
improved alignments but also to machine translation outputs of higher quality.</p>

<p class=AbstractTitle><a name=b12A3></a>Left-to-Right Target Generation for
Hierarchical Phrase-based Translation</p>

<p class=AbstractAuthor>Taro Watanabe<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Watanabe, T.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Hajime Tsukada<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Tsukada, H.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Hideki Isozaki<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Isozaki, H.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText><span style='mso-font-kerning:0pt'>We present a hierarchical
phrase-based statistical machine translation in which</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>a target sentence is efficiently generated in
left-to-right order. The model is</span><span style='mso-font-kerning:0pt;
mso-ansi-language:EN-GB'> </span><span style='mso-font-kerning:0pt'>a class of
synchronous-CFG with a Greibach Normal Form-like structure for the</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>projected production rule:</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>The paired target-side of a production rule takes
a phrase-prefixed form. The</span><span style='mso-font-kerning:0pt;mso-ansi-language:
EN-GB'> </span><span style='mso-font-kerning:0pt'>decoder for the
target-normalized form is based on an Early-style top down</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>parser on the source side. The target-normalized
form coupled with our top down</span><span style='mso-font-kerning:0pt;
mso-ansi-language:EN-GB'> </span><span style='mso-font-kerning:0pt'>parser
implies a left-to-right generation of translations which enables us a</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>straightforward integration with ngram language
models. Our model was</span><span style='mso-font-kerning:0pt;mso-ansi-language:
EN-GB'> </span><span style='mso-font-kerning:0pt'>experimented on a
Japanese-to-English newswire translation task, and showed</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>statistically significant performance improvements
against a phrase-based</span><span style='mso-font-kerning:0pt;mso-ansi-language:
EN-GB'> </span><span style='mso-font-kerning:0pt'>translation system.</span><span
style='mso-tab-count:1'>&nbsp; </span></p>

<div style='border:none;border-bottom:solid windowtext 1.0pt;mso-border-bottom-alt:
solid windowtext .5pt;padding:0in 0in 1.0pt 0in'>

<h3>12B: Lexical Issues III</h3>

</div>

<p class=MsoBodyText>Session Chair: Nicoletta Calzolari<span style='mso-tab-count:
7'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b12B1></a>You Can't Beat Frequency (Unless You
Use Linguistic Knowledge) – A Qualitative Evaluation of Association Measures
for Collocation and Term Extraction</p>

<p class=AbstractAuthor>Joachim Wermter<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Wermter, J.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Udo Hahn<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Hahn, U.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText>In the past years, a number of lexical association
measures have been studied to help extract new scientific terminology or general-language
collocations. The implicit assumption of this research was that newly designed
term measures involving more sophisticated statistical criteria would
outperform simple counts of co-occurrence frequencies. We here explicitly test
this assumption. By way of four qualitative criteria, we show that purely
statistics-based measures reveal virtually no difference compared with
frequency of occurrence counts, while linguistically more informed metrics do
reveal such a marked difference.</p>

<p class=AbstractTitle><a name=b12B2></a>Ontologizing Semantic Relations</p>

<p class=AbstractAuthor>Marco Pennacchiotti<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Pennacchiotti, M.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Patrick Pantel<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Pantel, P.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText><span style='mso-font-kerning:0pt'>Many algorithms have
been developed to harvest lexical semantic resources, however few have linked
the mined knowledge into formal knowledge repositories. In this paper, we
propose two algorithms for automatically ontologizing (attaching) semantic
relations into WordNet. We present an empirical evaluation on the task of
attaching partof and causation relations, showing an improvement on F-score
over a baseline model.</span><span style='mso-tab-count:1'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b12B3></a>Semantic Taxonomy Induction from
Heterogenous Evidence </p>

<p class=AbstractAuthor>Rion Snow<!--[if supportFields]><span style='mso-element:
field-begin'></span> XE &quot;Snow, R.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Daniel Jurafsky<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Jurafsky, D.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Andrew Y. Ng<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Ng, A.Y.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText><span style='mso-font-kerning:0pt'>We propose a novel
algorithm for inducing semantic taxonomies. Previous algorithms for taxonomy
induction have typically focused on independent classifiers for discovering new
single relationships based on hand-constructed or automatically discovered
textual patterns. By contrast, our algorithm flexibly incorporates evidence
from multiple classifiers over heterogenous relationships to optimize the
entire structure of the taxonomy, using knowledge of a word’s coordinate terms
to help in determining its hypernyms, and vice versa. We apply our algorithm on
the problem of sense-disambiguated noun hyponym acquisition, where we combine
the predictions of hypernym and coordinate term classifiers with the knowledge
in a preexisting semantic taxonomy (WordNet 2.1). We add 10; 000 novel synsets
to WordNet 2.1 at 84% precision, a relative error reduction of 70% over a
non-joint algorithm using the same component classifiers. Finally, we show that
a taxonomy built using our algorithm shows a 23% relative F-score improvement
over WordNet 2.1 on an independent testset of hypernym pairs.</span><span
style='mso-tab-count:2'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span
style='mso-tab-count:4'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<div style='border:none;border-bottom:solid windowtext 1.0pt;mso-border-bottom-alt:
solid windowtext .5pt;padding:0in 0in 1.0pt 0in'>

<h3>12C: Information Extraction III</h3>

</div>

<p class=SessionChair>Session Chair: Yorick Wilks<span style='mso-tab-count:
7'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b12C1></a>Names and Similarities on the Web:
Fact Extraction in the Fast Lane</p>

<p class=AbstractAuthor>Marius Pa&#351;ca<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Pa&#351;ca, M.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Dekang Lin<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Lin, D.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Jeffrey Bigham<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Bigham, J.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Andrei Lifchits<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Lifchits, A.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Alpa Jain<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Jain, A.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText><span style='mso-font-kerning:0pt'>In a new approach to
large-scale extraction of facts from unstructured text, distributional
similarities become an integral part of both the iterative acquisition of
high-coverage contextual extraction patterns, and the validation and ranking of
candidate facts. The evaluation measures the quality and coverage of facts
extracted from one hundred million Web documents, starting from ten seed facts
and using no additional knowledge, lexicons or complex tools.</span><span
style='mso-tab-count:6'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b12C2></a>Weakly Supervised Named Entity
Transliteration and Discovery from Multilingual Comparable Corpora</p>

<p class=AbstractAuthor>Alexandre Klementiev<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Klementiev, A.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Dan Roth<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Roth, D.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText><span style='mso-font-kerning:0pt'>Named Entity
recognition (</span><st1:stockticker><span style='mso-font-kerning:0pt'>NER</span></st1:stockticker><span
style='mso-font-kerning:0pt'>) is an important part of many natural language
processing tasks. Current approaches often employ machine-learning techniques
and require supervised data. However, many languages lack such resources. This
paper presents an (almost) unsupervised learning algorithm for automatic
discovery of Named Entities (NEs) in a resource free language, given a
bilingual corpora in which it is weakly temporally aligned with a resource rich
language. NEs have similar time distributions across such corpora, and often
some of the tokens in a multi-word NE are transliterated. We develop an
algorithm that exploits both observations iteratively. The algorithm makes use
of a new, frequency based, metric for time distributions and a resource free
discriminative approach to transliteration. Seeded with a small number of
transliteration pairs, our algorithm discovers multi-word NEs, and takes
advantage of a dictionary (if one exists) to account for translated or
partially translated NEs. We evaluate the algorithm on an English-Russian
corpus, and show high level of NEs discovery in Russian.</span><span
style='mso-tab-count:1'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span
style='mso-tab-count:5'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b12C3></a>A Composite Kernel to Extract
Relations between Entities with both Flat and Structured Features</p>

<p class=AbstractAuthor>Min Zhang<!--[if supportFields]><span style='mso-element:
field-begin'></span> XE &quot;Zhang, M.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Jie Zhang<!--[if supportFields]><span
lang=FR style='mso-ansi-language:FR'><span style='mso-element:field-begin'></span></span>
XE &quot;Zhang, J.(1)&quot; <![endif]--><!--[if supportFields]><span lang=FR
style='mso-ansi-language:FR'><span style='mso-element:field-end'></span></span><![endif]-->,
Jian Su<!--[if supportFields]><span style='mso-element:field-begin'></span> XE
&quot;Su, J.&quot; <![endif]--><!--[if supportFields]><span style='mso-element:
field-end'></span><![endif]--><span style='mso-spacerun:yes'>&nbsp;</span>and
Guodong Zhou<!--[if supportFields]><span style='mso-element:field-begin'></span>
XE &quot;Zhou, G.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText><span style='mso-font-kerning:0pt'>This paper proposes a
novel composite kernel for relation extraction. The composite kernel consists
of two individual kernels: an entity kernel that allows for entity-related
features and a convolution parse tree kernel that models syntactic information
of relation examples. The motivation of our method is to fully utilize the nice
properties of kernel methods to explore diverse knowledge for relation
extraction. Our study illustrates that the composite kernel can effectively
capture both flat and structured features without the need for extensive
feature engineering, and can also easily scale to include more features.
Evaluation on the ACE corpus shows that our method outperforms the previous
best-reported methods and significantly outperforms previous two dependency
tree kernels for relation extraction.<o:p></o:p></span></p>

<div style='border:none;border-bottom:solid windowtext 1.0pt;mso-border-bottom-alt:
solid windowtext .5pt;padding:0in 0in 1.0pt 0in'>

<h3>SRW 3: Parsing</h3>

</div>

<p class=SessionChair>Session Chair: Stephen Wan<span style='mso-tab-count:
7'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b12D1></a>S6<span style='mso-tab-count:1'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>Extraction
of Tree Adjoining Grammars from a Treebank for Korean</p>

<p class=AbstractAuthor>Jungyeul Park</p>

<p class=MsoBodyText>We present the implementation of a system which extracts
not only lexicalized grammars but also feature-based lexicalized grammars from
Korean Sejong Treebank. We report on some practical experiments where we
extract <st1:stockticker>TAG</st1:stockticker> grammars and tree schemata.
Above all, full-scale syntactic tags and well-formed morphological analysis in
Sejong Treebank allow us to extract syntactic features. In addition, we modify
Treebank for extracting lexicalized grammars and convert lexicalized grammars
into tree schemata to resolve limited lexical coverage problem of extracted
lexicalized grammars.<span style='mso-tab-count:1'>&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b12D2></a>S7<span style='mso-tab-count:1'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>Parsing
and Subcategorization Data</p>

<p class=AbstractAuthor>Jianguo Li<span style='mso-tab-count:2'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=MsoBodyText>In this paper, we compare the performance of a
state-of-the-art statistical parser (Bikel, 2004) in parsing written and spoken
language and in generating subcategorization cues from written and spoken
language. Although Bikel’s parser achieves a higher accuracy for parsing
written language, it achieves a higher accuracy when extracting
subcategorization cues from spoken language. Additionally, we explore the
utility of punctuation in helping parsing and extraction of subcategorization
cues. Our experiments show that punctuation is of little help in parsing spoken
language and extracting subcategorization cues from spoken language. This
indicates that there is no need to add punctuation in transcribing spoken
corpora simply in order to help parsers.<span style='mso-tab-count:2'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b12D3></a>S8<span style='mso-tab-count:1'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>Clavius:
Bi-Directional Parsing for Generic Multimodal Interaction</p>

<p class=AbstractAuthor>Frank Rudzicz</p>

<p class=MsoBodyText>We introduce a new multi-threaded parsing algorithm on
unification grammars designed specifically for multimodal interaction and noisy
environments. By lifting some traditional constraints, namely those related to
the ordering of constituents, we overcome several difficulties of other systems
in this domain. We also present several criteria used in this model to
constrain the search process using dynamically loadable scoring functions. Some
early analyses of our implementation are discussed.</p>

<div style='border:solid windowtext 1.0pt;mso-border-alt:solid windowtext .5pt;
padding:1.0pt 4.0pt 1.0pt 4.0pt'>

<h2>Friday 21st July 1000am–1030am</h2>

</div>

<div style='border:none;border-bottom:solid windowtext 1.0pt;mso-border-bottom-alt:
solid windowtext .5pt;padding:0in 0in 1.0pt 0in'>

<h3>13A: Parsing VI</h3>

</div>

<p class=SessionChair>Session Chair: Srinivas Bangalore<span style='mso-tab-count:
7'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b13A1></a>Japanese Dependency Parsing Using
Co-occurrence Information and a Combination of Case Elements</p>

<p class=AbstractAuthor>Takeshi Abekawa<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Abekawa, T.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Manabu Okumura<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Okumura, M.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText><span style='mso-font-kerning:0pt'>In this paper, we
present a method that improves Japanese dependency parsing by using large-scale
statistical information. It takes into account two kinds of information not
considered in previous statistical (machine learning based) parsing methods:
information about dependency relations among the case elements of a verb, and
information about co-occurrence relations between a verb and its case element.
This information can be collected from the results of automatic dependency
parsing of large-scale corpora. The results of an experiment in which our
method was used to rerank the results obtained using an existing machine
learning based parsing method showed that our method can improve the accuracy
of the results obtained using the existing method.<o:p></o:p></span></p>

<div style='border:none;border-bottom:solid windowtext 1.0pt;mso-border-bottom-alt:
solid windowtext .5pt;padding:0in 0in 1.0pt 0in'>

<h3>13B: Question Answering I</h3>

</div>

<p class=SessionChair>Session Chair: Dan Moldovan<span style='mso-tab-count:
7'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b13B1></a>Answer Extraction, Semantic
Clustering, and Extractive Summarization for Clinical Question Answering</p>

<p class=AbstractAuthor>Dina Demner-Fushman<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Demner-Fushman, D.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Jimmy Lin<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Lin, J.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText><span style='mso-font-kerning:0pt'>This paper presents a
hybrid approach</span><span style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'>
</span><span style='mso-font-kerning:0pt'>to question answering in the clinical</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>domain that combines techniques from</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>summarization and information retrieval.</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>We tackle a frequently-occurring class of</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>questions that takes the form “What is</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>the best drug treatment for X?” Starting</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>from an initial set of MEDLINE citations,</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>our system first identifies the drugs under</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>study. Abstracts are then clustered using</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>semantic classes from the UMLS ontology.</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>Finally, a short extractive summary</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>is generated for each abstract to populate</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>the clusters. Two evaluations—a</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>manual one focused on short answers and</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>an automatic one focused on the supporting</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>abstracts—demonstrate that our system</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>compares favorably to PubMed, the</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>search system most widely used by physicians</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>today.</span><span style='mso-tab-count:1'>&nbsp;&nbsp; </span></p>

<div style='border:none;border-bottom:solid windowtext 1.0pt;mso-border-bottom-alt:
solid windowtext .5pt;padding:0in 0in 1.0pt 0in'>

<h3>13C: Semantics III</h3>

</div>

<p class=SessionChair>Session Chair: Alexander Koller<span style='mso-tab-count:
7'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b13C1></a>Discovering asymmetric entailment
relations between verbs using selectional preferences</p>

<p class=AbstractAuthor>Fabio Massimo Zanzotto<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Zanzotto, F.M.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Marco Pennacchiotti<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Pennacchiotti, M.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Maria Teresa Pazienza<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Pazienza, M.T.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText><span lang=EN-GB style='mso-ansi-language:EN-GB'>In this
paper we investigate a novel method to detect asymmetric entailment relations
between verbs. Our starting point is the idea that some point-wise verb
selectional preferences carry relevant semantic information. Experiments using
WordNet as a gold standard show promising results. Where applicable, our
method, used in combination with other approaches, significantly increases the
performance of entailment detection. A combined approach including our model
improves the AROC of 5% with respect to standard models.<o:p></o:p></span></p>

<div style='border:none;border-bottom:solid windowtext 1.0pt;mso-border-bottom-alt:
solid windowtext .5pt;padding:0in 0in 1.0pt 0in'>

<h3>13D: Applications III</h3>

</div>

<p class=SessionChair>Session Chair:<span style='mso-spacerun:yes'>&nbsp;
</span>Eva Haji&#269;ov&aacute;<span style='mso-tab-count:7'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b13D1></a>Event Extraction in a Plot Advice
Agent </p>

<p class=AbstractAuthor>Harry Halpin<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Halpin, H.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Johanna D. Moore<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Moore, J.D.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText><span style='mso-font-kerning:0pt'>In this paper we
present how the automatic extraction of events from text can be used to both
classify narrative texts according to plot quality and produce advice in an
interactive learning environment intended to help students with story writing.
We focus on the story-rewriting task, in which an exemplar story is read to the
students and the students rewrite the story in their own words. The system
automatically extracts events from the raw text, formalized as a sequence of
temporally ordered predicate-arguments. These events are given to a
machine-learner that produces a coarse-grained rating of the story. The results
of the machine-learner and the extracted events are then used to generate fine-grained
advice for the students.<o:p></o:p></span></p>

<div style='border:solid windowtext 1.0pt;mso-border-alt:solid windowtext .5pt;
padding:1.0pt 4.0pt 1.0pt 4.0pt'>

<h2>Friday 21st July 1100am–1230pm</h2>

</div>

<div style='border:none;border-bottom:solid windowtext 1.0pt;mso-border-bottom-alt:
solid windowtext .5pt;padding:0in 0in 1.0pt 0in'>

<h3>14A: Parsing VII</h3>

</div>

<p class=SessionChair>Session Chair: Srinivas Bangalore<span style='mso-tab-count:
7'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b14A1></a>An All-Subtrees Approach to
Unsupervised Parsing</p>

<p class=AbstractAuthor>Rens Bod<!--[if supportFields]><span style='mso-element:
field-begin'></span> XE &quot;Bod, R.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText>We investigate generalizations of the all-subtrees &quot;DOP&quot;
approach to unsupervised parsing. Unsupervised DOP models assign all possible
binary trees to a set of sentences<span style='mso-ansi-language:EN-GB'> </span>and
next use (a large random subset of) all subtrees from these binary trees to
compute the most probable parse trees. We will test both a relative frequency
estimator for unsupervised DOP and a maximum likelihood estimator which is
known to be statistically consistent. We report state-of-the-art results on
English (WSJ), German (NEGRA) and Chinese (<st1:stockticker>CTB</st1:stockticker>)
data. To the best of our knowledge this is the first paper which tests a
maximum likelihood estimator for DOP on the Wall Street Journal, leading to the
surprising result that an unsupervised parsing model beats a widely used
supervised model (a treebank PCFG).</p>

<p class=AbstractTitle><a name=b14A2></a>Advances in Discriminative Parsing</p>

<p class=AbstractAuthor>Joseph Turian<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Turian, J.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and I. Dan Melamed<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Melamed, I.D.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText><span style='mso-font-kerning:0pt'>The present work
advances the accuracy and training speed of discriminative parsing. Our
discriminative parsing method has no generative component, yet surpasses a
generative baseline on constituent parsing, and does so with minimal linguistic
cleverness. Our model can incorporate arbitrary features of the input and parse
state, and performs feature selection incrementally over an exponential feature
space during training. We demonstrate the flexibility of our approach by
testing it with several parsing strategies and various feature sets. Our
implementation is freely available at: http://nlp.cs.nyu.edu/parser/.<o:p></o:p></span></p>

<p class=AbstractTitle><a name=b14A3></a>Prototype-Driven Grammar Induction</p>

<p class=AbstractAuthor>Aria Haghighi<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Haghighi, A.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Dan Klein<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Klein, D.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText><span style='mso-font-kerning:0pt'>We investigate
prototype-driven learning for primarily unsupervised grammar induction. Prior
knowledge is specified declaratively, by providing a few canonical examples of
each target phrase type. This sparse prototype information is then propagated
across a corpus using distributional similarity features, which augment an
otherwise standard PCFG model. We show that distributional features are
effective at distinguishing bracket labels, but not determining bracket
locations. To improve the quality of the induced trees, we combine our PCFG
induction with the CCM model of Klein and Manning (2002), which has
complementary strengths: it identifies brackets but does not label them. Using
only a handful of prototypes, we show substantial improvements over naive PCFG
induction for English and Chinese grammar induction.<o:p></o:p></span></p>

<div style='border:none;border-bottom:solid windowtext 1.0pt;mso-border-bottom-alt:
solid windowtext .5pt;padding:0in 0in 1.0pt 0in'>

<h3>14B: Question Answering II</h3>

</div>

<p class=SessionChair>Session Chair: Dan Moldovan<span style='mso-tab-count:
7'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b14B1></a>Exploring Correlation of Dependency
Relation Paths for Answer Extraction</p>

<p class=AbstractAuthor>Dan Shen<!--[if supportFields]><span style='mso-element:
field-begin'></span> XE &quot;Shen, D.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Dietrich Klakow<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Klakow, D.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText>In this paper, we explore correlation of dependency
relation paths to rank candidate answers in answer extraction. Using the
correlation measure, we compare dependency relations of a candidate answer and
mapped question phrases in sentence with the corresponding relations in
question. Different from previous studies, we propose an approximate
phrase-mapping algorithm and incorporate the mapping score into the correlation
measure. The correlations are further incorporated into a Maximum Entropy-based
ranking model which estimates path weights from training. Experimental results
show that our method significantly outperforms state-of-the-art syntactic
relation-based methods by up to 20% in <st1:stockticker>MRR</st1:stockticker>.</p>

<p class=AbstractTitle><a name=b14B2></a>Question Answering with Lexical Chains
Propagating Verb Arguments</p>

<p class=AbstractAuthor>Adrian Novischi<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Novischi, A.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Dan Moldovan<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Moldovan, D.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText>This paper describes an algorithm for propagating verb
arguments along lexical chains consisting of WordNet relations. The algorithm
creates verb argument structures using VerbNet syntactic patterns. In order to
increase the coverage, a larger set of verb senses were automatically
associated with the existing patterns from VerbNet. The algorithm is used in an
in-house Question Answering system for re-ranking the set of candidate answers.
Tests on factoid questions from TREC 2004 indicate that the algorithm improved
the system performance by 2.4%.</p>

<p class=AbstractTitle><a name=b14B3></a>Methods for Using Textual Entailment
in Open-Domain Question Answering</p>

<p class=AbstractAuthor>Sanda Harabagiu<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Harabagiu, S.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Andrew Hickl<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Hickl, A.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText><span style='mso-font-kerning:0pt'>Work on the semantics
of questions has argued that the relation between a question and its answer(s)
can be cast in terms of logical entailment. In this paper, we demonstrate how
computational systems designed to recognize textual entailment can be used to
enhance the accuracy of current open-domain automatic question answering (Q/A)
systems. In our experiments, we show that when textual entailment information
is used to either filter or rank answers returned by a Q/A system, accuracy can
be increased by as much as 20% overall.</span><span style='mso-tab-count:6'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<div style='border:none;border-bottom:solid windowtext 1.0pt;mso-border-bottom-alt:
solid windowtext .5pt;padding:0in 0in 1.0pt 0in'>

<h3>14C: Semantics IV</h3>

</div>

<p class=SessionChair>Session Chair: Alexander Koller<span style='mso-tab-count:
7'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b14C1></a>Using String-Kernels for Learning
Semantic Parsers</p>

<p class=AbstractAuthor>Rohit J. Kate<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Kate, R.J.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Raymond J. Mooney<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Mooney, R.J.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText><span lang=EN-GB style='mso-ansi-language:EN-GB'>We
present a new approach for mapping natural language sentences to their formal
meaning representations using string-kernel-based classifiers. Our system
learns these classifiers for every production in the formal language grammar.
Meaning representations for novel natural language sentences are obtained by
finding the most probable semantic parse using these string classifiers. Our
experiments on two real-world data sets show that this approach compares
favorably to other existing systems and is particularly robust to noise.</span><span
style='mso-tab-count:6'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b14C2></a>A Bootstrapping Approach to
Unsupervised Detection of Cue Phrase Variants</p>

<p class=AbstractAuthor>Rashid M. Abdalla<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Abdalla, R.M.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Simone Teufel<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Teufel, S.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText><span style='mso-font-kerning:0pt'>We investigate the unsupervised
detection of semi-fixed cue phrases such as “This paper proposes a novel
approach …” from unseen text, on the basis of only a handful of seed cue
phrases with the desired semantics. The problem, in contrast to bootstrapping
approaches for Question Answering and Information Extraction, is that it is
hard to find a constraining context for occurrences of semi-fixed cue phrases.
Our method uses components of the cue phrase itself, rather than external
context, to bootstrap. It successfully excludes phrases which are different
from the target semantics, but which look superficially similar. The method
achieves 88% accuracy, outperforming standard bootstrapping approaches.<o:p></o:p></span></p>

<p class=AbstractTitle><a name=b14C3></a>Semantic Role Labeling via FrameNet,
VerbNet and PropBank </p>

<p class=AbstractAuthor>Ana-Maria Giuglea<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Giuglea, A-M.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Alessandro Moschitti<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Moschitti, A.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText><span style='mso-font-kerning:0pt'>This article describes
a robust semantic parser that uses a broad knowledge base created by
interconnecting three major resources: FrameNet, VerbNet and PropBank. The
FrameNet corpus contains the examples annotated with semantic roles whereas the
VerbNet lexicon provides the knowledge about the syntactic behavior of the
verbs. We connect VerbNet and FrameNet by mapping the FrameNet frames to the
VerbNet Intersective Levin classes. The PropBank corpus, which is tightly
connected to the VerbNet lexicon, is used to increase the verb coverage and
also to test the effectiveness of our approach. The results indicate that our
model is an interesting step towards the design of more robust semantic parsers.<o:p></o:p></span></p>

<div style='border:none;border-bottom:solid windowtext 1.0pt;mso-border-bottom-alt:
solid windowtext .5pt;padding:0in 0in 1.0pt 0in'>

<h3>14D: Resources II</h3>

</div>

<p class=SessionChair>Session Chair:<span style='mso-spacerun:yes'>&nbsp;
</span>Eva Haji&#269;ov&aacute;<span style='mso-tab-count:7'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b14D1></a>Multilingual Legal Terminology on the
Jibiki Platform: The LexALP Project</p>

<p class=AbstractAuthor><span lang=FR style='mso-ansi-language:FR'>Gilles
S&eacute;rasset</span><!--[if supportFields]><span style='mso-element:field-begin'></span><span
lang=FR style='mso-ansi-language:FR'> XE &quot;S&eacute;rasset, G.&quot; </span><![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span lang=FR
style='mso-ansi-language:FR'>, Francis Brunet-Manquat</span><!--[if supportFields]><span
lang=FR style='mso-ansi-language:FR'><span style='mso-element:field-begin'></span>
XE &quot;Brunet-Manquat, F.&quot; </span><![endif]--><!--[if supportFields]><span
lang=FR style='mso-ansi-language:FR'><span style='mso-element:field-end'></span></span><![endif]--><span
lang=FR style='mso-ansi-language:FR'><span
style='mso-spacerun:yes'>&nbsp;</span>and Elena Chiocchetti</span><!--[if supportFields]><span
lang=FR style='mso-ansi-language:FR'><span style='mso-element:field-begin'></span>
XE &quot;Chiocchetti, E.&quot; </span><![endif]--><!--[if supportFields]><span
lang=FR style='mso-ansi-language:FR'><span style='mso-element:field-end'></span></span><![endif]--><span
lang=FR style='mso-ansi-language:FR'><o:p></o:p></span></p>

<p class=MsoBodyText><span style='mso-font-kerning:0pt'>This paper presents the
particular use of</span><span style='mso-font-kerning:0pt;mso-ansi-language:
EN-GB'> </span><span style='mso-font-kerning:0pt'>“Jibiki” (Papillon’s web
server development</span><span style='mso-font-kerning:0pt;mso-ansi-language:
EN-GB'> </span><span style='mso-font-kerning:0pt'>platform) for the LexALP1
project.</span><span style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>LexALP’s goal is to harmonise the terminology</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>on spatial planning and sustainable</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>development used within the Alpine</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>Convention2, so that the member states</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>are able to cooperate and communicate</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>efficiently in the four official languages</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>(French, German, Italian and Slovene). To</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>this purpose, LexALP uses the Jibiki platform</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>to build a term bank for the contrastive</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>analysis of the specialised terminology</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>used in six different national legal</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>systems and four different languages. In</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>this paper we present how a generic platform</span><span
style='mso-font-kerning:0pt;mso-ansi-language:EN-GB'> </span><span
style='mso-font-kerning:0pt'>like Jibiki can cope with a new kind of
dictionary.</span><span style='mso-tab-count:6'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b14D2></a>Leveraging Reusability: Cost-effective
Lexical Acquisition for Large-scale Ontology Translation</p>

<p class=AbstractAuthor>G. Craig Murray<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Murray, G.C.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Bonnie Dorr<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Dorr, B.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Jimmy Lin<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Lin, J.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Jan Haji&#269;<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Haji&#269;, J.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Pavel Pecina<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Pecina, P.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText><span style='mso-font-kerning:0pt'>Thesauri and ontologies
provide important value in facilitating access to digital archives by
representing underlying principles of organization. Translation of such
resources into multiple languages is an important component for providing
multilingual access.<span style='mso-spacerun:yes'>&nbsp; </span>However, the
specificity of vocabulary terms in most ontologies precludes fully-automated
machine translation using general-domain lexical resources. In this paper, we
present an efficient process for leveraging human translations when
constructing domain-specific lexical resources. We evaluate the effectiveness
of this process by producing a probabilistic phrase dictionary and translating
a thesaurus of 56,000 concepts used to catalogue a large archive of oral
histories. Our experiments demonstrate a cost-effective technique for accurate machine
translation of large ontologies.</span><span style='mso-tab-count:1'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b14D3></a>Accurate Collocation Extraction Using
a Multilingual Parser</p>

<p class=AbstractAuthor>Violeta Seretan<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Seretan, V.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Eric Wehrli<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Wehrli, E.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText>This paper focuses on the use of advanced techniques of
text analysis as support for collocation extraction. A hybrid system is
presented that combines statistical methods and multilingual parsing for
detecting accurate collocational information from English, French, Spanish and
Italian corpora. The advantage of relying on full parsing over using a
traditional window method (which ignores the syntactic information) is first
theoretically motivated, then empirically validated by a comparative evaluation
experiment.</p>

<div style='border:solid windowtext 1.0pt;mso-border-alt:solid windowtext .5pt;
padding:1.0pt 4.0pt 1.0pt 4.0pt'>

<h2>Friday 21st July 200pm–330pm</h2>

</div>

<div style='border:none;border-bottom:solid windowtext 1.0pt;mso-border-bottom-alt:
solid windowtext .5pt;padding:0in 0in 1.0pt 0in'>

<h3>15A: Machine Translation VI</h3>

</div>

<p class=SessionChair>Session Chair: Dekai Wu<span style='mso-tab-count:7'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b15A1></a>Scalable Inference and Training of
Context-rich Syntactic Translation Models</p>

<p class=AbstractAuthor>Michel Galley<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Galley, M.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Jonathan Graehl<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Graehl, J.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Kevin Knight<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Knight, K.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Daniel Marcu<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Marcu, D.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Steve DeNeefe<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;DeNeefe, S.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Wei Wang<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Wang, W.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Ignacio Thayer<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Thayer, I.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText>Statistical MT has made great progress in the last few
years, but current translation models are weak on re-ordering and target
language fluency. Syntactic approaches seek to remedy these problems.<span
style='mso-spacerun:yes'>&nbsp; </span>In this paper, we take the framework for
acquiring multi-level syntactic translation rules of (Galley et al., 2004) from
aligned tree-string pairs, and present two main extensions of their approach:
first, instead of merely computing a single derivation that minimally explains
a sentence pair, we construct a large number of derivations that include
contextually richer rules, and account for multiple interpretations of
unaligned words.<span style='mso-spacerun:yes'>&nbsp; </span>Second, we propose
probability estimates and a training procedure for weighting these rules.<span
style='mso-spacerun:yes'>&nbsp; </span>We contrast different approaches on real
examples, show that our estimates based on multiple derivations favor phrasal
re-orderings that are linguistically better motivated, and establish that our
larger rules provide a 3.63 BLEU point increase over minimal rules.</p>

<p class=AbstractTitle><a name=b15A2></a>Modelling lexical redundancy for
machine translation</p>

<p class=AbstractAuthor>David Talbot<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Talbot, D.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Miles Osborne<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Osborne, M.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText>Certain distinctions made in the lexicon of one language
may be redundant when translating into another language. We quantify redundancy
among source types by the similarity of their distributions over target types.
We propose a language-independent framework for minimising lexical redundancy
that can be optimised directly from parallel text. Optimisation of the source
lexicon for a given target language is viewed as model selection over a set of
cluster-based translation models.</p>

<p class=MsoBodyText>Redundant distinctions between types may exhibit
monolingual regularities, for example, inflexion patterns. We define a prior
over model structure using a Markov random field and learn features over sets
of monolingual types that are predictive of bilingual redundancy. The prior
makes model selection more robust without the need for language-specific
assumptions regarding redundancy. Using these models in a phrase-based <st1:stockticker>SMT</st1:stockticker>
system, we show significant improvements in translation quality for certain
language pairs.<span style='mso-tab-count:1'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b15A3></a>Empirical Lower Bounds on the
Complexity of Translational Equivalence</p>

<p class=AbstractAuthor>Benjamin Wellington<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Wellington, B.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Sonjia Waxmonsky<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Waxmonsky, S.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and <span style='mso-bidi-font-weight:
bold'>I. Dan Melamed</span><!--[if supportFields]><span style='mso-bidi-font-weight:
bold'><span style='mso-element:field-begin'></span></span> XE &quot;Melamed,
I.D.&quot; <![endif]--><!--[if supportFields]><span style='mso-bidi-font-weight:
bold'><span style='mso-element:field-end'></span></span><![endif]--></p>

<p class=MsoBodyText><span style='mso-font-kerning:0pt'>This paper describes a
study of the patterns of translational equivalence exhibited by a variety of
bitexts. The study found that the complexity of these patterns in every bitext
was higher than suggested in the literature. These findings shed new light on
why “syntactic” constraints have not helped to improve statistical translation
models, including finite state phrase-based models, tree-to-string models, and
tree-to-tree models. The paper also presents evidence that inversion
transduction grammars cannot generate some translational equivalence relations,
even in relatively simple real bitexts in syntactically similar languages with
rigid word order. Instructions for replicating our experiments are at
http://nlp.cs.nyu.edu/GenPar/</span><st1:stockticker><span style='mso-font-kerning:
 0pt'>ACL</span></st1:stockticker><span style='mso-font-kerning:0pt'>06<o:p></o:p></span></p>

<div style='border:none;border-bottom:solid windowtext 1.0pt;mso-border-bottom-alt:
solid windowtext .5pt;padding:0in 0in 1.0pt 0in'>

<h3>15B: Language Modelling</h3>

</div>

<p class=SessionChair>Session Chair: Jianfeng Gao<span style='mso-tab-count:
7'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b15B1></a>A Hierarchical Bayesian Language Model
Based On Pitman-Yor Processes</p>

<p class=AbstractAuthor>Yee Whye Teh<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Teh, Y.W.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText><span style='mso-font-kerning:0pt'>We propose a new hierarchical
Bayesian n-gram model of natural languages. Our model makes use of a
generalization of the commonly used Dirichlet distributions called Pitman-Yor
processes which produce power-law distributions more closely resembling those
in natural languages. We show that an approximation to the hierarchical
Pitman-Yor language model recovers the exact formulation of interpolated
Kneser-Ney, one of the best smoothing methods for n-gram language models.
Experiments verify that our model gives cross entropy results superior to
interpolated Kneser-Ney and comparable to modified Kneser-Ney.</span><span
style='mso-tab-count:1'>&nbsp;&nbsp; </span><span style='mso-tab-count:5'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b15B2></a>A Phonetic-Based Approach to Chinese
Chat Text Normalization</p>

<p class=AbstractAuthor>Yunqing Xia<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Xia, Y.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Kam-Fai Wong<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Wong, K-F.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Wenjie Li<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Li, W.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText>Chatting is a popular communication media on the Internet
via ICQ, chat rooms, etc. Chat language is different from natural language due
to its anomalous and dynamic natures, which renders conventional NLP tools
inapplicable. The dynamic problem is enormously troublesome because it makes
static chat language corpus outdated quickly in representing contemporary chat
language. To address the dynamic problem, we propose the phonetic mapping
models to present mappings between chat terms and standard words via phonetic
transcription, i.e. Chinese Pinyin in our case. Different from character
mappings, the phonetic mappings can be constructed from available standard
Chinese corpus. To perform the task of dynamic chat language term
normalization, we extend the source channel model by incorporating the phonetic
mapping models. Experimental results show that this method is effective and
stable in normalizing dynamic chat language terms.<span style='mso-tab-count:
1'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b15B3></a>Discriminative Pruning of Language
Models for Chinese Word Segmentation</p>

<p class=AbstractAuthor>Jianfeng Li<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Li, J.(1)&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Haifeng Wang<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Wang, H.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Dengjun Ren<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Ren, D.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Guohua Li<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Li, G.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText><span lang=EN-GB style='mso-ansi-language:EN-GB'>This
paper presents a discriminative pruning method of n-gram language model for
Chinese word segmentation. To reduce the size of the language model that is
used in a Chinese word segmentation system, importance of each bigram is
computed in terms of discriminative pruning criterion that is related to the
performance loss caused by pruning the bigram. Then we propose a step-by-step
growing algorithm to build the language model of desired size. Experimental
results show that the discriminative pruning method leads to a much smaller
model compared with the model pruned using the state-of-the-art method. At the
same Chinese word segmentation F-measure, the number of bigrams in the model
can be reduced by up to 90%. Correlation between language model perplexity and
word segmentation performance is also discussed.</span><span style='mso-tab-count:
5'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<div style='border:none;border-bottom:solid windowtext 1.0pt;mso-border-bottom-alt:
solid windowtext .5pt;padding:0in 0in 1.0pt 0in'>

<h3>15C: Information Retrieval II</h3>

</div>

<p class=SessionChair>Session Chair: Rosie Jones<span style='mso-tab-count:
7'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b15C1></a>Novel Association Measures Using Web
Search with Double Checking</p>

<p class=AbstractAuthor>Hsin-Hsi Chen<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Chen, H-H.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Ming-Shun Lin<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Lin, M-S.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Yu-Chuan Wei<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Wei, Y-C.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText>A web search with double checking model is proposed to
explore the web as a live corpus.<span style='mso-spacerun:yes'>&nbsp;
</span>Five association measures including variants of Dice, Overlap Ratio,
Jaccard, and Cosine, as well as Co-Occurrence Double Check (CODC), are
presented. In the experiments on Rubenstein-Goodenough’s benchmark data set,
the CODC measure achieves correlation coefficient 0.8492, which competes with
the performance (0.8914) of the model using WordNet. The experiments on link
detection of named entities using the strategies of direct association,
association matrix and scalar association matrix verify that the double-check
frequencies are reliable. Further study on named entity clustering shows that
the five measures are quite useful. In particular, CODC measure is very stable
on word-word and name-name experiments. The application of CODC measure to
expand community chains for personal name disambiguation achieves 9.65% and
14.22% increase compared to the system without community expansion. All the
experiments illustrate that the novel model of web search with double-checking
is feasible for mining associations from the web.</p>

<p class=AbstractTitle><a name=b15C2></a>Semantic Retrieval for the Accurate
Identification of Relational Concepts in Massive Textbases </p>

<p class=AbstractAuthor>Yusuke Miyao<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Miyao, Y.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Tomoko Ohta<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Ohta, T.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Katsuya Masuda<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Masuda, K.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Yoshimasa Tsuruoka<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Tsuruoka, Y.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Kazuhiro Yoshida<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Yoshida, K.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Takashi Ninomiya<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Ninomiya, T.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Jun'ichi Tsujii<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Tsujii, J.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText>This paper introduces a novel framework for the accurate
retrieval of relational concepts from huge texts.<span
style='mso-spacerun:yes'>&nbsp; </span>Prior to retrieval, all sentences are
annotated with predicate argument structures and ontological identifiers by
applying a deep parser and a term recognizer.<span
style='mso-spacerun:yes'>&nbsp; </span>During the run time, user requests are
converted into queries of region algebra on these annotations.<span
style='mso-spacerun:yes'>&nbsp; </span>Structural matching with pre-computed
semantic annotations establishes the accurate and efficient retrieval of
relational concepts.<span style='mso-spacerun:yes'>&nbsp; </span>This framework
was applied to a text retrieval system for MEDLINE.<span
style='mso-spacerun:yes'>&nbsp; </span>Experiments on the retrieval of biomedical
correlations revealed that the cost is sufficiently small for real-time
applications and that the retrieval precision is significantly improved.</p>

<p class=AbstractTitle><a name=b15C3></a>Exploring Distributional Similarity
Based Models for Query Spelling Correction</p>

<p class=AbstractAuthor>Mu Li<!--[if supportFields]><span style='mso-element:
field-begin'></span> XE &quot;Li, M.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Muhua Zhu<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Zhu, M.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Yang Zhang<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Zhang, Y.(1)&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Ming Zhou<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Zhou, M.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText>A query speller is crucial to search engine in improving
web search relevance. This paper describes novel methods for use of
distributional similarity estimated from query logs in learning improved query
spelling correction models. The key to our methods is the property of
distributional similarity between two terms: it is high between a frequently
occurring misspelling and its correction, and low between two irrelevant terms
only with similar spellings. We present two models that are able to take
advantage of this property. Experimental results demonstrate that the
distributional similarity based models can significantly outperform their
baseline systems in the web query spelling correction task.</p>

<div style='border:none;border-bottom:solid windowtext 1.0pt;mso-border-bottom-alt:
solid windowtext .5pt;padding:0in 0in 1.0pt 0in'>

<h3>15D: Generation I</h3>

</div>

<p class=SessionChair>Session Chair: Donia Scott<span style='mso-tab-count:
7'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b15D1></a>Robust PCFG-Based Generation using
Automatically Acquired <st1:stockticker>LFG</st1:stockticker> Approximations </p>

<p class=AbstractAuthor>Aoife Cahill<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Cahill, A.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Josef van Genabith<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;van Genabith, J.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText>We present a novel PCFG-based architecture for robust
probabilistic generation based on wide-coverage <st1:stockticker>LFG</st1:stockticker>

approximations (Cahill et al., 2004) automatically extracted from treebanks,
maximising the probability of a tree given an f-structure. We evaluate our
approach using string-based evaluation. We currently achieve coverage of
95.26%, a BLEU score of 0.7227 and string accuracy of 0.7476 on the Penn-II WSJ
Section 23 sentences of length &#8804;20.</p>

<p class=AbstractTitle><a name=b15D2></a>Incremental generation of spatial
referring expressions in situated dialog</p>

<p class=AbstractAuthor>John D. Kelleher<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Kelleher, J.D.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Geert-Jan M. Kruijff<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Kruijff, G-J. M.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText>This paper presents an approach to incrementally
generating locative expressions. It addresses the issue of combinatorial
explosion inherent in the construction of relational context models by: (a)
contextually defining the set of objects in the context that may function as a
landmark, and (b) sequencing the order in which spatial relations are
considered using a cognitively motivated hierarchy of relations, and visual and
discourse salience.<span style='mso-tab-count:5'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b15D3></a>Learning to Predict Case Markers in
Japanese</p>

<p class=AbstractAuthor><span lang=FR style='mso-ansi-language:FR'>Hisami
Suzuki</span><!--[if supportFields]><span style='mso-element:field-begin'></span><span
lang=FR style='mso-ansi-language:FR'> XE &quot;Suzuki, H.&quot; </span><![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span lang=FR
style='mso-ansi-language:FR'><span style='mso-spacerun:yes'>&nbsp;</span>and
Kristina Toutanova</span><!--[if supportFields]><span style='mso-element:field-begin'></span><span
lang=FR style='mso-ansi-language:FR'> XE &quot;Toutanova, K.&quot; </span><![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span lang=FR
style='mso-ansi-language:FR'><o:p></o:p></span></p>

<p class=MsoBodyText>Japanese case markers, which indicate the grammatical
relation of the complement NP to the predicate, often pose challenges to the
generation of Japanese text, be it done by a foreign language learner, or by a
machine translation (MT) system. In this paper, we describe the task of
predicting Japanese case markers and propose machine learning methods for
solving it in two settings: (i) monolingual, when given information only from
the Japanese sentence; and (ii) bilingual, when also given information from a
corresponding English source sentence in an MT context. We formulate the task
after the well-studied task of English semantic role labelling, and explore
features from a syntactic dependency structure of the sentence. For the
monolingual task, we evaluated our models on the Kyoto Corpus and achieved over
84% accuracy in assigning correct case markers for each phrase. For the
bilingual task, we achieved an accuracy of 92% per phrase using a bilingual
dataset from a technical domain. We show that in both settings, features that
exploit dependency information, whether derived from gold-standard annotations
or automatically assigned, contribute significantly to the prediction of case
markers.</p>

<div style='border:solid windowtext 1.0pt;mso-border-alt:solid windowtext .5pt;
padding:1.0pt 4.0pt 1.0pt 4.0pt'>

<h2>Friday 21st July 400pm–500pm</h2>

</div>

<div style='border:none;border-bottom:solid windowtext 1.0pt;mso-border-bottom-alt:
solid windowtext .5pt;padding:0in 0in 1.0pt 0in'>

<h3>16A: Text Classification II</h3>

</div>

<p class=SessionChair>Session Chair: Peter Turney<span style='mso-tab-count:
7'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b16A1></a>Are These Documents Written from
Different Perspectives? A Test of Different Perspectives Based On Statistical
Distribution Divergence</p>

<p class=AbstractAuthor>Wei-Hao Lin<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Lin, W-H.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Alexander Hauptmann<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Hauptmann, A.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText>In this paper we investigate how to automatically
determine if two document collections are written from different
perspectives.<span style='mso-spacerun:yes'>&nbsp; </span>By perspectives we
mean a point of view, for example, from the perspective of Democrats or
Republicans.<span style='mso-spacerun:yes'>&nbsp; </span>We propose a test of
different perspectives based on distribution divergence between the statistical
models of two collections.<span style='mso-spacerun:yes'>&nbsp;
</span>Experimental results show that the test can successfully distinguish
document collections of different perspectives from other types of collections.</p>

<p class=AbstractTitle><a name=b16A2></a>Word Sense and Subjectivity </p>

<p class=AbstractAuthor>Janyce Wiebe<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Wiebe, J.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Rada Mihalcea<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Mihalcea, R.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText><span style='mso-font-kerning:0pt'>Subjectivity and
meaning are both important properties of language. This paper explores their
interaction, and brings empirical evidence in support of the hypotheses that
(1) subjectivity is a property that can be associated with word senses, and (2)
word sense disambiguation can directly benefit from subjectivity annotations.<o:p></o:p></span></p>

<div style='border:none;border-bottom:solid windowtext 1.0pt;mso-border-bottom-alt:
solid windowtext .5pt;padding:0in 0in 1.0pt 0in'>

<h3>16B: Question Answering III</h3>

</div>

<p class=SessionChair>Session Chair: John Prange<span style='mso-tab-count:
7'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b16B1></a>Improving QA Accuracy by Question
Inversion</p>

<p class=AbstractAuthor>John Prager<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Prager, J.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Pablo Duboue<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Duboue, P.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Jennifer Chu-Carroll<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Chu-Carroll, J.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText><span lang=EN-GB style='mso-ansi-language:EN-GB'>This
paper demonstrates a conceptually simple but effective method of increasing the
accuracy of QA systems on factoid-style questions.<span
style='mso-spacerun:yes'>&nbsp; </span>We define the notion of an inverted
question, and show that by requiring that the answers to the original and
inverted questions be mutually consistent, incorrect answers get demoted in
confidence and correct ones promoted.<span style='mso-spacerun:yes'>&nbsp;
</span>Additionally, we show that lack of validation can be used to assert
no-answer (nil) conditions.<span style='mso-spacerun:yes'>&nbsp; </span>We
demonstrate increases of performance on TREC and other question-sets, and
discuss the kinds of future activities that can be particularly beneficial to
approaches such as ours.</span><span style='mso-tab-count:6'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b16B2></a>Reranking Answers for Definitional QA
Using Language Modeling</p>

<p class=AbstractAuthor>Yi Chen<!--[if supportFields]><span style='mso-element:
field-begin'></span> XE &quot;Chen, Y.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]-->, Ming Zhou<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Zhou, M.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Shilong Wang<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Wang, S.(2)&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText>Statistical ranking methods based on centroid vector
(profile) extracted from ex-ternal knowledge have become widely adopted in the
top definitional QA systems in TREC 2003 and 2004. In these approaches, terms
in the centroid vector are treated as a bag of words based on the independent
assumption. To relax this assumption, this paper proposes a novel language
model-based answer reranking method to improve the existing bag-of-words model
approach by considering the dependence of the words in the centroid vector.
Experiments have been conducted to evaluate the different dependence models.
The results on the TREC 2003 test set show that the reranking approach with
biterm language model, significantly outperforms the one with the bag-of-words
model and unigram language model by 14.9% and 12.5% respectively in
F-Measure(5).</p>

<div style='border:none;border-bottom:solid windowtext 1.0pt;mso-border-bottom-alt:
solid windowtext .5pt;padding:0in 0in 1.0pt 0in'>

<h3>16C: Grammars III</h3>

</div>

<p class=SessionChair>Session Chair: Gerald Penn<span style='mso-tab-count:
7'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b16C1></a>Highly constrained unification
grammars</p>

<p class=AbstractAuthor>Daniel Feinstein<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Feinstein, D.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Shuly Wintner<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Wintner, S.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText><span style='mso-font-kerning:0pt'>Unification grammars
are widely accepted as an expressive means for describing the structure of
natural languages. In general, the recognition problem is undecidable for
unification grammars. Even with restricted variants of the formalism, offline
parsable grammars, the problem is computationally hard. We present two natural
constraints on unification grammars which limit their expressivity. We first
show that non-reentrant unification grammars generate exactly the class of
context-free languages. We then relax the constraint and show that
one-reentrant unification grammars generate exactly the class of tree-adjoining
languages. We thus relate the commonly used and linguistically motivated
formalism of unification grammars to more restricted, computationally tractable
classes of languages.<o:p></o:p></span></p>

<p class=AbstractTitle><a name=b16C2></a>A polynomial parsing algorithm for the
topological model Synchronizing Constituent and Dependency Grammars,
Illustrated by German Word Order Phenomena</p>

<p class=AbstractAuthor>Kim Gerdes<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Gerdes, K.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Sylvain Kahane<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Kahane, S.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText>This paper describes a minimal topology driven parsing
algorithm for topological grammars that synchronizes a rewriting grammar and a
dependency grammar, obtaining two linguistically motivated syntactic
structures. The use of non-local slash and visitor features can be restricted
to obtain a CKY type analysis in polynomial time. German long distance
phenomena illustrate the algorithm, bringing to the fore the procedural needs
of the analyses of syntax-topology mismatches in constraint based approaches
like for example HPSG.</p>

<div style='border:none;border-bottom:solid windowtext 1.0pt;mso-border-bottom-alt:
solid windowtext .5pt;padding:0in 0in 1.0pt 0in'>

<h3>16D: Generation II</h3>

</div>

<p class=SessionChair>Session Chair: Donia Scott<span style='mso-tab-count:
7'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>

<p class=AbstractTitle><a name=b16D1></a>Stochastic Language Generation Using
WIDL-expressions and its Application in Machine Translation and Summarization</p>

<p class=AbstractAuthor>Radu Soricut<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Soricut, R.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Daniel Marcu<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Marcu, D.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText>We propose WIDL-expressions as a flexible formalism that
facilitates the integration of a generic sentence realization system within
end-to-end language processing applications. WIDL-expressions represent
compactly probability distributions over finite sets of candidate realizations,
and have optimal algorithms for realization via interpolation with language
model probability distributions. We show the effectiveness of a WIDL-based NLG
system in two sentence realization tasks: automatic translation and headline
generation.</p>

<p class=AbstractTitle><a name=b16D2></a>Learning to Say It Well: Reranking
Realizations by Predicted Synthesis Quality</p>

<p class=AbstractAuthor>Crystal Nakatsu<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;Nakatsu, C.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
style='mso-spacerun:yes'>&nbsp;</span>and Michael White<!--[if supportFields]><span
style='mso-element:field-begin'></span> XE &quot;White, M.&quot; <![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--></p>

<p class=MsoBodyText><span style='mso-font-kerning:0pt'>This paper presents a
method for adapting a language generator to the strengths and weaknesses of a
synthetic voice, thereby improving the naturalness of synthetic speech in a
spoken language dialogue system. The method trains a discriminative reranker to
select paraphrases that are predicted to sound natural when synthesized. The
ranker is trained on realizer and synthesizer features in supervised fashion,
using human judgments of synthetic voice quality on a sample of the paraphrases
representative of the generator’s capability. Results from a cross-validation
study indicate that discriminative paraphrase reranking can achieve substantial
improvements in naturalness on average, ameliorating the problem of highly
variable synthesis quality typically encountered with today’s unit selection
synthesizers.<o:p></o:p></span></p>

   
</div>

<hr class="low" /><!-- end content --> 
    <!-- end main content --> 
  </div>

 
 <hr class="low" /> 

  <div id="footer"> 
    <p> 
       
        Questions, comments? Contact the <a href='&#109&#97&#105&#108&#116o:bpowl%65%79%40%69%63s%2emq%2e%65d%75.%61%75'>&#119&#101bma&#115&#116e&#114</a>
  </div>

</div> 
 
</body>
</html>
