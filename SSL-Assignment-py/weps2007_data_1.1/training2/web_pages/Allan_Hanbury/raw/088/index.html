<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
    <title>Muscle - WP5 - Scientific Meetings</title>
</head>

<body>

<a NAME="top"</a>

<table BORDER=0 CELLSPACING=0 CELLPADDING=5 COLS=0 WIDTH="100%">
<td bgcolor=#bbbbbb><font size=+1><b>Scientific meetings</b></font></td>
</table>
<br>

<center>
<h2>MUSCLE 4th Scientific Meeting</h2>
<font size=+1>February 15-17 2006, Istanbul, Turkey</font>
</center>

<br>

<p align="justify">
MUSCLE's 4th scientific meeting  will be held in Istanbul on Feb 15-17, 2006 and will be hosted by Enis Cetin. See
<a href="http://www.muscle-noe.net/events/2006_02_15_Istanbul/istanbul_welcome.php" target=new>here</a> for the official webpages of the meeting and the agenda.
</p>

<br>

<table BORDER=0 CELLSPACING=0 CELLPADDING=5 COLS=0 WIDTH="100%">
<td bgcolor=#eeeeee><font size=+1><b>Exhibitions related to WP5</b></font></td>
</table>

<br>

<h3>Showcases and demo session</h3>

<ul>
<li><b>Horst Bischof and Martina Uray (TU Graz):</b> On-line Boosting Applications in Vision.<br>
<p align="justify"><b>Abstract:</b> The videos/demos will demonstrate several recent applications of on-line boosting in particular complex background modelling, tracking and object recognition. The used algorithms enable real time processing.</p>
<li><b>Enis Cetin (Bilkent):</b> Fire and Smoke Detection in Video.<br><br>
<li><b>Enis Cetin (Bilkent):</b> Detecting faces and falling people.<br><br>
<li><b>Panos Trahanias and Antonis Argyros (FORTH):</b> Hand and finger detection and tracking.<br><br>
<li><b>Andreas Rauber (TU Wien - IFS):</b> PlaySOM and PocketSOMPlayer: Alternative Interfaces to Large Music Collections.<br>
<p align="justify"><b>Abstract:</b> With the rising popularity of digital music archives the need for new access methods such as interactive exploration or similarity-based search become significant. Those methods demand more sophisticated technologies than meta data based approaches currently offer, including queries for artists, albums, titles or manually assigned genre information. Especially end-users, whose collections are more and more often stored uniformly in a library of, e.g., MP3 or OGG files, should be able to chose music according to their mood based on similarity. The SOMeJB system provides automatic indexing and organization of music repositories based on perceived sound similarity of single tracks. A a map metaphor is for visualization with similar songs being placed into similar regions on the map.<br>

We present the PlaySOM, as well as the PocketSOMPlayer, two novel interfaces that enable one to browse a music collection by navigating a map of clustered music tracks and to select regions of interest containing similar tracks for playing. The PlaySOM system is primarily designed to allow interaction via a large-screen device, whereas the PocketSOMPlayer is implemented for mobile devices, supporting both local as well as streamed audio replay. This approach offers content-based organization of music as an alternative to conventional navigation of audio archives, i.e. flat or hierarchical listings of music tracks that are sorted and filtered by meta information.</p>
<li><b>Bill Christmas (UNIS):</b> Automatic annotation of sports videos.<br>
<p align="justify"><b>Abstract:</b> We are developing a system to generate completely automatic annotation of tennis matches. The system currently classifies shots, tracks the players and the ball, estimates the camera calibration, and generates the match score.</p> 
<li><b>Sara Colantonio, Davide Moroni, Ovidio Salvetti (CNR-ISTI):</b> Shape modeling and deformation analysis of the left ventricle of the heart.<br>
<p align="justify"><b>Abstract:</b> A method has been developed for studying the left ventricle modification during the heart cycle. The method implements a model suitable to analyze the deformation parameters of the organ that can be used to compare different shapes and pathological scenarios.</p>
</ul>

<br>

<h3>E-teams presentations</h3>

<blockquote>

<h3><u>Wednesday 15 Feb, 16:30 - 17:00 :</u> E-team "Choosing Features for CBIR and Automated Image Annotation"</h3>

<p align="justify">
This session is organized and coordinated by Allan Hanbury (TU Vienna - PRIP).
</p>

<ul>
<li><b>Allan Hanbury (TU Vienna - PRIP):</b> Current achievements of the e-team <b>(30 min)</b>.
</ul>

<h3><u>Wednesday 15 Feb, 17:00 - 17:30 :</u> E-team "Semantic from audio"</h3>

<p align="justify">
This session is organized and coordinated by Graziano Bertini (ISTI - CNR). Time allocated to each presentation is in text.
</p>

<ul>
<li><b>A. T. Cemgil, and S. J. Godsill (University of Cambridge): </b> Bayesian Numerical Methods for analysis of Musical Audio <b>(10 min)</b>. <br>
<p align="justify"><b>Abstract:</b>
The main objective of the research was to investigate probabilistic models and associated Bayesian numerical methods for inference in musical audio.
Our approach focuses on methodology that enables us to take advantage of generic adaptive and self-learning solutions that need minimal
supervision.Guided by this philosophy, we have been able to produce new methodology in applications such as musical score transcription, source separation and
audio restoration. We believe that our methodology will be potentially useful for many multimedia applications. 
</p>
<li><b>E. Benetos and C. Kotropoulos (AIIA-AUTH), T. Lidy and A. Rauber (TU Wien - IFS):</b> Musical Instrument Classification <b>(10 min)</b>.<br>
<p align="justify"><b>Abstract:</b> In this presentation, a class of algorithms for automatic classification of individual musical instrument sounds is presented. Two feature sets were employed, the first containing perceptual features and MPEG-7 descriptors and the second containing rhythm patterns developed for the SOMeJB project. The features were measured for 300 sound recordings consisting of 6 different musical instrument classes. Subsets of the feature set are selected using branch-and-bound search, obtaining the most suitable features for classification. A class of supervised classifiers is developed based on the non-negative matrix factorization (NMF). The standard NMF method is examined as well as its modifications: the local and the sparse NMF. The experiments compare the two feature sets alongside the various NMF algorithms. The results demonstrate an almost perfect classification for the first set using the standard NMF algorithm (classification error 1.0%), outperforming the state-of-the-art techniques tested for the aforementioned experiment. </p>
<li><b>TU Wien, ISTI - CNR:</b> Current achievemnt of our work will treat with "Transient music enhancement by semantic driven control" <b>(10 min)</b>.<br>
<p align="justify"><b>Abstract:</b> The set of audio treatment methods commonly used in the mastering process of commercial music tend to increase the loudness perception of the final audio product. The side effect of these audio manipulation is the loss of transients and dynamic variations, which results in a flat sound. The widely diffused compressed audio formats (mp3, wma) introduces further degradation of the recorded music. The goal of the our proposed Aria algorithm is to enhance the music song transient heavily somewhat modified during the mastering and compression phase. At the day, the aliquota of the effect parameters can be set handly by  a psicoacustic criterion. We propose to study the possibility to dinamically define the above parameters by some features extracted by the song.</p>
</ul>

<h3><u>Wednesday 15 Feb, 17:30 - 18:00 :</u> E-team "Content Analysis Showcase"</h3>

<p align="justify">
This session is organized and coordinated by Andreas Rauber (TU Vienna - IFS). Time allocated to each presentation is in text.
</p>

<ul>
<li><b>Andreas Rauber (TU Wien - IFS)</b>: e-team Overview <b>(10 min)</b>.<br>
<p align="justify"><b>Abstract:</b>
The goal of this e-team is to bring together the wide range of semantic analysis and    annotation capabilities that are present within MUSCLE to show on a single show-case demo. We would compile a video sequence, based on contributions from each partner, by combining TV short recordings of different kinds (say news reports, music clips, etc.) and home videos. These would be integrated and shared by all team members, who are then invited to perform whatever semantic extraction and analysis (single- or multi-modal) they can apply to this video, such as all kind of low-level feature extraction, face detection, moving objects, fire-and-smoke detection, logo detection, music genre analysis, speech recognition, text detection and recognition, etc. To achieve this, partners are allowed to use all kind of algorithms, additional external information, as well ass additional data they may have and use within their own labs to enhance the information extracted from the video.</p>
<li><b>Nicu Sebe (University of Amsterdam):</b> contribution of UoA <b>(10 min)</b>.<br><br>
<li><b>Andreas Rauber (TU Wien - IFS):</b> contribution of TU Wien <b>(10 min)</b>.<br><br>
<li>Summary, outlook and further steps, invitation to other contribuents. 
</ul>

<h3><u>Friday 17 Feb, 10:30 - 12:30 :</u> E-team "Person detection and recognition, tracking and analysis"</h3>

<p align="justify">
This session is organized and coordinated by Montse Pardas (UPC). Time allocated to each presentation is in text.
</p>

<ul>
<li><b>Montse Pardas (UPC):</b> E-team presentation, current activities and future plans <b>(10 min)</b>.<br><br>
<li><b>Csaba Beleznai (Advanced Computer Vision GmbH - ACV):</b> Human Detection in Difficult Scenarios by Combining Motion and Appearance <b>(20 min)</b>.<br>
<p align="justify"><b>Abstract:</b> Reliable human detection is a key issue in automated visual surveillance systems. Motion detection provides a strong cue towards accomplishing this task. However, typical scenarios usually contain motion clutter leading to false alarms. We present a real-time framework which combines a model-based human detection approach relying on motion detection and statistical learning in order to validate detected objects and remove spurious observations. The combined detection scheme shows improvements in terms of lower false alarm rates and improved tracking performance for difficult scenarios containing moving shadows and vehicles. Detection and tracking results for such scenarios will be presented. </p>
<li><b>László Havasi, Student Member, Zoltán Szlávik, Member, and Tamás Szirányi (SZTAKI): </b>Detection of Pedestrians´ Legs for Scene Analysis in Video Surveillance System <b>(20 min)</b>.<br>
<p align="justify"><b>Abstract:</b> A robust walk detection algorithm is presented, based on our symmetry approach which can be used to extract biometric characteristics from video image-sequences. To obtain a useful descriptor of a walking person, we temporally track the symmetries of a person's legs. In a further processing stage, these patterns are filtered, then re-sampled and transformed to a subspace with a much smaller dimension of an "eigenwalk space". Our method is suitable for use in indoor or outdoor surveillance scenes. Image registration methods are presented which are applicable to multi-camera systems viewing human subjects in motion. Determining the leading leg of the walking subject is important and the presented method can identify this from two successive walk-steps (one walk cycle). Using this approach, we can detect sufficient numbers of corresponding points for the estimation of correspondence between views of two cameras in overlapping and in a special case of non-overlapping camera configurations.</p>
<li><b>Cristian Canton (UPC): </b> 3D Human action recognition in multiple view scenarios <b>(15 min)</b>.<br>
<p align="justify"><b>Abstract:</b> We present a view-independent approach to the recognition of human gestures of several people in low resolution sequences from multiple calibrated cameras. In contraposition with other multi-ocular gesture recognition systems based on generating a classification on a fusion of features coming from different views, our system performs a data fusion (3D representation of the scene) and then a feature extraction and classification. Motion descriptors introduced by Bobick et al. for 2D data are extended to 3D and a set of features based on 3D invariant statistical moments are computed. Finally, a Bayesian classifier is employed to perform recognition over a small set of actions. Results are provided showing the effectiveness of the proposed algorithm in a Smart-Room scenario.</p>
<li><b>Mehmet Turkan and Enis Cetin (Bilkent University): </b> Real-time face detection in video using edge projections <b>(20 min)</b>.<br><br>
<li><b>Thang V. Pham (ISLA/University of Amsterdam):</b> Fast support vector classification by space partitioning and local approximation <b>(20 min)</b>.<br>
<p align="justify"><b>Abstract:</b> In this talk we present a method to speed up support vector classification. This is especially important when data is high-dimensional. Unlike previous approaches which focus on less support vectors, we partition the data space into local regions, and perform approximation by linear functions. The experimental results on 31 datasets show that the performance degrades marginally, while the speedup is significant, up to three orders of magnitude.</p>
<li><b>S. Asteriadis, I. Pitas, N. Nikolaidis (AIIA - AUTH):</b> Eye and lip area detection using geometrical information <b>(15 min)</b>.<br>
<p align="justify"><b>Abstract:</b> 
We present a method for facial features detection that uses geometrical information, even in low resolution images. Having located a face region using a face detector, the edge map is extracted. A vector pointing to the closest edge pixel is then assigned to every pixel. PCA is applied on length and slope information for these vectors on areas of fixed dimensions and recognition follows based on eigenvectors extracted during a training stage. A similar approach is used for lip area detection. For eye center localization, edge and intensity information is used. The proposed method can work very efficiently on low-resolution images.</p> 

</ul>

<h3><u>Friday 17 Feb, 13:30 - 15:00 :</u> E-team "Visual Saliency"</h3>

<p align="justify">
This session is organized by Nozha Boujemaa and Valerie Gouet-Brunet (INRIA/Imedia) and coordinated by Montse Pardas (UPC). Time allocated to each presentation is in text.
</p>

<ul>
<li><b>T. Amiaz and N. Kiryati (TAU-Visual):</b> Piecewise-smooth optical flow <b>(20 min)</b>.<br>
<p align="justify"><b>Abstract:</b> We propose a new algorithm for dense optical flow computation.
Dense optical flow schemes are challenged by the presence of
motion discontinuities. In state of the art optical flow methods,
over-smoothing of flow discontinuities accounts for most of the error.
A breakthrough in the performance of optical flow computation has
recently been achieved by Brox et al. Our algorithm embeds
their functional within a two phase active contour segmentation framework.
Piecewise-smooth flow fields are accommodated and flow boundaries
are crisp. Experimental results show the superiority of our algorithm
with respect to alternative techniques.
We also study a special case of optical flow computation, in which the
camera is static. In this case we utilize a known background image to
separate the moving elements in the sequence from the static elements.
Tests with challenging real world sequences
demonstrate the performance gains made possible by incorporating the
static camera assumption in our algorithm.</p>
<li><b>I. Laptev (INRIA/Vista):</b> Periodic Motion Detection and Segmentation via Approximate Sequence Alignment <b>(20 min)</b>.<br>
<p align="justify"><b>Abstract:</b>
A method for detecting and segmenting periodic motion will be presented. We exploit periodicity as a cue and detect periodic motion in complex scenes where common methods for motion segmentation are likely to fail. We note that periodic motion detection can be seen as an approximate case of sequence alignment where an image sequence is matched to itself over one or more periods of time. To use this observation, we first consider alignment of two video sequences obtained by independently moving cameras. Under assumption of constant translation, the fundamental matrices and the homographies are shown to be time-linear matrix functions. These dynamic quantities can be estimated by matching corresponding space-time points with similar local motion and shape. For periodic motion, we match corresponding points across periods and develop a RANSAC procedure to simultaneously estimate the period and the dynamic geometric transformations between periodic views. Using this method, we demonstrate detection and segmentation of human periodic motion in complex scenes with non-rigid backgrounds, moving camera and motion parallax.</p>
<li><b>Petros Maragos (ICCS-NTUA):</b> <b>(10 min)</b><br><br>
<li><b>Julien Fauqueur (University of Cambridge):</b> Multiresolution detection of local structures <b>(10 min)</b>.<br>
<p align="justify"><b>Abstract:</b>
In the context of visual recognition, we present some recent results on our work to detect different types of local features, such as corners, blobs, edges, ridges. Our methods are based on multiresolution analysis (with the DTCWT transform) and enable us to detect these various salient structures at different scales. The properties of DTCWT transform make it a privileged basis to satisfy both speed and accuracy factors. We will show some results and discuss open issues, such as accurate scale selection of a feature. 
</p>
<li><b>Alireza Tavakoli Targhi, Babak rasolzadeh, Morten Björkman, Eric Hayman, Jan- olof Eklundeh (KTH):</b> The Influence of Texture Cues in an Attention System Integrating Top-Down and Bottom-Up Information <b>(20 min)</b>.<br>
<p align="justify"><b>Abstract:</b>
The aim of this paper is to show that texture can play an important role in attention and influence both top-down and bottom-up effects. We do that by adding a texture measure to an earlier presented model based on biased/weighted feature saliency maps for combining top-down and bottom up information.  Here we  use a texture cue so called SVD-Transform based on a direct Singular Value Decomposition  of the image patch as texture descriptor.    The transform provides a measure of roughness by considering the singular values of a matrix which is formed very simply by inserting the greyvalues of a square patch around a pixel directly into a matrix of the same size. We will show that the properties of our texture cue make it well suited in a system for visual attention in both of bottom-up and top-down task. Textured objects pop out from the background using the transform in a bottom-up manner. Furthermore,for the top-down processing, our saliency combination model can with the help of an extra texture  cue, incorporate information about cluttered objects or even objects with more details in their structures, promoting the texture cue in the top-down map. The results show the advantages of this texture descriptor and it turns out to direct attention to salient areas and objects ignored by earlier models, including the one we've presented previously. 
</p>
</ul>

</blockquote>

<br>

<h3>WP3 Dissemination</h3>

<p align="justify">
There will be a one hour plenary session, where Allan Hanbury will present:
</p>

<ul>
<li>Evaluation campaigns running in 2006
<li>new data available on the WP3 server.
</ul>

<br>
<center>
<a href="Musclemeeting4-Istanbul-Feb06.html#top"><img src="fleche_up_transp.gif" border=0></a>
</center>
<hr>
<i>
<!-- Created: Tue Sep 28 15:28:28 Paris 2004 -->
<!-- hhmts start -->
Last modified: Fri Feb 10 20:26:48 CET 2006
<!-- hhmts end -->
</i>
</body>
</html>
